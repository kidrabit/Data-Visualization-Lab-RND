{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, os.path\n",
    "# os.environ['TF_CPP_MIN_LOG_LEVEL'] = '0' \n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "tf_device='/gpu:0'\n",
    "import random\n",
    "import numpy as np\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import tensorflow as tf\n",
    "tf.get_logger().setLevel('INFO')\n",
    "tf.keras.backend.floatx()\n",
    "from collections import deque\n",
    "import itertools\n",
    "from sklearn.utils import shuffle\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.layers import Dense\n",
    "from libs.utils import *\n",
    "from libs.generate_boxes import *\n",
    "from libs.dqn import *\n",
    "\n",
    "num_episode = 1500\n",
    "global_step = 0\n",
    "tr_l, h_fill, tr_r, avg_loss_l,history_eps,used_boxes_eps  = [],[],[],[],[],[]\n",
    "N_MDD = 7\n",
    "K = 4# 선택할 박스 수\n",
    "n_candidates =4 # K보다 커야 함\n",
    "\n",
    "# generating boxes\n",
    "# boxes_multi1,gt_pos1 = generation_3dbox(case_size=[[20,20,20]], min_s = 1, N_mdd=N_MDD)#, is_prediv='h')\n",
    "\n",
    "boxes_multi1 = [np.array([[20, 20,  4],\n",
    "         [20,  4,  4],\n",
    "         [20,  4,  4],\n",
    "         [20,  4,  4],\n",
    "         [20,  4,  4],\n",
    "         [20,  4,  4],\n",
    "         [20, 20,  4],\n",
    "         [20, 20,  4],\n",
    "         [20, 20,  4]])]\n",
    "gt_pos1 = [np.array([[ 0,  0,  0],\n",
    "         [ 0,  0,  4],\n",
    "         [ 0,  4,  4],\n",
    "         [ 0,  8,  4],\n",
    "         [ 0, 12,  4],\n",
    "         [ 0, 16,  4],\n",
    "         [ 0,  0,  8],\n",
    "         [ 0,  0, 12],\n",
    "         [ 0,  0, 16]])]\n",
    "\n",
    "boxes_multi2 = [np.array([[20, 20,  5],\n",
    "        [ 4, 20,  5],\n",
    "        [ 4, 20,  5],\n",
    "        [ 4, 20,  5],\n",
    "        [ 4, 20,  5],\n",
    "        [ 4, 20,  5],\n",
    "        [10, 20,  5],\n",
    "        [10, 20,  5],\n",
    "        [20, 20,  5]])]\n",
    "\n",
    "gt_pos2 = [np.array([[ 0,  0,  0],\n",
    "        [ 0,  0,  5],\n",
    "        [ 4,  0,  5],\n",
    "        [ 8,  0,  5],\n",
    "        [12,  0,  5],\n",
    "        [16,  0,  5],\n",
    "        [ 0,  0, 10],\n",
    "        [10,  0, 10],\n",
    "        [ 0,  0, 15]])]\n",
    "# idx = np.lexsort((gt_pos2[:,1],gt_pos2[:,0],gt_pos2[:,2]))\n",
    "# gt_pos2 = gt_pos2[idx]\n",
    "# boxes_multi2 = np.array(boxes_multi2)[idx]\n",
    "\n",
    "num_max_boxes = max(len(boxes_multi1[0]), len(boxes_multi2[0]))#len(boxes_multi[0]) #N_MDD+3# \n",
    "num_max_remain = num_max_boxes# - K\n",
    "print('num_max_boxes',num_max_boxes,'num_max_remain',num_max_remain)\n",
    "\n",
    "env=Bpp3DEnv()\n",
    "agent = DQNAgent( L=20, B=20, H=20, n_remains = num_max_remain, n_loading = K, \n",
    "                 lr=1e-4, exp_steps=900, train_st = 500, memory_len=1000, update_target_rate = 50,\n",
    "                net = 'DDQN')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for e in range(1,num_episode):\n",
    "    st=time.time()\n",
    "    \n",
    "    if num_episode>1000 and agent.epsilon <= 0.01:\n",
    "        agent.epsilon = 0\n",
    "    \n",
    "    if e%2 == 1: boxes_multi,gt_pos = boxes_multi1.copy(), gt_pos1.copy()\n",
    "    else: boxes_multi,gt_pos = boxes_multi2.copy(), gt_pos2.copy()\n",
    "    \n",
    "    env.reset()# env 초기화\n",
    "    done = False\n",
    "    step = 0\n",
    "    history, h_load, h_remain_size, h_load_size = [],[],[],[]\n",
    "    t_history, t_load, t_remain_size, t_load_size = [],[],[],[]\n",
    "    used_boxes, pred_pos = [],[]\n",
    "    \n",
    "    #boxes_all = np.array(boxes_multi)[e%100].copy()\n",
    "    boxes_all = np.array(boxes_multi)[0].copy()\n",
    "    r_boxes = boxes_all.copy() #single case\n",
    "    \n",
    "    if agent.epsilon > agent.epsilon_end and len(agent.memory)>=agent.train_start:\n",
    "        agent.epsilon -= agent.epsilon_decay_step\n",
    "\n",
    "    while not done:#중박스에 대해 반복##############\n",
    "        state = env.container_s.copy()\n",
    "        state_h = env.container_h.copy()\n",
    "        global_step += 1\n",
    "        step += 1\n",
    "        k = min(K, len(r_boxes))\n",
    "        selected = cbn_select_boxes(r_boxes[:n_candidates], k) #r_boxes의 앞에서부터 n_candidates 만큼의 상자 중 k개의 중박스 선택 \n",
    "        #selected = r_boxes.reshape((-1,1,3))\n",
    "        s_order = get_selected_order(selected, k)\n",
    "        \n",
    "        num_loaded_c, loading_size_c, loading_pos_c, next_state_c, next_h_c, loaded_mh_c = get_selected_location(s_order, state, state_h, env.height)\n",
    "        in_state, in_r_boxes, in_loading = raw2input(state_h, len(num_loaded_c), r_boxes, num_max_remain, K, loading_size_c, env.height) # input 변환 \n",
    "        \n",
    "        # action 선택 수가 1일 때 action 스킵\n",
    "        if len(num_loaded_c) == 1:\n",
    "            action_idx = 0\n",
    "        else: # 선택 가능\n",
    "            # action 실행\n",
    "            action_idx = agent.get_action(in_state, np.array(loaded_mh_c), in_r_boxes, in_loading)\n",
    "        \n",
    "        # 다음 step ########################\n",
    "        # env의 다음 state\n",
    "        env.step(next_state_c[action_idx], next_h_c[action_idx]) \n",
    "        # r_boxes 업데이트\n",
    "        num_loaded_box = num_loaded_c[action_idx]\n",
    "        if num_loaded_box != 0: # 적입된 박스가 하나 이상 있는 경우 -> append\n",
    "            new_used_boxes = loading_size_c[action_idx] # 기존 적재 박스, 기존 + 새 박스 -> 새로 적재된 박스 \n",
    "            r_boxes = get_remain(new_used_boxes, r_boxes) # 남은 박스 업데이트 = 남은 박스 - 새로 적재 박스\n",
    "            # 사용된 박스, 위치 업데이트\n",
    "            used_boxes = used_boxes + loading_size_c[action_idx] #사용된 박스 업데이트\n",
    "            pred_pos = pred_pos + loading_pos_c[action_idx] #예측된 위치 업데이트    \n",
    "        \n",
    "        else: # 적입된 박스가 없을 경우 - action = zero, state=t_state, 해당 박스들 모두 스킵\n",
    "            r_boxes = get_remain(s_order[action_idx], r_boxes) # 남은 박스 업데이트 = 남은 박스 - 선택된 박스\n",
    "        if len(r_boxes) == 0 or np.sum(env.container_h != env.height) == 0: # 더 이상 적입할 적입할 수 없는 경우(박스 다 사용 or 다 채워짐)\n",
    "            done = True\n",
    "        \n",
    "        # action 선택 수가 1이 이상이고 마지막 박스가 적입되었을 때 append \n",
    "        if len(num_loaded_c) != 1 or done:\n",
    "            # (s, a) append ########################\n",
    "            history.append(in_state[action_idx]) # 적재 전 상태\n",
    "            h_load.append(loaded_mh_c[action_idx]) # 적재된 상태\n",
    "            h_remain_size.append(in_r_boxes[action_idx]) # 선택 후 전처리된 남은 박스 사이즈\n",
    "            h_load_size.append(in_loading[action_idx]) # 적재된 박스 사이즈\n",
    "\n",
    "            # (s') append ########################\n",
    "            t_state = next_state_c[action_idx].copy() # 다음 state \n",
    "            t_state_h = next_h_c[action_idx].copy() \n",
    "            if done: #마지막 박스까지 채워졌을 경우 or 더 이상 적입할 수 없을 경우\n",
    "                in_t_history = t_state_h.reshape((1, env.length, env.breadth, 1))\n",
    "                loaded_mh_c = np.zeros((1, env.length, env.breadth, 2)) #(N, L, B, 2)\n",
    "                in_t_remains = np.zeros((1, env.length, env.breadth, num_max_remain))\n",
    "                in_t_loading = np.zeros((1, env.length, env.breadth, K))\n",
    "            else:\n",
    "                k = min(K, len(r_boxes))\n",
    "                selected = cbn_select_boxes(r_boxes[:n_candidates], k) # 업데이트된 남은 박스들 중 중박스 선택\n",
    "                #selected = r_boxes.reshape((-1,1,3))\n",
    "                s_order = get_selected_order(selected, k) # 정렬 선택\n",
    "                num_loaded_c, loading_size_c, loading_pos_c, next_state_c, next_h_c, loaded_mh_c = get_selected_location(s_order, t_state, t_state_h, env.height)\n",
    "                in_t_history, in_t_remains, in_t_loading = \\\n",
    "                    raw2input(t_state_h, len(num_loaded_c), r_boxes, num_max_remain,  K, loading_size_c, env.height)\n",
    "                \n",
    "            t_history.append(in_t_history) # 적재 전 상태\n",
    "            t_load.append(np.array(loaded_mh_c)) # 적재된 상태\n",
    "            t_remain_size.append(in_t_remains) # 선택 후 전처리된 남은 박스 사이즈\n",
    "            t_load_size.append(in_t_loading) # 적재된 박스 사이즈\n",
    "                \n",
    "        \n",
    "        # terminal reward 계산 -> replay memory에 추가 \n",
    "        if done:\n",
    "            # terminal reward \n",
    "            avg_tr = 0 if len(tr_r)==0 else np.mean(tr_r)\n",
    "            terminal_reward = (env.terminal_reward())**3 # - avg_tr\n",
    "            tr_l.append(terminal_reward)\n",
    "            h_fill.append(env.terminal_reward())\n",
    "            if agent.epsilon >= 0.7: tr_r.append(env.terminal_reward())\n",
    "            \n",
    "            a_repeate = 1#6 if env.terminal_reward() ==1.0 else 1\n",
    "            # append samples\n",
    "            is_last = False \n",
    "            N = len(history)\n",
    "            for i in range(N):\n",
    "                if i == N-1: is_last=True\n",
    "                reward=(0.99**(N-i-1))*terminal_reward #step reward\n",
    "                for a in range(a_repeate):\n",
    "                    agent.append_sample(history[i], h_load[i], h_remain_size[i], h_load_size[i], reward, is_last,\n",
    "                                        t_history[i], t_load[i], t_remain_size[i], t_load_size[i])\n",
    "            if global_step > agent.train_start:\n",
    "                agent.draw_tensorboard(terminal_reward, env.terminal_reward(), step, e)\n",
    "                \n",
    "        # 학습 시작\n",
    "        if len(agent.memory) >= agent.train_start:\n",
    "            agent.train_model()\n",
    "            if global_step % agent.update_target_rate == 0: # target model 업데이트\n",
    "                print('update',e)\n",
    "                agent.update_target_model()\n",
    "            avg_loss_l.append(agent.avg_loss/float(step))\n",
    "    \n",
    "        #vis_box(used_boxes,pred_pos)\n",
    "        #print(env.container_h)\n",
    "    #if global_step > agent.train_start:\n",
    "    #    agent.draw_tensorboard(score, step, e)\n",
    "    log = \"=====episode: {:5d} | \".format(e)\n",
    "    log += \"memory length: {:5d} | \".format(len(agent.memory))\n",
    "    log += \"epsilon: {:.3f} | \".format(agent.epsilon)\n",
    "    log += \"reward(): {:.3f}, {:.3f}| \".format(env.terminal_reward(), terminal_reward)\n",
    "    #log += \"t_reward: {:.3f} | \".format(t_reward)\n",
    "    #log += \"q avg : {:3.2f} | \".format(agent.avg_q_max / float(step))\n",
    "    log += \"avg loss : {:6f} \".format(agent.avg_loss / float(step))\n",
    "    log += \"time: {:.3f}\".format(time.time()-st)\n",
    "    print(log)\n",
    "    agent.avg_q_max, agent.avg_loss = 0, 0\n",
    "    \n",
    "    #used_boxes_eps.append(used_boxes)\n",
    "    #history_eps.append(history)\n",
    "    # 1000 에피소드마다 모델 저장\n",
    "    if e % 1000 == 0:\n",
    "        agent.model.save_weights(\"save_model/model\", save_format=\"tf\")\n",
    "    \n",
    "#print('total training time', time.time()-st)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "plt.figure(figsize=(20,3))\n",
    "plt.plot(h_fill,'g-')\n",
    "plt.ylim(-0.2,1.3)\n",
    "plt.grid()\n",
    "\n",
    "plt.figure(figsize=(20,3))\n",
    "plt.plot(avg_loss_l,'b-')\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "# %matplotlib inline\n",
    "vis_box(used_boxes,pred_pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "# %matplotlib inline\n",
    "vis_box(np.array(boxes_multi)[0],np.array(gt_pos)[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
