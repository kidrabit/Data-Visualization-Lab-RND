{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, os.path\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\"\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from collections import deque\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.layers import Dense\n",
    "from utils import generation_2dbox,whole_upleft, box_cornel, feasible_location,action_options_list\n",
    "config = tf.compat.v1.ConfigProto(device_count={'GPU':1})\n",
    "sess = tf.compat.v1.Session(config=config) \n",
    "\n",
    "# 상태가 입력, 큐함수가 출력인 인공신경망 생성\n",
    "class DQN(tf.keras.Model):\n",
    "    def __init__(self, state_size):\n",
    "        super(DQN, self).__init__()\n",
    "        self.dnn1 = Dense(512, activation='relu',input_shape=state_size)#\n",
    "        self.dnn2 = Dense(256, activation='relu')#\n",
    "        self.dnn3 = Dense(256, activation='relu')#\n",
    "        self.fc = Dense(256, activation='relu')\n",
    "        self.fc_out = Dense(1,activation='tanh')\n",
    "\n",
    "    def call(self, x):\n",
    "        x = self.dnn1(x)\n",
    "        x = self.dnn2(x)\n",
    "        x = self.dnn3(x)\n",
    "        x = self.fc(x)\n",
    "        q = self.fc_out(x)\n",
    "        return q\n",
    "\n",
    "\n",
    "# 브레이크아웃 예제에서의 DQN 에이전트\n",
    "class DQNAgent:\n",
    "    def __init__(self, state_size=(20*20+20*20,)):\n",
    "        #self.render = False\n",
    "        # 상태와 행동의 크기 정의\n",
    "        self.state_size = state_size\n",
    "        #self.action_size = action_size\n",
    "\n",
    "        # DQN 하이퍼파라미터\n",
    "        self.discount_factor = 0.99\n",
    "        self.learning_rate = 1e-4\n",
    "        self.epsilon = 1.\n",
    "        self.epsilon_start, self.epsilon_end = 1.0, 0.02\n",
    "        self.exploration_steps = 500\n",
    "        self.epsilon_decay_step = self.epsilon_start - self.epsilon_end\n",
    "        self.epsilon_decay_step /= self.exploration_steps\n",
    "        self.batch_size = 32\n",
    "        self.train_start = 1000\n",
    "        self.update_target_rate = 10\n",
    "\n",
    "        # 리플레이 메모리, 최대 크기 100,000\n",
    "        self.memory = deque(maxlen=2500)#17000\n",
    "        \n",
    "        # 모델과 타깃 모델 생성\n",
    "        self.model = DQN(state_size)\n",
    "        self.target_model = DQN(state_size)\n",
    "        self.optimizer = Adam(self.learning_rate)#, clipnorm=10.)\n",
    "        # 타깃 모델 초기화\n",
    "        self.update_target_model()\n",
    "        self.avg_q_max, self.avg_loss = 0, 0\n",
    "        self.writer = tf.summary.create_file_writer('summary')\n",
    "        self.model_path = os.path.join(os.getcwd(), 'save_model', 'model')\n",
    "\n",
    "    # 타깃 모델을 모델의 가중치로 업데이트\n",
    "    def update_target_model(self):\n",
    "        self.target_model.set_weights(self.model.get_weights())\n",
    "\n",
    "    # 입실론 탐욕 정책으로 행동 선택\n",
    "    def get_action(self, history,a_ops):\n",
    "        if len(a_ops)==0:\n",
    "            return 0,-1\n",
    "        \n",
    "        q_value_list=[]\n",
    "        history = np.array([history.flatten()]*len(a_ops))\n",
    "        actions = np.array([sample.flatten() for sample in a_ops])\n",
    "        q_value_list=self.model(np.concatenate([history,actions],axis=1))\n",
    "        \n",
    "        if np.random.rand() <= self.epsilon:\n",
    "            random_action_idx=random.randrange(len(a_ops))\n",
    "            random_action=a_ops[random_action_idx]\n",
    "            #print('random_action_idx',random_action_idx)\n",
    "            return random_action,random_action_idx\n",
    "        else:\n",
    "            #print('maximum action idx',np.argmax(q_value_list))\n",
    "            #q_value = self.model(history)\n",
    "            amax_action_idx=np.argmax(q_value_list)\n",
    "            amax_action=a_ops[amax_action_idx]\n",
    "            return amax_action,amax_action_idx\n",
    "\n",
    "    # 샘플 <s, a, r, s'>을 리플레이 메모리에 저장\n",
    "    def append_sample(self, history, action, reward, next_history,next_action):\n",
    "        self.memory.append((history, action, reward, next_history,next_action))\n",
    "\n",
    "    # 텐서보드에 학습 정보를 기록\n",
    "    #def draw_tensorboard(self, score, step, episode):\n",
    "    #    with self.writer.as_default():\n",
    "    #        tf.summary.scalar('Total Reward/Episode', score, step=episode)\n",
    "    #        tf.summary.scalar('Average Max Q/Episode',\n",
    "    #                          self.avg_q_max / float(step), step=episode)\n",
    "    #        tf.summary.scalar('Duration/Episode', step, step=episode)\n",
    "    #        tf.summary.scalar('Average Loss/Episode',\n",
    "    #                          self.avg_loss / float(step), step=episode)\n",
    "\n",
    "    # 리플레이 메모리에서 무작위로 추출한 배치로 모델 학습\n",
    "    def train_model(self):\n",
    "        #if self.epsilon > self.epsilon_end:\n",
    "        #    self.epsilon -= self.epsilon_decay_step\n",
    "\n",
    "        ## 메모리에서 배치 크기만큼 무작위로 샘플 추출\n",
    "        batch = random.sample(self.memory, self.batch_size)\n",
    "\n",
    "        history = np.array([sample[0].flatten() for sample in batch])\n",
    "        actions = np.array([sample[1].flatten() for sample in batch])\n",
    "        rewards = np.array([sample[2] for sample in batch])\n",
    "        next_history = np.array([sample[3].flatten() for sample in batch])\n",
    "        #dones = np.array([sample[4] for sample in batch])\n",
    "        next_actions = [np.array(sample[4]).reshape((-1,20*20)) for sample in batch]\n",
    "\n",
    "        # 학습 파라메터\n",
    "        model_params = self.model.trainable_variables\n",
    "        with tf.GradientTape() as tape:\n",
    "            # 현재 상태에 대한 모델의 큐함수\n",
    "            predicts = self.model(np.concatenate([history,actions],axis=1))\n",
    "            #one_hot_action = tf.one_hot(actions, self.action_size)\n",
    "            #predicts = tf.reduce_sum(one_hot_action * predicts, axis=1)\n",
    "\n",
    "            # 다음 상태에 대한 타깃 모델의 큐함수\n",
    "            #target_predicts = self.target_model(next_history)\n",
    "            target_predicts=[]\n",
    "            for i,na in enumerate(next_actions):\n",
    "                target_predicts.append(self.target_model(np.concatenate([np.array([next_history[i]]*len(na)),na],axis=1)))\n",
    "            #target_predicts=self.target_model(np.concatenat([next_history,next_actions]))\n",
    "            \n",
    "            # 벨만 최적 방정식을 구성하기 위한 타깃과 큐함수의 최대 값 계산\n",
    "            #max_q = np.amax(target_predicts, axis=1)\n",
    "            targets =[]\n",
    "            for i,tp in enumerate(target_predicts):\n",
    "                max_q=np.amax(tp)\n",
    "                targets.append([(1- 0.75)*rewards[i] + 0.75*max_q])\n",
    "            targets=np.array(targets)\n",
    "            #targets = rewards + (1 - dones) * self.discount_factor * max_q\n",
    "            #targets = (1- self.discount_factor)*rewards + self.discount_factor*max_q\n",
    "\n",
    "            # 후버로스 계산\n",
    "            #error = tf.abs(targets - predicts)\n",
    "            #quadratic_part = tf.clip_by_value(error, 0.0, 1.0)\n",
    "            #linear_part = error - quadratic_part\n",
    "            #loss = tf.reduce_mean(0.5 * tf.square(quadratic_part) + linear_part)\n",
    "            loss = tf.reduce_mean(tf.square(targets - predicts))\n",
    "            \n",
    "            self.avg_loss += loss.numpy()\n",
    "\n",
    "        # 오류함수를 줄이는 방향으로 모델 업데이트\n",
    "        grads = tape.gradient(loss, model_params)\n",
    "        self.optimizer.apply_gradients(zip(grads, model_params))\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import gym_bpp\n",
    "# 환경과 DQN 에이전트 생성\n",
    "#env = gym.make('BreakoutDeterministic-v4')\n",
    "env = gym.make('bpp-v0')\n",
    "agent = DQNAgent()\n",
    "global_step = 0\n",
    "num_episode = 2000\n",
    "# np.random.seed(seed=100)\n",
    "\n",
    "frac_list=[]\n",
    "avg_loss_list=[]\n",
    "history_eps=[]\n",
    "used_boxes_eps=[]\n",
    "for e in range(num_episode):\n",
    "    done = False\n",
    "    step = 0\n",
    "    # env 초기화\n",
    "    env.reset()\n",
    "    boxes,gt_tmp=generation_2dbox(N_epi=1,c_l=20,c_b=20)#N,2\n",
    "    boxes=boxes[0]\n",
    "    \n",
    "    history = np.zeros((20,20))#container\n",
    "    history_list=[]\n",
    "    action_list=[]\n",
    "    reward_list=[]\n",
    "    next_history_list=[]\n",
    "    upleft_list=[]\n",
    "    next_action_list=[]\n",
    "    used_boxes=[]\n",
    "    \n",
    "    if e < 500 and agent.epsilon > agent.epsilon_end:\n",
    "        agent.epsilon -= agent.epsilon_decay_step\n",
    "\n",
    "    while not done:#박스에 대해 반복##############\n",
    "        \n",
    "        #if agent.render:\n",
    "        #    env.render()\n",
    "        global_step += 1\n",
    "        step += 1\n",
    "        box=boxes[step-1]#[L,B]\n",
    "        w_upleft =whole_upleft(*box_cornel([0,0],20,20),box[0],box[1])\n",
    "        \n",
    "        # 바로 전 history를 입력으로 받아 행동을 선택\n",
    "        for i,ul in enumerate(upleft_list):\n",
    "            w_upleft += whole_upleft(*box_cornel([ul[0],ul[1]], used_boxes[i][0], used_boxes[i][1]),box[0],box[1])\n",
    "        f_upleft = feasible_location(history,w_upleft,box[0],box[1])\n",
    "        a_ops=action_options_list(f_upleft)\n",
    "        action,action_idx = agent.get_action(history,a_ops)\n",
    "        \n",
    "        # 중박스 스킵 허용\n",
    "        if action_idx==-1 and step != len(boxes): continue\n",
    "            \n",
    "        #메모리 추가(action이 없거나 박스가 다 사용되거나)-------------------------------\n",
    "        if  action_idx==-1 or step ==len(boxes):#취할 action이 더 없는 경우(대박스가 다 충진된 경우 포함)\n",
    "            done=True\n",
    "            print('action = 0 or all boxes are used',action_idx,'|',step)\n",
    "            if len(frac_list)==0:\n",
    "                t_reward=env.terminal_reward()\n",
    "            else:\n",
    "                t_reward=env.terminal_reward()- np.mean(frac_list)#terminal reward \n",
    "            frac_list.append(env.terminal_reward())\n",
    "                \n",
    "            for i in range((len(history_list))):\n",
    "                if len(next_action_list[i])>0:\n",
    "                    reward=(0.99**(len(history_list)-i-1))*t_reward #step reward\n",
    "                    agent.append_sample(history_list[i], action_list[i], reward, next_history_list[i],next_action_list[i])\n",
    "        # 선택한 행동으로 환경에서 한 타임스텝 진행\n",
    "        else:\n",
    "            used_boxes.append(box)\n",
    "            next_box=boxes[step]\n",
    "            next_w_upleft=whole_upleft(*box_cornel([0,0],20,20),next_box[0],next_box[1])\n",
    "            upleft=f_upleft[action_idx]\n",
    "            upleft_list.append(upleft)\n",
    "            next_history = env.step(upleft,box[0],box[1])#action options=[] -> done=True\n",
    "            # 샘플 <s, a, r, s'>을 리플레이 메모리에 저장\n",
    "            history_list.append(history)\n",
    "            action_list.append(action)\n",
    "            next_history_list.append(next_history)\n",
    "            # getting next action after calculate upleft_list\n",
    "            for i,ul in enumerate(upleft_list):\n",
    "                next_w_upleft += whole_upleft(*box_cornel([ul[0],ul[1]],boxes[i][0],boxes[i][1]),next_box[0],next_box[1])\n",
    "            next_f_upleft_list = feasible_location(next_history,next_w_upleft,box[0],box[1])\n",
    "            next_action_list.append(action_options_list(next_f_upleft_list))\n",
    "            #agent.avg_q_max += np.amax(agent.model(history)[0])\n",
    "            history = next_history\n",
    "            \n",
    "        # epi 1000 이상부터 ### 메모리 차면 학습X ############################################################\n",
    "        if e >= 5:#if len(agent.memory) >= agent.train_start:\n",
    "            agent.train_model()\n",
    "            # 일정 시간마다 타겟모델을 모델의 가중치로 업데이트\n",
    "            if global_step % agent.update_target_rate == 0:\n",
    "                agent.update_target_model()\n",
    "                \n",
    "        if done:#------------------------------------------------------------------------------\n",
    "            # 각 에피소드 당 학습 정보를 기록\n",
    "            #if global_step > agent.train_start:\n",
    "            #    agent.draw_tensorboard(score, step, e)\n",
    "            log = \"============episode: {:5d} | \".format(e)\n",
    "            log += \"memory length: {:5d} | \".format(len(agent.memory))\n",
    "            log += \"epsilon: {:.3f} | \".format(agent.epsilon)\n",
    "            log += \"env.terminal_reward(): {:.3f} | \".format(env.terminal_reward())\n",
    "            #log += \"t_reward: {:.3f} | \".format(t_reward)\n",
    "            #log += \"q avg : {:3.2f} | \".format(agent.avg_q_max / float(step))\n",
    "            log += \"avg loss : {:6f}\".format(agent.avg_loss / float(step))\n",
    "            print(log)\n",
    "            #agent.avg_q_max, agent.avg_loss = 0, 0\n",
    "            avg_loss_list.append(agent.avg_loss/float(step))\n",
    "            agent.avg_loss = 0\n",
    "    \n",
    "    #used_boxes_eps.append(used_boxes)\n",
    "    #history_eps.append(history)\n",
    "    # 1000 에피소드마다 모델 저장\n",
    "    if e % 1000 == 0:\n",
    "        agent.model.save_weights(\"save_model/model\", save_format=\"tf\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,5))\n",
    "plt.plot(list(range(5)),frac_list[:5],'b.-')\n",
    "plt.plot(list(range(5,5+len(frac_list[5:]))),frac_list[5:],'g.-')\n",
    "plt.grid()\n",
    "\n",
    "plt.figure(figsize=(20,5))\n",
    "plt.plot(avg_loss_list[5:],'b.-')\n",
    "plt.grid()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
