{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a470b30e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-08T16:07:35.839783Z",
     "start_time": "2021-05-08T16:07:32.644768Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import math\n",
    "import time\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from collections import deque\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Dense, Conv1D, MaxPooling1D, Flatten, concatenate, Conv2D, MaxPooling2D\n",
    "import tensorflow.keras.losses as kls\n",
    "#import tensorflow_probability as tfp\n",
    "\n",
    "from libs.utils import *\n",
    "from libs.generate_boxes import  *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "75a732a7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-08T16:07:35.855352Z",
     "start_time": "2021-05-08T16:07:35.842733Z"
    }
   },
   "outputs": [],
   "source": [
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '0'\n",
    "tf.get_logger().setLevel('INFO')\n",
    "tf.keras.backend.floatx()\n",
    "\n",
    "plt.style.use('fivethirtyeight')\n",
    "plt.rcParams['figure.figsize'] = (20,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b4eba616",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-08T16:07:37.363763Z",
     "start_time": "2021-05-08T16:07:37.353788Z"
    }
   },
   "outputs": [],
   "source": [
    "class Actor(tf.keras.Model):\n",
    "    def __init__(self, state_size, selected_size, remain_size, output_size):\n",
    "        super(Actor, self).__init__()\n",
    "        \n",
    "        l1, b1, k1 = state_size\n",
    "        self.state_size = (l1*b1*k1,)\n",
    "        self.case_dnn1 = Dense(64, activation='relu', input_shape=self.state_size)\n",
    "        self.case_dnn2 = Dense(64, activation='relu')\n",
    "        \n",
    "        l2, b2, k2 = selected_size\n",
    "        self.selected_size = (l2*b2*k2,)\n",
    "        self.select_dnn1 = Dense(64, activation='relu', input_shape=self.selected_size)\n",
    "        self.select_dnn2 = Dense(64, activation='relu')\n",
    "        \n",
    "        l3, b3, k3 = remain_size\n",
    "        self.remain_size = (l3*b3*k3,)\n",
    "        self.remain_dnn1 = Dense(128, activation='relu', input_shape=self.remain_size)\n",
    "        self.remain_dnn2 = Dense(128, activation='relu')\n",
    "        \n",
    "        self.d1 = Dense(256, activation='relu')\n",
    "        self.d2 = Dense(256, activation='relu')\n",
    "        self.d3 = Dense(128, activation='relu')\n",
    "        self.out = Dense(output_size, activation='softmax')\n",
    "        \n",
    "    def call(self, cb_list):\n",
    "        c, s, r = cb_list[0], cb_list[1], cb_list[2]\n",
    "        c = tf.reshape(c, [-1, self.state_size[0]])\n",
    "        s = tf.reshape(s, [-1, self.selected_size[0]])\n",
    "        r = tf.reshape(r, [-1, self.remain_size[0]])\n",
    "        \n",
    "        c = self.case_dnn1(c)\n",
    "        c = self.case_dnn2(c)\n",
    "        \n",
    "        s = self.select_dnn1(s)\n",
    "        s = self.select_dnn2(s)\n",
    "        \n",
    "        r = self.remain_dnn1(r)\n",
    "        r = self.remain_dnn2(r)\n",
    "        \n",
    "        x = concatenate([c,s,r])\n",
    "        x = self.d1(x)\n",
    "        x = self.d2(x)\n",
    "        x = self.d3(x)\n",
    "        q = self.out(x)\n",
    "        return q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "692ea8f3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-08T16:07:39.542594Z",
     "start_time": "2021-05-08T16:07:39.526992Z"
    }
   },
   "outputs": [],
   "source": [
    "class Critic(tf.keras.Model):\n",
    "    def __init__(self, state_size, selected_size, remain_size, output_size):\n",
    "        super(Critic, self).__init__()\n",
    "        \n",
    "        l1, b1, k1 = state_size\n",
    "        self.state_size = (l1*b1*k1,)\n",
    "        self.case_dnn1 = Dense(64, activation='relu', input_shape=self.state_size)\n",
    "        self.case_dnn2 = Dense(64, activation='relu')\n",
    "        \n",
    "        l2, b2, k2 = selected_size\n",
    "        self.selected_size = (l2*b2*k2,)\n",
    "        self.select_dnn1 = Dense(64, activation='relu', input_shape=self.selected_size)\n",
    "        self.select_dnn2 = Dense(64, activation='relu')\n",
    "        \n",
    "        l3, b3, k3 = remain_size\n",
    "        self.remain_size = (l3*b3*k3,)\n",
    "        self.remain_dnn1 = Dense(128, activation='relu', input_shape=self.remain_size)\n",
    "        self.remain_dnn2 = Dense(128, activation='relu')\n",
    "        \n",
    "        self.d1 = Dense(256, activation='relu')\n",
    "        self.d2 = Dense(256, activation='relu')\n",
    "        self.d3 = Dense(128, activation='relu')\n",
    "        self.out = Dense(output_size, activation='softmax')\n",
    "        \n",
    "    def call(self, cb_list):\n",
    "        c, s, r = cb_list[0], cb_list[1], cb_list[2]\n",
    "        c = tf.reshape(c, [-1, self.state_size[0]])\n",
    "        s = tf.reshape(s, [-1, self.selected_size[0]])\n",
    "        r = tf.reshape(r, [-1, self.remain_size[0]])\n",
    "        \n",
    "        c = self.case_dnn1(c)\n",
    "        c = self.case_dnn2(c)\n",
    "        \n",
    "        s = self.select_dnn1(s)\n",
    "        s = self.select_dnn2(s)\n",
    "        \n",
    "        r = self.remain_dnn1(r)\n",
    "        r = self.remain_dnn2(r)\n",
    "        \n",
    "        x = concatenate([c,s,r])\n",
    "        x = self.d1(x)\n",
    "        x = self.d2(x)\n",
    "        x = self.d3(x)\n",
    "        q = self.out(x)\n",
    "        return q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "c6d7f682",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-08T16:14:00.737863Z",
     "start_time": "2021-05-08T16:14:00.717585Z"
    }
   },
   "outputs": [],
   "source": [
    "class PPO_Agent():\n",
    "    def __init__(self, L=20, B=20, H=20, n_remains=5, lr=1e-8, exp_steps=500,\n",
    "                train_st = 200, memory_len = 500):\n",
    "        self.state_size = (L,B,1)\n",
    "        self.selected_size = (L,B,2)\n",
    "        self.remain_size = (L,B,n_remains)\n",
    "        self.output_size = 1\n",
    "        \n",
    "        self.discount_factor = 0.99\n",
    "        self.learning_rate = lr #1e-8 #1e-4\n",
    "        self.exploration_steps = exp_steps\n",
    "        self.batch_size = 32\n",
    "        self.train_start = train_st\n",
    "        self.beta = 0.2\n",
    "        self.clip_pram = 0.2\n",
    "        self.memory = deque(maxlen=memory_len)\n",
    "        self.gamma = 0.9\n",
    "        self.actor = Actor(self.state_size, self.selected_size, self.remain_size, self.output_size)\n",
    "        self.critic = Critic(self.state_size, self.selected_size, self.remain_size, self.output_size)\n",
    "        self.actor_optimizer = Adam(self.learning_rate)\n",
    "        self.critic_optimizer = Adam(self.learning_rate)\n",
    "        self.avg_actor_loss = 0\n",
    "        self.avg_critic_loss = 0\n",
    "        \n",
    "    def get_action(self, state, loaded_mh_c, r_boxes):\n",
    "        q_values = self.actor([state, loaded_mh_c, r_boxes])\n",
    "        argmax_idx = np.where(q_values == tf.math.reduce_max(q_values))\n",
    "        action_idx = argmax_idx[0][0]\n",
    "        return q_values, argmax_idx, action_idx\n",
    "        \n",
    "    def append_sample(self, history, load, remain_size, load_size, reward, last, next_history, next_load, next_remain_size, next_load_size):\n",
    "        self.memory.append(( history, load, remain_size, load_size, reward, last, next_history, next_load, next_remain_size, next_load_size))\n",
    "        \n",
    "    def actor_loss_temp(self, probs, actions, adv, old_probs, closs):\n",
    "        probability = probs\n",
    "        entropy = tf.reduce_mean(tf.math.negative(tf.math.multiply(probability, tf.math.log(probability))))\n",
    "        sur1 = []\n",
    "        sur2 = []\n",
    "        \n",
    "        for pb, t, op in zip(probability, adv, old_probs):\n",
    "            t = tf.constant(t)\n",
    "            op = tf.constant(op)\n",
    "            ratio = tf.math.divide(pb, op)\n",
    "            s1 = tf.math.multiply(ratio, t)\n",
    "            s2 = tf.math.multiply(tf.clip_by_value(ratio, 1.0-self.clip_pram, 1.0 + self.clip_pram), t)\n",
    "            sur1.append(s1)\n",
    "            sur2.append(s2)\n",
    "        \n",
    "        sr1 = tf.stack(sur1)\n",
    "        sr2 = tf.stack(sur2)\n",
    "        \n",
    "        loss = tf.math.negative(tf.reduce_mean(tf.math.minimum(sr1, sr2)) - closs + 0.001 * entropy)\n",
    "        return loss\n",
    "    \n",
    "    def get_actor_loss(self, discnt_rewards, a):\n",
    "        return 0.5 * kls.mean_squared_error(discnt_rewards, a)\n",
    "    \n",
    "    def get_critic_loss(self, discnt_rewards, v):\n",
    "        return 0.5 * kls.mean_squared_error(discnt_rewards, v)\n",
    "    \n",
    "    def train_model(self):\n",
    "        batch = random.sample(self.memory, self.batch_size)\n",
    "        \n",
    "        history = np.array([sample[0] for sample in batch])\n",
    "        load = np.array([sample[1] for sample in batch])\n",
    "        remain_size = np.array([sample[2] for sample in batch])\n",
    "        load_size = np.array([sample[3] for sample in batch])\n",
    "        reward = np.array([sample[4] for sample in batch])\n",
    "        dones = np.array([sample[5] for sample in batch])\n",
    "        next_history = [sample[6] for sample in batch] \n",
    "        next_load = [sample[7] for sample in batch]\n",
    "        next_remain_size = [sample[8] for sample in batch] \n",
    "        next_load_size = [sample[9] for sample in batch] \n",
    "        \n",
    "        with tf.GradientTape() as actor_tape, tf.GradientTape() as critic_tape:\n",
    "            actor = self.actor([history, load, remain_size])\n",
    "            critic = self.critic([history, load, remain_size])\n",
    "            targets = []\n",
    "            for i in range(self.batch_size):\n",
    "                next_value = self.critic([next_history[i], next_load[i], next_remain_size[i]])\n",
    "                targets.append(next_value)\n",
    "            targets = np.array(targets)\n",
    "            targets = targets.reshape(-1, 1)\n",
    "            print(actor.shape, critic.shape, targets.shape)\n",
    "            actor_loss = self.get_actor_loss(targets, actor)\n",
    "            critic_loss = self.get_critic_loss(targets, value)\n",
    "                \n",
    "        actor_grads = actor_tape.gradient(actor_loss, self.actor.trainable_variables)\n",
    "        critic_grads = critic_tape.gradient(critic_loss, self.critic.trainable_variables)\n",
    "        self.actor_optimizer.apply_gradients(zip(actor_grads, self.actor.trainable_variables))\n",
    "        self.critic_optimizer.apply_gradients(zip(critic_grads, self.critic.trainable_variables))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "675cd31a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-08T16:23:32.579848Z",
     "start_time": "2021-05-08T16:23:32.569878Z"
    }
   },
   "outputs": [],
   "source": [
    "num_episode = 1500\n",
    "global_step = 0\n",
    "allow_skip = False\n",
    "tr_l, h_fill, tr_r, avg_actor_loss_l, avg_critic_loss_l, history_eps, used_boxes_eps = [],[],[],[],[],[],[]\n",
    "N_MDD = 7\n",
    "K = 4\n",
    "n_candidates = 4\n",
    "\n",
    "boxes_multi1 = [np.array([[20, 20,  4],\n",
    "         [20,  4,  4],\n",
    "         [20,  4,  4],\n",
    "         [20,  4,  4],\n",
    "         [20,  4,  4],\n",
    "         [20,  4,  4],\n",
    "         [20, 20,  4],\n",
    "         [20, 20,  4],\n",
    "         [20, 20,  4]])]\n",
    "gt_pos1 = [np.array([[ 0,  0,  0],\n",
    "         [ 0,  0,  4],\n",
    "         [ 0,  4,  4],\n",
    "         [ 0,  8,  4],\n",
    "         [ 0, 12,  4],\n",
    "         [ 0, 16,  4],\n",
    "         [ 0,  0,  8],\n",
    "         [ 0,  0, 12],\n",
    "         [ 0,  0, 16]])]\n",
    "\n",
    "boxes_multi2 = [np.array([[20, 20,  5],\n",
    "        [ 4, 20,  5],\n",
    "        [ 4, 20,  5],\n",
    "        [ 4, 20,  5],\n",
    "        [ 4, 20,  5],\n",
    "        [ 4, 20,  5],\n",
    "        [10, 20,  5],\n",
    "        [10, 20,  5],\n",
    "        [20, 20,  5]])]\n",
    "\n",
    "gt_pos2 = [np.array([[ 0,  0,  0],\n",
    "        [ 0,  0,  5],\n",
    "        [ 4,  0,  5],\n",
    "        [ 8,  0,  5],\n",
    "        [12,  0,  5],\n",
    "        [16,  0,  5],\n",
    "        [ 0,  0, 10],\n",
    "        [10,  0, 10],\n",
    "        [ 0,  0, 15]])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "caa89af3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-08T16:23:42.985815Z",
     "start_time": "2021-05-08T16:23:42.975864Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_max_boxes 9 num_max_remain 9\n"
     ]
    }
   ],
   "source": [
    "num_max_boxes = max(len(boxes_multi1[0]), len(boxes_multi2[0]))\n",
    "num_max_remain = num_max_boxes\n",
    "print('num_max_boxes',num_max_boxes,'num_max_remain',num_max_remain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "ecf9eb21",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-08T16:23:43.227643Z",
     "start_time": "2021-05-08T16:23:43.217918Z"
    }
   },
   "outputs": [],
   "source": [
    "env=Bpp3DEnv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "b5ff342c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-08T16:23:43.455350Z",
     "start_time": "2021-05-08T16:23:43.422809Z"
    }
   },
   "outputs": [],
   "source": [
    "agent = PPO_Agent(L=20, B=20, H=20, n_remains=num_max_remain, lr=1e-4, exp_steps=900,\n",
    "                train_st=500, memory_len=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "7367bad7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-08T16:23:44.421286Z",
     "start_time": "2021-05-08T16:23:44.400575Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[20, 20,  5],\n",
       "       [ 4, 20,  5],\n",
       "       [ 4, 20,  5],\n",
       "       [ 4, 20,  5],\n",
       "       [ 4, 20,  5],\n",
       "       [ 4, 20,  5],\n",
       "       [10, 20,  5],\n",
       "       [10, 20,  5],\n",
       "       [20, 20,  5]])"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "boxes_multi, gt_pos = boxes_multi2.copy(), gt_pos2.copy()\n",
    "env.reset()\n",
    "done = False\n",
    "step = 0\n",
    "\n",
    "history, h_load, h_remain_size, h_load_size = [],[],[],[]\n",
    "next_history, next_load, next_remain_size, next_load_size = [],[],[],[]\n",
    "used_boxes, pred_pos = [],[]\n",
    "\n",
    "boxes_all = np.array(boxes_multi)[0].copy()\n",
    "r_boxes = boxes_all.copy()\n",
    "r_boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "7783e2cb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-08T16:23:44.617652Z",
     "start_time": "2021-05-08T16:23:44.609783Z"
    }
   },
   "outputs": [],
   "source": [
    "q_list, arg_list, action_list = [], [], []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "e7a810cd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-08T16:23:45.834970Z",
     "start_time": "2021-05-08T16:23:44.793974Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action\n",
      "tf.Tensor(\n",
      "[[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]], shape=(4, 1), dtype=float32) (array([0, 1, 2, 3], dtype=int64), array([0, 0, 0, 0], dtype=int64)) 0\n",
      "Action\n",
      "tf.Tensor(\n",
      "[[1.]\n",
      " [1.]], shape=(2, 1), dtype=float32) (array([0, 1], dtype=int64), array([0, 0], dtype=int64)) 0\n",
      "Action\n",
      "tf.Tensor(\n",
      "[[1.]\n",
      " [1.]], shape=(2, 1), dtype=float32) (array([0, 1], dtype=int64), array([0, 0], dtype=int64)) 0\n"
     ]
    }
   ],
   "source": [
    "while not done:\n",
    "        state = env.container.copy()\n",
    "        state_h = env.update_h().copy()\n",
    "        step += 1\n",
    "        \n",
    "        k = min(K, len(r_boxes))\n",
    "        selected = cbn_select_boxes(r_boxes[:n_candidates], k)\n",
    "        s_order = get_selected_order(selected, k)\n",
    "        s_loc_c, num_loaded_box_c, loading_size_c, loading_pos_c, next_cube_c , next_state_c = get_selected_location(s_order, state) \n",
    "        loaded_mh_c = np.array([get_loaded_mh(s_loc, env.length, env.breadth, env.height) for s_loc in s_loc_c] ) # 3D -> 2D\n",
    "        in_state, in_r_boxes, in_loading = raw2input(state_h, len(s_loc_c), r_boxes, num_max_remain, K, loading_size_c, env.height)\n",
    "        \n",
    "        s_order, s_loc_c, num_loaded_box_c, loading_size_c, loading_pos_c, next_cube_c , next_state_c, loaded_mh_c, in_state, in_r_boxes, in_loading =\\\n",
    "            get_unique(s_order, s_loc_c, num_loaded_box_c, loading_size_c, loading_pos_c, next_cube_c , next_state_c, loaded_mh_c, in_state, in_r_boxes, in_loading)\n",
    "        \n",
    "        if len(s_loc_c) == 1:\n",
    "            action_idx = 0\n",
    "        else:\n",
    "            q, arg, action_idx = agent.get_action(in_state, loaded_mh_c, in_r_boxes)\n",
    "        print('Action')\n",
    "        print(q, arg, action_idx)\n",
    "        q_list.append(q)\n",
    "        arg_list.append(arg)\n",
    "        action_list.append(action_idx)\n",
    "        \n",
    "        env.convert_state(next_cube_c[action_idx]) \n",
    "        num_loaded_box = num_loaded_box_c[action_idx]\n",
    "        if num_loaded_box != 0:\n",
    "            new_used_boxes = loading_size_c[action_idx]\n",
    "            r_boxes = get_remain(new_used_boxes, r_boxes)\n",
    "        else:\n",
    "            r_boxes = get_remain(s_order[action_idx], r_boxes)\n",
    "        used_boxes = used_boxes + loading_size_c[action_idx]\n",
    "        pred_pos = pred_pos + loading_pos_c[action_idx]  \n",
    "        if len(r_boxes) == 0 or np.sum(env.container_h != env.height) == 0:\n",
    "            done = True\n",
    "\n",
    "        if len(s_loc_c) != 1 or done:\n",
    "            history.append(in_state[action_idx])\n",
    "            h_load.append(loaded_mh_c[action_idx])\n",
    "            h_remain_size.append(in_r_boxes[action_idx])\n",
    "            h_load_size.append(in_loading[action_idx])\n",
    "            \n",
    "            next_state = env.container.copy()\n",
    "            next_state_h = env.container_h.copy() \n",
    "            if done:\n",
    "                in_next_history = next_state_h.reshape((1, env.length, env.breadth, 1))\n",
    "                loaded_mh_c = np.zeros((1, env.length, env.breadth, 2))\n",
    "                in_next_remains = np.zeros((1, env.length, env.breadth, num_max_remain))\n",
    "                in_next_loading = np.zeros((1, env.length, env.breadth, K))\n",
    "            else:\n",
    "                k = min(K, len(r_boxes))\n",
    "                selected = cbn_select_boxes(r_boxes[:n_candidates], k)\n",
    "                s_order = get_selected_order(selected, k)\n",
    "                s_loc_c, num_loaded_box_c, loading_size_c, loading_pos_c, next_cube_c , next_state_c  =\\\n",
    "                    get_selected_location(s_order, next_state)\n",
    "                loaded_mh_c = np.array( [get_loaded_mh(s_loc, env.length, env.breadth, env.height) for s_loc in s_loc_c] )\n",
    "                in_next_history, in_next_remains, in_next_loading =\\\n",
    "                    raw2input(next_state_h, len(s_loc_c), r_boxes, num_max_remain,  K, loading_size_c, env.height)\n",
    "\n",
    "            s_order, s_loc_c, num_loaded_box_c, loading_size_c, loading_pos_c, next_cube_c , next_state_c, loaded_mh_c, in_next_history, in_next_remains, in_next_loading =\\\n",
    "                get_unique(s_order, s_loc_c, num_loaded_box_c, loading_size_c, loading_pos_c, next_cube_c , next_state_c, loaded_mh_c, in_next_history, in_next_remains, in_next_loading)\n",
    "\n",
    "            next_history.append(in_next_history)\n",
    "            next_load.append(loaded_mh_c)\n",
    "            next_remain_size.append(in_next_remains)\n",
    "            next_load_size.append(in_next_loading)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "12358e29",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-08T16:23:57.242090Z",
     "start_time": "2021-05-08T16:23:57.229711Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "5d8eda89",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-08T16:23:58.089648Z",
     "start_time": "2021-05-08T16:23:58.078789Z"
    }
   },
   "outputs": [],
   "source": [
    "avg_tr = 0 if len(tr_r)==0 else np.mean(tr_r)\n",
    "terminal_reward = env.terminal_reward()\n",
    "tr_l.append(terminal_reward)\n",
    "h_fill.append(env.terminal_reward())\n",
    "tr_r.append(env.terminal_reward())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "c807bdda",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-08T16:23:58.391038Z",
     "start_time": "2021-05-08T16:23:58.374984Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "avg_tr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "26a94c4e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-08T16:23:59.688306Z",
     "start_time": "2021-05-08T16:23:59.670722Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.75"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "terminal_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "7f3ba406",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-08T16:24:05.001910Z",
     "start_time": "2021-05-08T16:24:04.980966Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 : 0.7350749999999999 : 1\n",
      "1 : 0.7424999999999999 : 1\n",
      "2 : 0.75 : 1\n"
     ]
    }
   ],
   "source": [
    "a_repeate = 6 if env.terminal_reward() ==1.0 else 1\n",
    "is_last = False\n",
    "N = len(history)\n",
    "for i in range(N):\n",
    "    if i == N-1: is_last=True\n",
    "    reward=(0.99**(N-i-1))*terminal_reward\n",
    "    print(i, ':', reward, ':', a_repeate)\n",
    "    for a in range(a_repeate):\n",
    "        agent.append_sample(history[i], h_load[i], h_remain_size[i], h_load_size[i], reward, is_last,\n",
    "                            next_history[i], next_load[i], next_remain_size[i], next_load_size[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "d37f4d57",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-08T16:24:08.541837Z",
     "start_time": "2021-05-08T16:24:08.526879Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(agent.memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "4d463dc0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-08T16:41:44.791592Z",
     "start_time": "2021-05-08T16:41:44.776634Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "batch = random.sample(agent.memory, 3)\n",
    "        \n",
    "history = np.array([sample[0] for sample in batch])\n",
    "load = np.array([sample[1] for sample in batch])\n",
    "remain_size = np.array([sample[2] for sample in batch])\n",
    "load_size = np.array([sample[3] for sample in batch])\n",
    "reward = np.array([sample[4] for sample in batch])\n",
    "dones = np.array([sample[5] for sample in batch])\n",
    "next_history = [sample[6] for sample in batch] \n",
    "next_load = [sample[7] for sample in batch]\n",
    "next_remain_size = [sample[8] for sample in batch] \n",
    "next_load_size = [sample[9] for sample in batch] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "97cb60d3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-08T16:41:45.803774Z",
     "start_time": "2021-05-08T16:41:45.797756Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 20, 20, 1)"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "a0c56cad",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-08T16:41:47.225453Z",
     "start_time": "2021-05-08T16:41:47.208011Z"
    }
   },
   "outputs": [],
   "source": [
    "vmin = 0\n",
    "vmax = 1\n",
    "nsup = 51"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "eedbd4e6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-08T16:41:47.405907Z",
     "start_time": "2021-05-08T16:41:47.393269Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.  , 0.02, 0.04, 0.06, 0.08, 0.1 , 0.12, 0.14, 0.16, 0.18, 0.2 ,\n",
       "       0.22, 0.24, 0.26, 0.28, 0.3 , 0.32, 0.34, 0.36, 0.38, 0.4 , 0.42,\n",
       "       0.44, 0.46, 0.48, 0.5 , 0.52, 0.54, 0.56, 0.58, 0.6 , 0.62, 0.64,\n",
       "       0.66, 0.68, 0.7 , 0.72, 0.74, 0.76, 0.78, 0.8 , 0.82, 0.84, 0.86,\n",
       "       0.88, 0.9 , 0.92, 0.94, 0.96, 0.98, 1.  ])"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z = np.linspace(vmin,vmax,nsup)\n",
    "z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "49b55ff6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-08T16:41:48.161812Z",
     "start_time": "2021-05-08T16:41:48.150814Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.02"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dz = (vmax - vmin) / (nsup - 1.)\n",
    "dz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "a9cf3593",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-08T16:45:43.401790Z",
     "start_time": "2021-05-08T16:45:43.335896Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actor\n",
      "tf.Tensor(\n",
      "[[1.]\n",
      " [1.]\n",
      " [1.]], shape=(3, 1), dtype=float32) \n",
      "\n",
      "Critic\n",
      "tf.Tensor(\n",
      "[[1.]\n",
      " [1.]\n",
      " [1.]], shape=(3, 1), dtype=float32) \n",
      "\n",
      "Next Critic\n",
      "\n",
      "targets\n",
      "[[<tf.Tensor: shape=(), dtype=float32, numpy=0.93376875>], [<tf.Tensor: shape=(), dtype=float32, numpy=0.1875>], [<tf.Tensor: shape=(), dtype=float32, numpy=0.935625>]]\n",
      "Act Loss: tf.Tensor([0.00219329 0.33007812 0.00207207], shape=(3,), dtype=float32)\n",
      "Crt Loss: tf.Tensor([0.00219329 0.33007812 0.00207207], shape=(3,), dtype=float32)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with tf.GradientTape() as actor_tape, tf.GradientTape() as critic_tape:\n",
    "    actor = agent.actor([history, load, remain_size])\n",
    "    value = agent.critic([history, load, remain_size])\n",
    "    #print(value.shape)\n",
    "    print('Actor')\n",
    "    print(actor, '\\n')\n",
    "    print('Critic')\n",
    "    print(value, '\\n')\n",
    "    targets = []\n",
    "    print('Next Critic')\n",
    "    for i in range(3):\n",
    "        #print(next_history[i].shape, next_load[i].shape, next_remain_size[i].shape)\n",
    "        next_value = agent.critic([next_history[i], next_load[i], next_remain_size[i]])\n",
    "        t_max_q = tf.math.reduce_max(next_value)\n",
    "        t = [(1- 0.75)*reward[i] + (1 - dones[i]) *0.75*t_max_q]\n",
    "        targets.append(t)\n",
    "    #targets = np.array(targets)\n",
    "    #targets = targets.reshape(-1, 1)\n",
    "    print('\\ntargets')\n",
    "    print(targets)\n",
    "    #print(actor.shape, value.shape, targets.shape)\n",
    "    actor_loss = agent.get_actor_loss(targets, actor)\n",
    "    critic_loss = agent.get_critic_loss(targets, value)\n",
    "    print('Act Loss:', actor_loss)\n",
    "    print('Crt Loss:', critic_loss)\n",
    "    print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "5e3b835f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-08T16:45:51.363299Z",
     "start_time": "2021-05-08T16:45:51.319510Z"
    }
   },
   "outputs": [],
   "source": [
    "actor_grads = actor_tape.gradient(actor_loss, agent.actor.trainable_variables)\n",
    "critic_grads = critic_tape.gradient(critic_loss, agent.critic.trainable_variables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "d9c92c78",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-08T16:45:52.282086Z",
     "start_time": "2021-05-08T16:45:52.224983Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Variable 'UnreadVariable' shape=() dtype=int64, numpy=1>"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.actor_optimizer.apply_gradients(zip(actor_grads, agent.actor.trainable_variables))\n",
    "agent.critic_optimizer.apply_gradients(zip(critic_grads, agent.critic.trainable_variables))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "ac1c5458",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-08T16:45:54.101985Z",
     "start_time": "2021-05-08T16:45:54.030204Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actor\n",
      "tf.Tensor(\n",
      "[[1.]\n",
      " [1.]\n",
      " [1.]], shape=(3, 1), dtype=float32)\n",
      "Critic\n",
      "tf.Tensor([[1.]], shape=(1, 1), dtype=float32)\n",
      "Next Critic\n",
      "0 tf.Tensor(\n",
      "[[1.]\n",
      " [1.]], shape=(2, 1), dtype=float32)\n",
      "targets\n",
      "[[1.]\n",
      " [1.]]\n"
     ]
    },
    {
     "ename": "InvalidArgumentError",
     "evalue": "Incompatible shapes: [3,1] vs. [2,1] [Op:SquaredDifference]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-184-15830e5b5252>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     16\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'targets'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtargets\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 18\u001b[1;33m     \u001b[0mactor_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0magent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_actor_loss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtargets\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mactor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     19\u001b[0m     \u001b[0mcritic_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0magent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_critic_loss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtargets\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Act Loss:'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mactor_loss\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-44-49a35de972fc>\u001b[0m in \u001b[0;36mget_actor_loss\u001b[1;34m(self, discnt_rewards, a)\u001b[0m\n\u001b[0;32m     54\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     55\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget_actor_loss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdiscnt_rewards\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ma\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 56\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[1;36m0.5\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mkls\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean_squared_error\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdiscnt_rewards\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ma\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     57\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     58\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget_critic_loss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdiscnt_rewards\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mv\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\RL\\lib\\site-packages\\tensorflow\\python\\util\\dispatch.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    199\u001b[0m     \u001b[1;34m\"\"\"Call target, and fall back on dispatchers if there is a TypeError.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    200\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 201\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    202\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    203\u001b[0m       \u001b[1;31m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\RL\\lib\\site-packages\\tensorflow\\python\\keras\\losses.py\u001b[0m in \u001b[0;36mmean_squared_error\u001b[1;34m(y_true, y_pred)\u001b[0m\n\u001b[0;32m   1196\u001b[0m   \u001b[0my_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconvert_to_tensor_v2_with_dispatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1197\u001b[0m   \u001b[0my_true\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmath_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcast\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1198\u001b[1;33m   \u001b[1;32mreturn\u001b[0m \u001b[0mK\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmath_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msquared_difference\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_true\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1199\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1200\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\RL\\lib\\site-packages\\tensorflow\\python\\ops\\gen_math_ops.py\u001b[0m in \u001b[0;36msquared_difference\u001b[1;34m(x, y, name)\u001b[0m\n\u001b[0;32m  10230\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m  10231\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m> 10232\u001b[1;33m       \u001b[0m_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mraise_from_not_ok_status\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m  10233\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_FallbackException\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m  10234\u001b[0m       \u001b[1;32mpass\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\RL\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\u001b[0m in \u001b[0;36mraise_from_not_ok_status\u001b[1;34m(e, name)\u001b[0m\n\u001b[0;32m   6860\u001b[0m   \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;34m\" name: \"\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;34m\"\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   6861\u001b[0m   \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 6862\u001b[1;33m   \u001b[0msix\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mraise_from\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_status_to_exception\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   6863\u001b[0m   \u001b[1;31m# pylint: enable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   6864\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\RL\\lib\\site-packages\\six.py\u001b[0m in \u001b[0;36mraise_from\u001b[1;34m(value, from_value)\u001b[0m\n",
      "\u001b[1;31mInvalidArgumentError\u001b[0m: Incompatible shapes: [3,1] vs. [2,1] [Op:SquaredDifference]"
     ]
    }
   ],
   "source": [
    "with tf.GradientTape() as actor_tape, tf.GradientTape() as critic_tape:\n",
    "    actor = agent.actor([history, load, remain_size])\n",
    "    value = agent.critic([history, load, remain_size])\n",
    "    print('Actor')\n",
    "    print(actor)\n",
    "    print('Critic')\n",
    "    print(critic)\n",
    "    targets = []\n",
    "    print('Next Critic')\n",
    "    for i in range(1):\n",
    "        next_value = agent.critic([next_history[i], next_load[i], next_remain_size[i]])\n",
    "        print(i, next_value)\n",
    "        targets.append(next_value)\n",
    "    targets = np.array(targets)\n",
    "    targets = targets.reshape(-1, 1)\n",
    "    print('targets')\n",
    "    print(targets)\n",
    "    actor_loss = agent.get_actor_loss(targets, actor)\n",
    "    critic_loss = agent.get_critic_loss(targets, value)\n",
    "    print('Act Loss:', actor_loss)\n",
    "    print('Crt Loss:', critic_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93ce1099",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RL",
   "language": "python",
   "name": "rl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
