{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "6a409700",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b283f812",
   "metadata": {},
   "outputs": [],
   "source": [
    "from autokeras.keras_layers import ExpandLastDim\n",
    "from autokeras.keras_layers import CastToFloat32\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, GlobalMaxPooling2D, GlobalAveragePooling2D, LSTM, SimpleRNN\n",
    "from tensorflow.keras.layers import InputLayer, Dense, Flatten, Dropout\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "from tensorflow.keras.layers.experimental.preprocessing import RandomTranslation, RandomFlip\n",
    "from tensorflow.keras.layers.experimental.preprocessing import Resizing\n",
    "from tensorflow.keras.layers import Concatenate\n",
    "from tensorflow.keras.applications import EfficientNetB7\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(action='ignore')\n",
    "\n",
    "import tensorflow as tf\n",
    "tf.get_logger().setLevel('ERROR')\n",
    "gpu_devices = tf.config.experimental.list_physical_devices(\"GPU\")\n",
    "for device in gpu_devices:\n",
    "    tf.config.experimental.set_memory_growth(device, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bfcb936",
   "metadata": {},
   "source": [
    "# Make Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "d959d193",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def calculateWeight(tlx):\n",
    "    tlx_weight = {'Mental':[0], \n",
    "                  'Physical':[0], \n",
    "                  'Temporal':[0], \n",
    "                  'Effort':[0],\n",
    "                  'Performance':[0],\n",
    "                  'Frustration':[0],\n",
    "                  'Sum':[0]}\n",
    "    tlx_weight = pd.DataFrame(tlx_weight)\n",
    "    for i in range(len(tlx)):\n",
    "        score = [0,0,0,0,0,0,0]\n",
    "        for col1 in range(1,len(tlx.columns)):\n",
    "            for col2 in range(col1+1, len(tlx.columns)):\n",
    "                if tlx[tlx.columns[col1]][i] > tlx[tlx.columns[col2]][i]:\n",
    "                    score[col1-1]+=1\n",
    "                elif tlx[tlx.columns[col1]][i] < tlx[tlx.columns[col2]][i]:\n",
    "                    score[col2-1]+=1\n",
    "                else :\n",
    "                    score[col1-1]+=0.5\n",
    "                    score[col2-1]+=0.5\n",
    "                    \n",
    "        score[6] = score[0]+score[1]+score[2]+score[3]+score[4]+score[5]\n",
    "        tlx_weight.loc[i]=score\n",
    "    #print(tlx_weight.loc[0])\n",
    "    return tlx_weight\n",
    "\n",
    "def calculate_tlxLevel(tlx, tlx_weight):\n",
    "    result = {'Mental':[0], \n",
    "                  'Physical':[0], \n",
    "                  'Temporal':[0], \n",
    "                  'Effort':[0],\n",
    "                  'Performance':[0],\n",
    "                  'Frustration':[0],\n",
    "                  'Score':[0]}\n",
    "    result = pd.DataFrame(result)\n",
    "    for i in range(len(tlx)):\n",
    "        score = [0,0,0,0,0,0,0]\n",
    "        for col in range(len(tlx_weight.columns)-1):\n",
    "            score[col] = int(tlx[tlx.columns[col+1]].loc[i] * tlx_weight[tlx_weight.columns[col]].loc[i] )\n",
    "        score[6] =int((score[0]+score[1]+score[2]+score[3]+score[4]+score[5] )/ tlx_weight[tlx_weight.columns[6]].loc[i]/10 + 0.5)\n",
    "        if score[6]>10: score[6]=10\n",
    "        if score[6]<0: score[6]=0\n",
    "        result.loc[i]=score\n",
    "    return result['Score']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "84b86490",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_fft_data(df, label, end):\n",
    "    col = ['POW.AF3.Theta', 'POW.AF3.Alpha',\n",
    "       'POW.AF3.BetaL', 'POW.AF3.BetaH', 'POW.AF3.Gamma', 'POW.F7.Theta',\n",
    "       'POW.F7.Alpha', 'POW.F7.BetaL', 'POW.F7.BetaH', 'POW.F7.Gamma',\n",
    "       'POW.F3.Theta', 'POW.F3.Alpha', 'POW.F3.BetaL', 'POW.F3.BetaH',\n",
    "       'POW.F3.Gamma', 'POW.FC5.Theta', 'POW.FC5.Alpha', 'POW.FC5.BetaL',\n",
    "       'POW.FC5.BetaH', 'POW.FC5.Gamma', 'POW.T7.Theta', 'POW.T7.Alpha',\n",
    "       'POW.T7.BetaL', 'POW.T7.BetaH', 'POW.T7.Gamma', 'POW.P7.Theta',\n",
    "       'POW.P7.Alpha', 'POW.P7.BetaL', 'POW.P7.BetaH', 'POW.P7.Gamma',\n",
    "       'POW.O1.Theta', 'POW.O1.Alpha', 'POW.O1.BetaL', 'POW.O1.BetaH',\n",
    "       'POW.O1.Gamma', 'POW.O2.Theta', 'POW.O2.Alpha', 'POW.O2.BetaL',\n",
    "       'POW.O2.BetaH', 'POW.O2.Gamma', 'POW.P8.Theta', 'POW.P8.Alpha',\n",
    "       'POW.P8.BetaL', 'POW.P8.BetaH', 'POW.P8.Gamma', 'POW.T8.Theta',\n",
    "       'POW.T8.Alpha', 'POW.T8.BetaL', 'POW.T8.BetaH', 'POW.T8.Gamma',\n",
    "       'POW.FC6.Theta', 'POW.FC6.Alpha', 'POW.FC6.BetaL', 'POW.FC6.BetaH',\n",
    "       'POW.FC6.Gamma', 'POW.F4.Theta', 'POW.F4.Alpha', 'POW.F4.BetaL',\n",
    "       'POW.F4.BetaH', 'POW.F4.Gamma', 'POW.F8.Theta', 'POW.F8.Alpha',\n",
    "       'POW.F8.BetaL', 'POW.F8.BetaH', 'POW.F8.Gamma', 'POW.AF4.Theta',\n",
    "       'POW.AF4.Alpha', 'POW.AF4.BetaL', 'POW.AF4.BetaH', 'POW.AF4.Gamma', 'MarkerValueInt']\n",
    "    col_rename = ['AF3.Theta', 'AF3.Alpha','AF3.BetaL', 'AF3.BetaH', 'AF3.Gamma', \n",
    "                  'F7.Theta','F7.Alpha', 'F7.BetaL', 'F7.BetaH', 'F7.Gamma',\n",
    "                  'F3.Theta', 'F3.Alpha', 'F3.BetaL', 'F3.BetaH','F3.Gamma', \n",
    "                  'FC5.Theta', 'FC5.Alpha', 'FC5.BetaL','FC5.BetaH', 'FC5.Gamma', \n",
    "                  'T7.Theta', 'T7.Alpha', 'T7.BetaL', 'T7.BetaH', 'T7.Gamma', \n",
    "                  'P7.Theta', 'P7.Alpha', 'P7.BetaL', 'P7.BetaH', 'P7.Gamma',\n",
    "                  'O1.Theta', 'O1.Alpha', 'O1.BetaL', 'O1.BetaH','O1.Gamma',\n",
    "                  'O2.Theta', 'O2.Alpha', 'O2.BetaL','O2.BetaH', 'O2.Gamma',\n",
    "                  'P8.Theta', 'P8.Alpha', 'P8.BetaL', 'P8.BetaH', 'P8.Gamma', \n",
    "                  'T8.Theta','T8.Alpha', 'T8.BetaL', 'T8.BetaH', 'T8.Gamma',\n",
    "                  'FC6.Theta', 'FC6.Alpha', 'FC6.BetaL', 'FC6.BetaH','FC6.Gamma', \n",
    "                  'F4.Theta', 'F4.Alpha', 'F4.BetaL','F4.BetaH', 'F4.Gamma',\n",
    "                  'F8.Theta', 'F8.Alpha','F8.BetaL', 'F8.BetaH', 'F8.Gamma',\n",
    "                  'AF4.Theta', 'AF4.Alpha', 'AF4.BetaL', 'AF4.BetaH', 'AF4.Gamma', 'vis_name']\n",
    "    data_extraction = df[col]\n",
    "    data_extraction.columns = col_rename\n",
    "    rest = data_extraction[data_extraction.vis_name == 0].dropna(axis=0)\n",
    "    survey = data_extraction[data_extraction.vis_name == 100].dropna(axis=0)\n",
    "    \n",
    "    vis = data_extraction[data_extraction.vis_name == 1].reset_index(drop=True).dropna(axis=0)\n",
    "    vis['label'] = label[0]\n",
    "    vis.drop(['vis_name'], axis=1, inplace=True)\n",
    "    \n",
    "    for i in range(2,end):\n",
    "        df = data_extraction[data_extraction.vis_name == i].reset_index(drop=True)\n",
    "        df['label'] = label[i-1]\n",
    "        df.drop(['vis_name'], axis=1, inplace=True)\n",
    "        df = df.dropna(axis=0)\n",
    "        vis = pd.concat([vis,df], ignore_index=True, axis=0)\n",
    "    return rest, survey, vis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "e83195eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "src = 'C:/EEG data/User/'\n",
    "files = os.listdir(src)\n",
    "tlx=[]\n",
    "for f in files:\n",
    "    tlx.append(pd.read_csv(src+f))\n",
    "src = 'C:/EEG data/prepross/'\n",
    "datas = os.listdir(src)\n",
    "workloadLevel = []\n",
    "for t in tlx:\n",
    "    workloadLevel.append(calculate_tlxLevel(t, calculateWeight(t)))\n",
    "eegData=[]\n",
    "for d in datas:\n",
    "    eegData.append(pd.read_csv(src+d))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "980e2714",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnt = 0\n",
    "for eeg, label in zip (eegData, workloadLevel):\n",
    "    if cnt == 0 :\n",
    "        rest, survey, vis = split_fft_data(eeg, label,21)\n",
    "        cnt+=1\n",
    "        continue\n",
    "    elif cnt == 6:\n",
    "        r, s, v = split_fft_data(eeg, label,21)\n",
    "    else:\n",
    "        r, s, v = split_fft_data(eeg, label, 22)\n",
    "    \n",
    "    rest = pd.concat([rest,r], ignore_index=True, axis=0)\n",
    "    survey = pd.concat([survey,s], ignore_index=True, axis=0)\n",
    "    vis = pd.concat([vis,v], ignore_index=True, axis=0)\n",
    "    cnt+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "baaa88c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(108212, 70) (108212, 11)\n"
     ]
    }
   ],
   "source": [
    "label=vis['label']\n",
    "label = to_categorical(label,11)\n",
    "data=vis.drop(['label'],axis=1)\n",
    "scaler = MinMaxScaler()\n",
    "scaled = scaler.fit_transform(data)\n",
    "data = pd.DataFrame(scaled, columns = data.columns, index=data.index)\n",
    "data\n",
    "print(data.shape, label.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cf544e8",
   "metadata": {},
   "source": [
    "# AUTO KERAS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "3994b3c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import autokeras as ak\n",
    "from tensorflow.keras.utils import plot_model\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "6730a961",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64927, 70) (43285, 70) (64927, 11) (43285, 11)\n"
     ]
    }
   ],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(data,label, train_size=0.6, \n",
    "                                                    random_state=True,\n",
    "                                                    stratify = label)\n",
    "\n",
    "print(x_train.shape, x_test.shape,y_train.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "66ddc034",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 50 Complete [00h 01m 06s]\n",
      "val_accuracy: 0.6862412095069885\n",
      "\n",
      "Best val_accuracy So Far: 0.7935025691986084\n",
      "Total elapsed time: 01h 02m 23s\n",
      "Epoch 1/20\n",
      "2029/2029 [==============================] - 9s 4ms/step - loss: 1.5322 - accuracy: 0.4160\n",
      "Epoch 2/20\n",
      "2029/2029 [==============================] - 8s 4ms/step - loss: 1.1952 - accuracy: 0.5227\n",
      "Epoch 3/20\n",
      "2029/2029 [==============================] - 8s 4ms/step - loss: 1.0249 - accuracy: 0.5873\n",
      "Epoch 4/20\n",
      "2029/2029 [==============================] - 8s 4ms/step - loss: 0.9055 - accuracy: 0.6350\n",
      "Epoch 5/20\n",
      "2029/2029 [==============================] - 8s 4ms/step - loss: 0.8111 - accuracy: 0.6760\n",
      "Epoch 6/20\n",
      "2029/2029 [==============================] - 8s 4ms/step - loss: 0.7264 - accuracy: 0.7118\n",
      "Epoch 7/20\n",
      "2029/2029 [==============================] - 8s 4ms/step - loss: 0.6544 - accuracy: 0.7435\n",
      "Epoch 8/20\n",
      "2029/2029 [==============================] - 8s 4ms/step - loss: 0.5906 - accuracy: 0.7723\n",
      "Epoch 9/20\n",
      "2029/2029 [==============================] - 8s 4ms/step - loss: 0.5320 - accuracy: 0.7973\n",
      "Epoch 10/20\n",
      "2029/2029 [==============================] - 8s 4ms/step - loss: 0.4804 - accuracy: 0.8208\n",
      "Epoch 11/20\n",
      "2029/2029 [==============================] - 8s 4ms/step - loss: 0.4334 - accuracy: 0.8411\n",
      "Epoch 12/20\n",
      "2029/2029 [==============================] - 8s 4ms/step - loss: 0.3866 - accuracy: 0.8600\n",
      "Epoch 13/20\n",
      "2029/2029 [==============================] - 8s 4ms/step - loss: 0.3471 - accuracy: 0.8767\n",
      "Epoch 14/20\n",
      "2029/2029 [==============================] - 8s 4ms/step - loss: 0.3115 - accuracy: 0.8906\n",
      "Epoch 15/20\n",
      "2029/2029 [==============================] - 8s 4ms/step - loss: 0.2816 - accuracy: 0.9029\n",
      "Epoch 16/20\n",
      "2029/2029 [==============================] - 8s 4ms/step - loss: 0.2540 - accuracy: 0.9148\n",
      "Epoch 17/20\n",
      "2029/2029 [==============================] - 8s 4ms/step - loss: 0.2315 - accuracy: 0.9242\n",
      "Epoch 18/20\n",
      "2029/2029 [==============================] - 8s 4ms/step - loss: 0.2120 - accuracy: 0.9327\n",
      "Epoch 19/20\n",
      "2029/2029 [==============================] - 8s 4ms/step - loss: 0.1961 - accuracy: 0.9393\n",
      "Epoch 20/20\n",
      "2029/2029 [==============================] - 8s 4ms/step - loss: 0.1840 - accuracy: 0.9442\n",
      "1353/1353 [==============================] - 2s 2ms/step\n",
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 1. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 1.]]\n",
      "1353/1353 [==============================] - 2s 2ms/step - loss: 0.5769 - accuracy: 0.8371\n",
      "1353/1353 [==============================] - 3s 2ms/step - loss: 0.5769 - accuracy: 0.8371\n",
      "[0.5768832564353943, 0.8371260166168213]\n",
      "Loss: 0.577   Accuracy: 0.837\n",
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 70)]              0         \n",
      "_________________________________________________________________\n",
      "multi_category_encoding (Mul (None, 70)                0         \n",
      "_________________________________________________________________\n",
      "normalization (Normalization (None, 70)                141       \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 1024)              72704     \n",
      "_________________________________________________________________\n",
      "re_lu (ReLU)                 (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 512)               524800    \n",
      "_________________________________________________________________\n",
      "re_lu_1 (ReLU)               (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 11)                5643      \n",
      "_________________________________________________________________\n",
      "classification_head_1 (Softm (None, 11)                0         \n",
      "=================================================================\n",
      "Total params: 603,288\n",
      "Trainable params: 603,147\n",
      "Non-trainable params: 141\n",
      "_________________________________________________________________\n",
      "('You must install pydot (`pip install pydot`) and install graphviz (see instructions at https://graphviz.gitlab.io/download/) ', 'for plot_model/model_to_dot to work.')\n",
      "83\n"
     ]
    }
   ],
   "source": [
    "trials=[50]\n",
    "for trial in trials:\n",
    "    clf_ = ak.StructuredDataClassifier(overwrite=True, max_trials=trial)\n",
    "    clf_.fit(x=x_train, y=y_train, epochs=20)\n",
    "\n",
    "    predicted_y = clf_.predict(x_test)\n",
    "    print(predicted_y)\n",
    "    loss, acc = clf_.evaluate(x_test, y_test)\n",
    "    print(clf_.evaluate(x_test, y_test))\n",
    "    print('Loss: %.3f   Accuracy: %.3f' % (loss,acc))\n",
    "\n",
    "    model = clf_.export_model()\n",
    "    model.summary()\n",
    "    plot_model(model, show_shapes=True)\n",
    "    tmp = int(acc*100)\n",
    "    print(tmp)\n",
    "    model.save('model/'+'FFT_ACC__'+str(tmp)+'_try_'+str(trial), save_format='tf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4afac19f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py37",
   "language": "python",
   "name": "py37"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
