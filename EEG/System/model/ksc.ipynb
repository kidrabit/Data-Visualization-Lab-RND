{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6a409700",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b283f812",
   "metadata": {},
   "outputs": [],
   "source": [
    "from autokeras.keras_layers import ExpandLastDim\n",
    "from autokeras.keras_layers import CastToFloat32\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, GlobalMaxPooling2D, GlobalAveragePooling2D, LSTM, SimpleRNN\n",
    "from tensorflow.keras.layers import InputLayer, Dense, Flatten, Dropout\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "from tensorflow.keras.layers.experimental.preprocessing import RandomTranslation, RandomFlip\n",
    "from tensorflow.keras.layers.experimental.preprocessing import Resizing\n",
    "from tensorflow.keras.layers import Concatenate\n",
    "from tensorflow.keras.applications import EfficientNetB7\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(action='ignore')\n",
    "\n",
    "import tensorflow as tf\n",
    "tf.get_logger().setLevel('ERROR')\n",
    "gpu_devices = tf.config.experimental.list_physical_devices(\"GPU\")\n",
    "for device in gpu_devices:\n",
    "    tf.config.experimental.set_memory_growth(device, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bfcb936",
   "metadata": {},
   "source": [
    "# Make Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "d959d193",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def calculateWeight(tlx):\n",
    "    tlx_weight = {'Mental':[0], \n",
    "                  'Physical':[0], \n",
    "                  'Temporal':[0], \n",
    "                  'Effort':[0],\n",
    "                  'Performance':[0],\n",
    "                  'Frustration':[0],\n",
    "                  'Sum':[0]}\n",
    "    tlx_weight = pd.DataFrame(tlx_weight)\n",
    "    for i in range(len(tlx)):\n",
    "        score = [0,0,0,0,0,0,0]\n",
    "        for col1 in range(1,len(tlx.columns)):\n",
    "            for col2 in range(col1+1, len(tlx.columns)):\n",
    "                if tlx[tlx.columns[col1]][i] > tlx[tlx.columns[col2]][i]:\n",
    "                    score[col1-1]+=1\n",
    "                elif tlx[tlx.columns[col1]][i] < tlx[tlx.columns[col2]][i]:\n",
    "                    score[col2-1]+=1\n",
    "                else :\n",
    "                    score[col1-1]+=0.5\n",
    "                    score[col2-1]+=0.5\n",
    "                    \n",
    "        score[6] = score[0]+score[1]+score[2]+score[3]+score[4]+score[5]\n",
    "        tlx_weight.loc[i]=score\n",
    "    #print(tlx_weight.loc[0])\n",
    "    return tlx_weight\n",
    "\n",
    "def calculate_tlxLevel(tlx, tlx_weight):\n",
    "    result = {'Mental':[0], \n",
    "                  'Physical':[0], \n",
    "                  'Temporal':[0], \n",
    "                  'Effort':[0],\n",
    "                  'Performance':[0],\n",
    "                  'Frustration':[0],\n",
    "                  'Score':[0]}\n",
    "    result = pd.DataFrame(result)\n",
    "    for i in range(len(tlx)):\n",
    "        score = [0,0,0,0,0,0,0]\n",
    "        for col in range(len(tlx_weight.columns)-1):\n",
    "            score[col] = int(tlx[tlx.columns[col+1]].loc[i] * tlx_weight[tlx_weight.columns[col]].loc[i] )\n",
    "        score[6] =int((score[0]+score[1]+score[2]+score[3]+score[4]+score[5] )/ tlx_weight[tlx_weight.columns[6]].loc[i]/10)\n",
    "        if score[6]>10: score[6]=10\n",
    "        if score[6]<0: score[6]=0\n",
    "        result.loc[i]=score\n",
    "    return result['Score']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "348ab3af",
   "metadata": {},
   "outputs": [],
   "source": [
    "def txtToDataframe_STEW(filename, flag1, flag2):\n",
    "    file = open(filename, 'r')\n",
    "    lines = file.readlines()\n",
    "    datas = []\n",
    "    for line in lines:\n",
    "        txt = line.replace('   ', ' ').lstrip().rstrip().replace(' ', ',')\n",
    "        data = txt.split(',')\n",
    "        datas.append(data)\n",
    "    df = pd.DataFrame(datas)\n",
    "    df.columns = ['AF3', 'F7', 'F3', 'FC5', 'T7', 'O1', 'O2', 'P8', 'T8', 'FC6', 'F4', 'F8', 'F8', 'AF4']\n",
    "    df['label1']=flag1\n",
    "    df['label2']=flag2\n",
    "    return df\n",
    "\n",
    "\n",
    "def getSTEWData(src) :\n",
    "    file_list = os.listdir(src)\n",
    "    \n",
    "    rating = pd.DataFrame(pd.read_csv(src+'ratings.txt'))\n",
    "    file_list.remove('ratings.txt')\n",
    "    \n",
    "    dataList=[]\n",
    "    highList=[]\n",
    "    lowList=[]\n",
    "    \n",
    "    print(rating.columns)\n",
    "    j=0\n",
    "    for i in rating['subject']:\n",
    "        if i<10:\n",
    "            num = str(0)+str(i)\n",
    "        else:\n",
    "            num = str(i)\n",
    "        dataList.append(txtToDataframe_STEW(src+'sub'+num+'_hi.txt', 1,rating['test'][j]))\n",
    "        dataList.append(txtToDataframe_STEW(src+'sub'+num+'_lo.txt', 0,rating['rest'][j]))\n",
    "        highList.append(txtToDataframe_STEW(src+'sub'+num+'_hi.txt', 1,rating['test'][j]))\n",
    "        lowList.append(txtToDataframe_STEW(src+'sub'+num+'_lo.txt', 0,rating['rest'][j]))\n",
    "        j+=1\n",
    "    return dataList, highList, lowList\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ed796465",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['subject', 'rest', 'test'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "src = './STEW Dataset/'\n",
    "originalData, highData, lowData = getSTEWData(src)\n",
    "\n",
    "mergedData = pd.concat([highData[0],highData[1]],ignore_index=True)\n",
    "for i in range(2,len(highData)):\n",
    "    mergedData = pd.concat([mergedData,highData[i]],ignore_index=True)\n",
    "mergedData = mergedData.apply(pd.to_numeric)\n",
    "\n",
    "label2=mergedData['label2']\n",
    "data2=mergedData.drop(['label1','label2'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "5e65794e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EEG data/User/김석연.csv\n",
      "EEG data/User/김예진.csv\n",
      "EEG data/User/문예완.csv\n",
      "EEG data/User/박기웅.csv\n",
      "EEG data/User/박혜연.csv\n",
      "EEG data/User/유상봉.csv\n",
      "EEG data/User/윤찬영.csv\n",
      "EEG data/User/임수빈.csv\n",
      "EEG data/User/정찬영.csv\n",
      "EEG data/User/홍혜인.csv\n"
     ]
    }
   ],
   "source": [
    "src = 'EEG data/User/'\n",
    "files = os.listdir(src)\n",
    "\n",
    "tlx=[]\n",
    "for f in files:\n",
    "    print(src+f)\n",
    "    tlx.append(pd.read_csv(src+f))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "c327aa76",
   "metadata": {},
   "outputs": [],
   "source": [
    "workloadLevel = []\n",
    "for t in tlx:\n",
    "    workloadLevel.append(calculate_tlxLevel(t, calculateWeight(t)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e83195eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EEG data/prepross/김석연.csv\n",
      "EEG data/prepross/김예진.csv\n",
      "EEG data/prepross/문예완.csv\n",
      "EEG data/prepross/박기웅.csv\n",
      "EEG data/prepross/박혜연.csv\n",
      "EEG data/prepross/유상봉.csv\n"
     ]
    }
   ],
   "source": [
    "src = 'EEG data/prepross/'\n",
    "datas = os.listdir(src)\n",
    "\n",
    "eegData=[]\n",
    "for d in datas:\n",
    "    print(src+d)\n",
    "    eegData.append(pd.read_csv(src+d))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e60cdbc4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def split_data(df, label, end):\n",
    "    col = ['EEG.AF3', 'EEG.F7', 'EEG.F3', 'EEG.FC5', 'EEG.T7',\n",
    "           'EEG.P7', 'EEG.O1', 'EEG.O2', 'EEG.P8', 'EEG.T8', 'EEG.FC6', 'EEG.F4',\n",
    "           'EEG.F8', 'EEG.AF4', 'MarkerValueInt']\n",
    "    col_rename = ['AF3', 'F7', 'F3', 'FC5', 'T7',\n",
    "           'P7', 'O1', 'O2', 'P8', 'T8', 'FC6', 'F4',\n",
    "           'F8', 'AF4', 'vis_name']\n",
    "    data_extraction = df[col]\n",
    "    data_extraction.columns = col_rename\n",
    "    rest = data_extraction[data_extraction.vis_name == 0]\n",
    "    survey = data_extraction[data_extraction.vis_name == 100]\n",
    "    \n",
    "    vis = data_extraction[data_extraction.vis_name == 1].reset_index(drop=True)\n",
    "    vis['label'] = label[0]\n",
    "    vis.drop(['vis_name'], axis=1, inplace=True)\n",
    "    \n",
    "    for i in range(2,end):\n",
    "        df = data_extraction[data_extraction.vis_name == i].reset_index(drop=True)\n",
    "        df['label'] = label[i-1]\n",
    "        df.drop(['vis_name'], axis=1, inplace=True)\n",
    "        vis = pd.concat([vis,df], ignore_index=True, axis=0)\n",
    "    return rest, survey, vis\n",
    "\n",
    "cnt = 0\n",
    "for eeg, label in zip (eegData, workloadLevel):\n",
    "    print(cnt,datas[cnt])\n",
    "    if cnt == 0 :\n",
    "        rest, survey, vis = split_data(eeg, label,21)\n",
    "        cnt+=1\n",
    "        continue\n",
    "    elif cnt == 6:\n",
    "        r, s, v = split_data(eeg, label,21)\n",
    "    else:\n",
    "        r, s, v = split_data(eeg, label, 22)\n",
    "    \n",
    "    rest = pd.concat([rest,r], ignore_index=True, axis=0)\n",
    "    survey = pd.concat([survey,s], ignore_index=True, axis=0)\n",
    "    vis = pd.concat([vis,v], ignore_index=True, axis=0)\n",
    "    cnt+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8e7a5ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "label=vis['label']\n",
    "data=vis.drop(['label'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f11e5ef6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "scaled = scaler.fit_transform(data)\n",
    "data = pd.DataFrame(scaled, columns = data.columns, index=data.index)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70be03ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler()\n",
    "scaled = scaler.fit_transform(data2)\n",
    "data2 = pd.DataFrame(scaled, columns = data2.columns, index=data2.index)\n",
    "data2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e13c048",
   "metadata": {},
   "outputs": [],
   "source": [
    "def windowing_dataset(data, label, window_size):\n",
    "    data_list = []\n",
    "    label_list = []\n",
    "    \n",
    "    for i in range(0,len(data)//window_size,window_size):\n",
    "        data_list.append(np.array(data.iloc[i:i+window_size]))\n",
    "        label_list.append(np.array(label.iloc[i]))\n",
    "    return np.array(data_list), np.array(label_list)\n",
    "\n",
    "X_train, y_train = windowing_dataset(data,label,14)\n",
    "X_test, y_test = windowing_dataset(data2,label2,14)\n",
    "print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce541f98",
   "metadata": {},
   "outputs": [],
   "source": [
    "label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba3bd0e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "label = to_categorical(label,10)\n",
    "label2 = to_categorical(label2,10)\n",
    "\n",
    "X_train, y_train = windowing_dataset(data,label,14)\n",
    "x, y = windowing_dataset(data2,label2,14)\n",
    "X_valid, X_test, y_valid, y_test = train_test_split(x,y, train_size=0.5, \n",
    "                                                    random_state=True,\n",
    "                                                    stratify = y)\n",
    "\n",
    "print(X_train.shape, X_test.shape, X_valid.shape,y_valid.shape,y_train.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c9e6765",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Model2_2_1():\n",
    "    model = Sequential()\n",
    "    model.add(InputLayer(input_shape=(14, 14)))\n",
    "    model.add(ExpandLastDim())\n",
    "    model.add(CastToFloat32())\n",
    "    \n",
    "    model.add(Conv2D(512, kernel_size=(4,4), strides=(1,1), activation='relu', padding='valid'))\n",
    "    model.add(Conv2D(512, kernel_size=(4,4), strides=(1,1), activation='relu', padding='valid'))\n",
    "    model.add(MaxPooling2D(2,2))\n",
    "    model.add(Dropout(0.25))\n",
    "    \n",
    "    model.add(Flatten())\n",
    "    model.add(Dropout(0.25))\n",
    "    \n",
    "    model.add(Dense(10, activation='sigmoid'))\n",
    "    \n",
    "    #model.summary()\n",
    "    return model\n",
    "\n",
    "def Model2_2_2():\n",
    "    model = Sequential()\n",
    "    model.add(InputLayer(input_shape=(14, 14)))\n",
    "    model.add(ExpandLastDim())\n",
    "    model.add(CastToFloat32())\n",
    "    \n",
    "    model.add(Conv2D(512, kernel_size=(2,2), strides=(1,1), activation='relu', padding='valid'))\n",
    "    model.add(Conv2D(512, kernel_size=(2,2), strides=(1,1), activation='relu', padding='valid'))\n",
    "    model.add(MaxPooling2D(2,2))\n",
    "    \n",
    "    model.add(Conv2D(128, kernel_size=(2,2), strides=(1,1), activation='relu', padding='same'))\n",
    "    model.add(Conv2D(128, kernel_size=(2,2), strides=(1,1), activation='relu', padding='same'))\n",
    "    model.add(MaxPooling2D(2,2))\n",
    "    \n",
    "    model.add(Conv2D(32, kernel_size=(2,2), strides=(1,1), activation='relu', padding='same'))\n",
    "    model.add(Conv2D(32, kernel_size=(2,2), strides=(1,1), activation='relu', padding='same'))\n",
    "    model.add(Dropout(0.25))\n",
    "    \n",
    "    model.add(Flatten())\n",
    "    model.add(Dropout(0.25))\n",
    "    \n",
    "    model.add(Dense(50, activation='relu'))\n",
    "    model.add(Dense(10, activation='sigmoid'))\n",
    "    \n",
    "    model.summary()\n",
    "    return model\n",
    "\n",
    "def Model2_2_3():\n",
    "    model = Sequential()\n",
    "    model.add(InputLayer(input_shape=(14, 14)))\n",
    "    model.add(ExpandLastDim())\n",
    "    model.add(CastToFloat32())\n",
    "    \n",
    "    model.add(Conv2D(512, kernel_size=(4,4), strides=(1,1), activation='relu', padding='valid'))\n",
    "    model.add(Conv2D(512, kernel_size=(4,4), strides=(1,1), activation='relu', padding='valid'))\n",
    "    model.add(MaxPooling2D(2,2))\n",
    "    \n",
    "    model.add(Conv2D(128, kernel_size=(4,4), strides=(1,1), activation='relu', padding='same'))\n",
    "    model.add(Conv2D(128, kernel_size=(4,4), strides=(1,1), activation='relu', padding='same'))\n",
    "    model.add(MaxPooling2D(2,2))\n",
    "    \n",
    "    model.add(Conv2D(32, kernel_size=(4,4), strides=(1,1), activation='relu', padding='same'))\n",
    "    model.add(Conv2D(32, kernel_size=(4,4), strides=(1,1), activation='relu', padding='same'))\n",
    "    \n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(50, activation='relu'))\n",
    "    model.add(Dropout(0.25))\n",
    "    model.add(Dense(10, activation='sigmoid'))\n",
    "    \n",
    "    #model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ec0f355",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "\n",
    "def runModel(model, crossentropy, x_train, y_train, x_valid, y_valid):\n",
    "    early_stop = EarlyStopping(monitor='val_loss', patience=10)\n",
    "    model.compile(optimizer='adam', loss= crossentropy, metrics='accuracy')\n",
    "\n",
    "    history = model.fit(x_train, y_train, \n",
    "                        epochs = 200, \n",
    "                        validation_data = (x_valid, y_valid), \n",
    "                        callbacks=[early_stop], verbose=0)\n",
    "    drawResult(history)\n",
    "    return model\n",
    "\n",
    "def eval_model(model, x_test, y_test):\n",
    "    y_pred = model.predict(x_test)\n",
    "    return f1_score(y_test.argmax(axis=1), y_pred.argmax(axis=1), average='macro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf323d02",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "crossentropy = 'categorical_crossentropy'\n",
    "\n",
    "print(eval_model(runModel(Model2_2_1(),crossentropy,\n",
    "                          X_train, y_train, X_valid, y_valid), X_test, y_test))\n",
    "print(eval_model(runModel(Model2_2_2(),crossentropy,\n",
    "                          X_train, y_train, X_valid, y_valid), X_test, y_test))\n",
    "print(eval_model(runModel(Model2_2_3(),crossentropy,\n",
    "                          X_train, y_train, X_valid, y_valid), X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eb1a13b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af249356",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py37",
   "language": "python",
   "name": "py37"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
