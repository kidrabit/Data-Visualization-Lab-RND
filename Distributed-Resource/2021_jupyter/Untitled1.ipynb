{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'hdbscan'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-9bcbf0ce3ba0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mcluster\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mhdbscan\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mseaborn\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0msns\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpreprocessing\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mPowerTransformer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnormalize\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mMinMaxScaler\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mStandardScaler\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'hdbscan'"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from struct import unpack\n",
    "from sklearn import cluster\n",
    "import datetime\n",
    "import hdbscan\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import PowerTransformer, normalize, MinMaxScaler, StandardScaler\n",
    "from tsnecuda import TSNE\n",
    "from struct import pack\n",
    "from sklearn_extra.cluster import KMedoids\n",
    "from matplotlib import colors\n",
    "from sklearn.metrics import silhouette_score, silhouette_samples\n",
    "import matplotlib.cm as cm\n",
    "\n",
    "\n",
    "_cmap = colors.ListedColormap(['#1A90F0', '#F93252', '#FEA250', '#276B29', '#362700', \n",
    "                               '#2C2572', '#D25ABE', '#4AB836', '#A859EA', '#65C459', \n",
    "                               '#C90B18', '#E02FD1', '#5FAFD4', '#DAF779', '#ECEE25', \n",
    "                               '#56B390', '#F3BBBE', '#8FC0AE', '#0F16F5', '#8A9EFE', \n",
    "                               '#A23965', '#03F70C', '#A8D520', '#952B77', '#2A493C', \n",
    "                               '#E8DB82', '#7C01AC', '#1938A3', '#3C4249', '#BC3D92', \n",
    "                               '#DEEDB1', '#3C673E', '#65F3D7', '#77110B', '#D16DD6', \n",
    "                               '#08EF68', '#CFFD6F', '#DC6B26', '#912D5D', '#8CA6F8', \n",
    "                               '#04EE96', '#54B0C1', '#6CBE38', '#24633B', '#DE41DD', \n",
    "                               '#5EF270', '#896991', '#E6D381', '#7B0681', '#D66C07'\n",
    "                              ])\n",
    "\n",
    "def transform(_in) :\n",
    "    #vg / h 분리\n",
    "    np_vgh = _in\n",
    "    np_vg = np.delete(_in, 2, axis = 1) #3열 삭제\n",
    "    np_h = (_in[:,2]).reshape(-1,1) #3열 추출\n",
    "    #log(h)\n",
    "    np_logh = np.log(np_h[::]+1)\n",
    "    #logh 병합\n",
    "    np_vgh_logh = np.hstack((np_vgh,np_logh))\n",
    "    # normalized v,g,h,logh\n",
    "    np_normal_l1_vgh_logh = normalize(np_vgh_logh, axis=0, norm='l1')\n",
    "    np_normal_l2_vgh_logh = normalize(np_vgh_logh, axis=0, norm='l2')\n",
    "    np_normal_max_vgh_logh = normalize(np_vgh_logh, axis=0, norm='max')\n",
    "    #tr standardization\n",
    "    std_scaler = StandardScaler()\n",
    "    fitted = std_scaler.fit(np_vgh_logh)\n",
    "    np_std_vgh_logh = std_scaler.transform(np_vgh_logh)\n",
    "    #print(np_std_vgh_logh)\n",
    "    #tr min-max scale\n",
    "    min_max_scaler = MinMaxScaler()\n",
    "    min_max_scaler.fit(np_vgh_logh)\n",
    "    np_min_max_vgh_logh=min_max_scaler.transform(np_vgh_logh)\n",
    "    #print(np_min_max_vgh_logh)\n",
    "    #yeo-johnson\n",
    "    pt_vgh_logh = PowerTransformer(method='yeo-johnson')\n",
    "    pt_vgh_logh.fit(np_vgh_logh)\n",
    "    np_yeojohnson_vgh_logh = pt_vgh_logh.transform(np_vgh_logh)\n",
    "\n",
    "    ret = []\n",
    "    ret.append(  _in                                  )  \n",
    "    ret.append(  np.delete(np_vgh_logh, 2, axis = 1)                   )\n",
    "    ret.append(  np.hstack((np_normal_l1_vgh_logh[:,[0,1]], np_h))     )\n",
    "    ret.append(  np.hstack((np_normal_l1_vgh_logh[:,[0,1]], np_logh))  ) \n",
    "    ret.append(  np.delete(np_normal_l1_vgh_logh, 2, axis = 1)         )\n",
    "    ret.append(  np.delete(np_normal_l1_vgh_logh, 3, axis = 1)         )\n",
    "    ret.append(  np.hstack((np_normal_l2_vgh_logh[:,[0,1]], np_h))     )\n",
    "    ret.append(  np.hstack((np_normal_l2_vgh_logh[:,[0,1]], np_logh))  )\n",
    "    ret.append(  np.delete(np_normal_l2_vgh_logh, 2, axis = 1)         )  \n",
    "    ret.append(  np.delete(np_normal_l2_vgh_logh, 3, axis = 1)         )\n",
    "    ret.append(  np.hstack((np_normal_max_vgh_logh[:,[0,1]], np_h))    )     \n",
    "    ret.append(  np.hstack((np_normal_max_vgh_logh[:,[0,1]], np_logh)) )    \n",
    "    ret.append(  np.delete(np_normal_max_vgh_logh, 2, axis = 1)        )\n",
    "    ret.append(  np.delete(np_normal_max_vgh_logh, 3, axis = 1)        ) \n",
    "    ret.append(  np.hstack((np_std_vgh_logh[:,[0,1]], np_h))           )\n",
    "    ret.append(  np.hstack((np_std_vgh_logh[:,[0,1]], np_logh))        )\n",
    "    ret.append(  np.delete(np_std_vgh_logh, 2, axis = 1)               )   \n",
    "    ret.append(  np.delete(np_std_vgh_logh, 3, axis = 1)               )\n",
    "    ret.append(  np.hstack((np_min_max_vgh_logh[:,[0,1]], np_h))       )     \n",
    "    ret.append(  np.hstack((np_min_max_vgh_logh[:,[0,1]], np_logh))    )     \n",
    "    ret.append(  np.delete(np_min_max_vgh_logh, 2, axis = 1)           )     \n",
    "    ret.append(  np.delete(np_min_max_vgh_logh, 3, axis = 1)           )       \n",
    "    ret.append(  np.hstack((np_yeojohnson_vgh_logh[:,[0,1]], np_h))    )         \n",
    "    ret.append(  np.hstack((np_yeojohnson_vgh_logh[:,[0,1]], np_logh)) )          \n",
    "    ret.append(  np.delete(np_yeojohnson_vgh_logh, 2, axis = 1)        )         \n",
    "    ret.append(  np.delete(np_yeojohnson_vgh_logh, 3, axis = 1)        )      \n",
    "    \n",
    "    return ret\n",
    "\n",
    "def binaryFileWrite2DHist(filename, hist_cluster) :\n",
    "    si = len(hist_cluster[0])\n",
    "    sj = len(hist_cluster)\n",
    "    with open(filename, 'wb') as fp:\n",
    "        for i in range(si):\n",
    "            for j in range(sj):\n",
    "                fp.write(pack('<i', hist_cluster[i][j]))   \n",
    "                \n",
    "def _TSNE(learning_rate, data) :\n",
    "    model = TSNE(learning_rate=learning_rate)\n",
    "    print(\"TSNE calc : \", end='')\n",
    "    startTime = datetime.datetime.now()\n",
    "    transformed = model.fit_transform(data)\n",
    "    endTime = datetime.datetime.now()\n",
    "    diffTime = endTime-startTime\n",
    "    exeTime = diffTime.total_seconds() * 1000\n",
    "    print(exeTime,'ms')\n",
    "    return transformed\n",
    "\n",
    "def kmeans(k, targetdata, originaldata, w, h, bfilename):\n",
    "    startTime = datetime.datetime.now()\n",
    "    kmeans = cluster.KMeans(n_clusters=k, random_state=0).fit(targetdata)\n",
    "    endTime = datetime.datetime.now()\n",
    "    diffTime = endTime-startTime\n",
    "    exeTime = diffTime.total_seconds() * 1000\n",
    "    \n",
    "    #print(targetdata, targetdata.shape)\n",
    "    #print(originaldata, originaldata.shape)\n",
    "    \n",
    "    hist_cluster=[[-1 for x in range(w)] for x in range(h)]\n",
    "    \n",
    "    for idx in range(len(targetdata)):\n",
    "        hist_cluster[originaldata[idx][0]][originaldata[idx][1]]=kmeans.labels_[idx]\n",
    "            \n",
    "    model = bfilename\n",
    "    etc = \"n_clusters_%d\"%(k)\n",
    "    \n",
    "    H = np.array(hist_cluster)\n",
    "    H = H[::]+1\n",
    "    \n",
    "    \n",
    "    #plt.imshow(H[::-1], cmap=_cmap)\n",
    "    \n",
    "    \n",
    "    plt.figure(figsize=(10,10))\n",
    "    plt.title(\"%s K-Mean %d clusters, \\ncalctime : %.2f ms\"%(dataset, k, exeTime))\n",
    "    plt.scatter(originaldata[:,1],originaldata[:,0], s=2, marker='s', cmap=_cmap, c=kmeans.labels_)\n",
    "    plt.savefig(\"kmeans_png/%s_%s_1.png\"%(model, etc), dpi=300)\n",
    "    plt.show()\n",
    "    print('KMEANS Time : ', exeTime,'ms', ', #clusters : ', k)\n",
    "    \n",
    "    print(exeTime,'ms')\n",
    "    \n",
    "    binaryFileWrite2DHist(\"kmeans_bin/%s_%s.bin\"%(model, etc), H) \n",
    "    \n",
    "    \n",
    "    return kmeans.labels_\n",
    "\n",
    "def kmeans_draw_elbow_2to10(targetdata, originaldata, w, h, bfilename):\n",
    "    \n",
    "    sse = []\n",
    "    sil = []\n",
    "    for k in range(2,11) :\n",
    "        startTime = datetime.datetime.now()\n",
    "        clusterer = cluster.KMeans(n_clusters=k, random_state=0)\n",
    "        kmeans = clusterer.fit(targetdata)\n",
    "        endTime = datetime.datetime.now()\n",
    "        diffTime = endTime-startTime\n",
    "        exeTime = diffTime.total_seconds() * 1000\n",
    "        \n",
    "        #for elbow\n",
    "        sse.append(kmeans.inertia_)\n",
    "        \n",
    "        ########################################################################################\n",
    "        # (start) for silhouette\n",
    "        ########################################################################################\n",
    "        fig, (ax1, ax2) = plt.subplots(1, 2)\n",
    "        fig.set_size_inches(18, 7)\n",
    "\n",
    "        # The 1st subplot is the silhouette plot\n",
    "        # The silhouette coefficient can range from -1, 1 but in this example all\n",
    "        # lie within [-0.1, 1]\n",
    "        ax1.set_xlim([-0.1, 1])\n",
    "        # The (n_clusters+1)*10 is for inserting blank space between silhouette\n",
    "        # plots of individual clusters, to demarcate them clearly.\n",
    "        ax1.set_ylim([0, len(targetdata) + (k + 1) * 10])\n",
    "\n",
    "        #print(kmeans.labels_)\n",
    "        silhouette_avg = silhouette_score(targetdata, kmeans.labels_)\n",
    "        \n",
    "        print(\"For n_clusters =\", k,\n",
    "              \"The average silhouette_score is :\", silhouette_avg)\n",
    "\n",
    "        # Compute the silhouette scores for each sample\n",
    "        sample_silhouette_values = silhouette_samples(targetdata, kmeans.labels_)\n",
    "    \n",
    "        y_lower=10\n",
    "        for i in range(k):\n",
    "            # Aggregate the silhouette scores for samples belonging to\n",
    "            # cluster i, and sort them\n",
    "            ith_cluster_silhouette_values = \\\n",
    "                sample_silhouette_values[kmeans.labels_ == i]\n",
    "\n",
    "            ith_cluster_silhouette_values.sort()\n",
    "\n",
    "            size_cluster_i = ith_cluster_silhouette_values.shape[0]\n",
    "            y_upper = y_lower + size_cluster_i\n",
    "\n",
    "            color = cm.nipy_spectral(float(i) / k)\n",
    "            ax1.fill_betweenx(np.arange(y_lower, y_upper),\n",
    "                              0, ith_cluster_silhouette_values,\n",
    "                              facecolor=color, edgecolor=color, alpha=0.7)\n",
    "\n",
    "            # Label the silhouette plots with their cluster numbers at the middle\n",
    "            ax1.text(-0.05, y_lower + 0.5 * size_cluster_i, str(i))\n",
    "\n",
    "            # Compute the new y_lower for next plot\n",
    "            y_lower = y_upper + 10  # 10 for the 0 samples\n",
    "        \n",
    "        #print(targetdata, targetdata.shape)\n",
    "        #print(originaldata, originaldata.shape)\n",
    "        ax1.set_title(\"The silhouette plot. silhouette_score is : %.6f\"%(silhouette_avg))\n",
    "        ax1.set_xlabel(\"The silhouette coefficient values\")\n",
    "        ax1.set_ylabel(\"Cluster label\")\n",
    "\n",
    "        # The vertical line for average silhouette score of all the values\n",
    "        ax1.axvline(x=silhouette_avg, color=\"red\", linestyle=\"--\")\n",
    "\n",
    "        ax1.set_yticks([])  # Clear the yaxis labels / ticks\n",
    "        ax1.set_xticks([-0.1, 0, 0.2, 0.4, 0.6, 0.8, 1])\n",
    "\n",
    "        # 2nd Plot showing the actual clusters formed\n",
    "        colors = cm.nipy_spectral(kmeans.labels_.astype(float) / k)\n",
    "        ax2.scatter(targetdata[:, 1], targetdata[:, 0], marker='.', s=30, lw=0, alpha=0.7,\n",
    "                    c=colors, edgecolor='k')\n",
    "\n",
    "        # Labeling the clusters\n",
    "        centers = clusterer.cluster_centers_\n",
    "        # Draw white circles at cluster centers\n",
    "        ax2.scatter(centers[:, 1], centers[:, 0], marker='o',\n",
    "                    c=\"white\", alpha=1, s=200, edgecolor='k')\n",
    "\n",
    "        for i, c in enumerate(centers):\n",
    "            ax2.scatter(c[1], c[0], marker='$%d$' % i, alpha=1,\n",
    "                        s=50, edgecolor='k')\n",
    "\n",
    "        ax2.set_title(\"%s K-Mean %d clusters, \\ncalctime : %.2f ms\"%(dataset, k, exeTime))\n",
    "        ax2.set_xlabel(\"Feature space for the 1st feature\")\n",
    "        ax2.set_ylabel(\"Feature space for the 2nd feature\")\n",
    "\n",
    "        plt.suptitle((\"Silhouette analysis for KMeans clustering on sample data \"\n",
    "                      \"with n_clusters = %d\" % k),\n",
    "                     fontsize=14, fontweight='bold')        \n",
    "        \n",
    "        ########################################################################################\n",
    "        # (end) for silhouette\n",
    "        ########################################################################################\n",
    "        \n",
    "        \n",
    "        hist_cluster=[[-1 for x in range(w)] for x in range(h)]\n",
    "\n",
    "        for idx in range(len(targetdata)):\n",
    "            hist_cluster[originaldata[idx][0]][originaldata[idx][1]]=kmeans.labels_[idx]\n",
    "\n",
    "        model = bfilename\n",
    "        etc = \"n_clusters_%d\"%(k)\n",
    "\n",
    "        H = np.array(hist_cluster)\n",
    "        H = H[::]+1\n",
    "\n",
    "\n",
    "        #plt.imshow(H[::-1], cmap=_cmap)\n",
    "\n",
    "\n",
    "        #plt.figure(figsize=(10,10))\n",
    "        #plt.title(\"%s K-Mean %d clusters, \\ncalctime : %.2f ms\"%(dataset, k, exeTime))\n",
    "        #plt.scatter(originaldata[:,1],originaldata[:,0], s=2, marker='s', cmap=_cmap, c=kmeans.labels_)\n",
    "        #plt.savefig(\"kmeans_png/%s_%s_1.png\"%(model, etc), dpi=300)\n",
    "        #plt.show()\n",
    "        print('KMEANS Time : ', exeTime,'ms', ', #clusters : ', k)\n",
    "\n",
    "        #print(exeTime,'ms')\n",
    "\n",
    "        binaryFileWrite2DHist(\"kmeans_bin/%s_%s.bin\"%(model, etc), H) \n",
    "    \n",
    "    plt.show() ## for silhouette plot\n",
    "    \n",
    "    \n",
    "    ## for elbow plot\n",
    "    plt.plot(range(2,11), sse, marker='o')\n",
    "    plt.xlabel('# of clusters')\n",
    "    plt.ylabel('SSE')\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "    return kmeans.labels_\n",
    "\n",
    "def kmeans_dr(k, targetdata, originaldata, w, h, bfilename):\n",
    "    startTime = datetime.datetime.now()\n",
    "    kmeans = cluster.KMeans(n_clusters=k, random_state=0).fit(targetdata)\n",
    "    endTime = datetime.datetime.now()\n",
    "    diffTime = endTime-startTime\n",
    "    exeTime = diffTime.total_seconds() * 1000\n",
    "    \n",
    "    #print(targetdata, targetdata.shape)\n",
    "    #print(originaldata, originaldata.shape)\n",
    "    \n",
    "    hist_cluster=[[-1 for x in range(w)] for x in range(h)]\n",
    "    \n",
    "    for idx in range(len(targetdata)):\n",
    "        hist_cluster[originaldata[idx][0]][originaldata[idx][1]]=kmeans.labels_[idx]\n",
    "            \n",
    "    model = bfilename\n",
    "    etc = \"n_clusters_%d\"%(k)\n",
    "    \n",
    "    H = np.array(hist_cluster)\n",
    "    H = H[::]+1\n",
    "    \n",
    "    plt.figure(figsize=(10,10))\n",
    "    plt.title(\"%s K-Mean %d clusters, \\ncalctime : %.2f ms\"%(dataset, k, exeTime))\n",
    "    plt.scatter(originaldata[:,1],originaldata[:,0], s=2,  marker='s',cmap=_cmap, c=kmeans.labels_)\n",
    "    \n",
    "    #plt.imshow(H[::-1], cmap=_cmap)\n",
    "    \n",
    "    \n",
    "    plt.savefig(\"kmeans_dr_png/%s_%s_1.png\"%(model, etc), dpi=300)\n",
    "    plt.show()\n",
    "    print('KMEANS Time : ', exeTime,'ms', ', #clusters : ', k)\n",
    "    \n",
    "    print(exeTime,'ms')\n",
    "    \n",
    "    binaryFileWrite2DHist(\"kmeans_dr_bin/%s_%s.bin\"%(model, etc), H) \n",
    "    \n",
    "    viewTSNEClustering(targetdata, kmeans.labels_, \"kmeans_dr_png/%s_%s_2.png\"%(model, etc) )\n",
    "    \n",
    "    return kmeans.labels_\n",
    "\n",
    "def kmedoids(k, _metric, targetdata, originaldata, w, h, bfilename) :\n",
    "    startTime = datetime.datetime.now()\n",
    "    \n",
    "    kmedoids = KMedoids(metric=_metric, n_clusters=k).fit(targetdata)\n",
    "    endTime = datetime.datetime.now()\n",
    "    diffTime = endTime-startTime\n",
    "    exeTime = diffTime.total_seconds() * 1000\n",
    "    \n",
    "    #print(targetdata, targetdata.shape)\n",
    "    #print(originaldata, originaldata.shape)\n",
    "    \n",
    "    hist_cluster=[[-1 for x in range(w)] for x in range(h)]\n",
    "    \n",
    "    for idx in range(len(targetdata)):\n",
    "        hist_cluster[originaldata[idx][0]][originaldata[idx][1]]=kmedoids.labels_[idx]\n",
    "            \n",
    "    model = bfilename\n",
    "    etc = \"metric_%s_n_clusters_%d\"%(_metric, k)\n",
    "    \n",
    "    H4 = np.array(hist_cluster)\n",
    "    H4 = H4[::]+1\n",
    "    plt.imshow(H4[::-1])\n",
    "    plt.title(\"%s KMedoids %d clusters, metric : %s, \\ncalctime : %.2f ms\"%(dataset, k, _metric, exeTime))\n",
    "    #plt.show()\n",
    "    print('KMedoids Time : ', exeTime,'ms', ', #clusters : ', k)\n",
    "    plt.savefig(\"kmedoids_png/%s_%s_1.png\"%(model, etc), dpi=300)\n",
    "    \n",
    "    print(exeTime,'ms')\n",
    "    \n",
    "    binaryFileWrite2DHist(\"kmedoids_bin/%s_%s.bin\"%(model, etc), H4) \n",
    "    \n",
    "    return kmedoids.labels_\n",
    "\n",
    "def dbscan(_eps, targetdata, originaldata, w, h, bfilename):\n",
    "    startTime = datetime.datetime.now()\n",
    "    dbscan = cluster.DBSCAN(eps=_eps).fit(targetdata)\n",
    "\n",
    "    endTime = datetime.datetime.now()\n",
    "    diffTime = endTime-startTime\n",
    "    exeTime = diffTime.total_seconds() * 1000\n",
    "\n",
    "    hist_dbscan_cluster=[[-1 for x in range(w)] for x in range(h)]\n",
    "    \n",
    "    for idx in range(len(originaldata)):\n",
    "        _i = originaldata[idx][0]\n",
    "        _j = originaldata[idx][1]\n",
    "        hist_dbscan_cluster[_i][_j]=dbscan.labels_[idx]\n",
    "        \n",
    "    model = bfilename\n",
    "    etc = \"eps_%.2f\"%(_eps)\n",
    "\n",
    "    H2 = np.array(hist_dbscan_cluster)\n",
    "    #plt.imshow(H2[::-1])\n",
    "    plt.figure(figsize=(10,10))\n",
    "    plt.title(\"%s DBSCAN eps : %f, \\ncalctime : %.2f ms\"%(dataset, _eps, exeTime))\n",
    "    plt.scatter(originaldata[:,1],originaldata[:,0], s=2,  marker='s',cmap=_cmap, c=dbscan.labels_)\n",
    "    \n",
    "    #plt.show()\n",
    "    plt.savefig(\"dbscan_png/%s_%s_1.png\"%(model, etc), dpi=300)\n",
    "    print('DBSCAN Time : ', exeTime,'ms', ', eps : ',_eps)\n",
    "    \n",
    "    \n",
    "    n_clusters_ = len(set(dbscan.labels_)) - (1 if -1 in dbscan.labels_ else 0)\n",
    "    print( \"# of clusters : \", n_clusters_   )\n",
    "    \n",
    "    binaryFileWrite2DHist(\"dbscan_bin/%s_%s.bin\"%(model, etc), H2)    \n",
    "    \n",
    "    return dbscan.labels_\n",
    "\n",
    "def dbscan_dr(_eps, targetdata, originaldata, w, h, bfilename, tsne_cnt):\n",
    "    startTime = datetime.datetime.now()\n",
    "    dbscan = cluster.DBSCAN(eps=_eps).fit(targetdata)\n",
    "\n",
    "    endTime = datetime.datetime.now()\n",
    "    diffTime = endTime-startTime\n",
    "    exeTime = diffTime.total_seconds() * 1000\n",
    "\n",
    "    hist_dbscan_cluster=[[-1 for x in range(w)] for x in range(h)]\n",
    "    \n",
    "    for idx in range(len(originaldata)):\n",
    "        _i = originaldata[idx][0]\n",
    "        _j = originaldata[idx][1]\n",
    "        hist_dbscan_cluster[_i][_j]=dbscan.labels_[idx]\n",
    "        \n",
    "    model = bfilename\n",
    "    etc = \"eps_%.2f\"%(_eps)\n",
    "\n",
    "    H2 = np.array(hist_dbscan_cluster)\n",
    "    \n",
    "    \n",
    "    #plt.imshow(H2[::-1])\n",
    "    plt.figure(figsize=(10,10))\n",
    "    plt.title(\"%s DBSCAN eps : %f, \\ncalctime : %.2f ms\"%(dataset, _eps, exeTime))\n",
    "    plt.scatter(originaldata[:,1],originaldata[:,0], s=2,  marker='s',cmap=_cmap, c=dbscan.labels_)\n",
    "    \n",
    "    plt.savefig(\"dbscan_dr_png/%d/%s_%s_1.png\"%(tsne_cnt,model, etc), dpi=300)\n",
    "    print('DBSCAN Time : ', exeTime,'ms', ', eps : ',_eps)\n",
    "    \n",
    "    n_clusters_ = len(set(dbscan.labels_)) - (1 if -1 in dbscan.labels_ else 0)\n",
    "    print( \"# of clusters : \", n_clusters_   )\n",
    "    \n",
    "    binaryFileWrite2DHist(\"dbscan_dr_bin/%d/%s_%s.bin\"%(tsne_cnt,model, etc), H2)    \n",
    "    \n",
    "    \n",
    "    viewTSNEClustering(targetdata, dbscan.labels_, \"dbscan_dr_png/%d/%s_%s_2.png\"%(tsne_cnt,model, etc) )\n",
    "    \n",
    "    return dbscan.labels_\n",
    "    \n",
    "    plt.figure(figsize=(10,10))\n",
    "    plt.title(\"%s K-Mean %d clusters, \\ncalctime : %.2f ms\"%(dataset, k, exeTime))\n",
    "    plt.scatter(originaldata[:,1],originaldata[:,0], s=2, marker='s', cmap=_cmap, c=kmeans.labels_)\n",
    "    plt.savefig(\"kmeans_png/%d/%s_%s_1.png\"%(tsne_cnt,model, etc), dpi=300)\n",
    "    plt.show()\n",
    "    print('KMEANS Time : ', exeTime,'ms', ', #clusters : ', k)\n",
    "\n",
    "def Hdbscan_dr(_min_cluster_size, _min_samples, _alpha, targetdata, originaldata, w, h, bfilename, tsne_cnt):\n",
    "    startTime = datetime.datetime.now()\n",
    "    clusterer = hdbscan.HDBSCAN(min_cluster_size=_min_cluster_size, min_samples =_min_samples, alpha=_alpha).fit(targetdata)\n",
    "\n",
    "    endTime = datetime.datetime.now()\n",
    "    diffTime = endTime-startTime\n",
    "    exeTime = diffTime.total_seconds() * 1000\n",
    "    #print(clusterer.labels_)\n",
    "    #print(max(clusterer.labels_))\n",
    "    #print(targetdata)\n",
    "    #print(originaldata)\n",
    "    #print(clusterer.labels_)\n",
    "\n",
    "    hist_cluster_hdbscan=[[-1 for x in range(w)] for x in range(h)]\n",
    "    \n",
    "    for idx in range(len(originaldata)):\n",
    "        _i = originaldata[idx][0]\n",
    "        _j = originaldata[idx][1]\n",
    "        hist_cluster_hdbscan[_i][_j]=clusterer.labels_[idx]\n",
    "\n",
    "        \n",
    "    model = bfilename\n",
    "    etc = \"minsize_%.1f_minsample_%.1f_alpha_%.1f\"%(_min_cluster_size, _min_samples, _alpha)\n",
    "    \n",
    "    \n",
    "    n_clusters_ = len(set(clusterer.labels_)) - (1 if -1 in clusterer.labels_ else 0)\n",
    "    print(  \"cluster num : \", n_clusters_   )\n",
    "    \n",
    "    H3 = np.array(hist_cluster_hdbscan)\n",
    "    plt.imshow(H3[::-1])\n",
    "    \n",
    "    \n",
    "    \n",
    "    plt.figure(figsize=(10,10))\n",
    "    plt.title(\"HDBSCAN [%s], min_cluster_size : %d, result cluster number : %d, \\ncalc time: %.2f ms\"%(dataset, _min_cluster_size, n_clusters_, exeTime))\n",
    "    plt.scatter(originaldata[:,1],originaldata[:,0], s=2,  marker='s',cmap=_cmap, c=clusterer.labels_)    \n",
    "    plt.savefig(\"hdbscan_dr_png/%d/%s_%s_1.png\"%(tsne_cnt,model, etc), dpi=300)\n",
    "    plt.show()\n",
    "    print(exeTime,'ms')    \n",
    "    \n",
    "    clusterer.condensed_tree_.plot(select_clusters=True,\n",
    "                               selection_palette=sns.color_palette('deep', 8))\n",
    "    \n",
    "    plt.savefig(\"hdbscan_dr_png/%d/%s_%s_1.5.png\"%(tsne_cnt,model, etc), dpi=300)\n",
    "    plt.show()\n",
    "                                   \n",
    "    viewTSNEClustering(targetdata, clusterer.labels_, \"hdbscan_dr_png/%d/%s_%s_2.png\"%(tsne_cnt,model, etc) )\n",
    "    \n",
    "    \n",
    "    binaryFileWrite2DHist(\"hdbscan_dr_bin/%d/%s_%s.bin\"%(tsne_cnt,model, etc), H3) \n",
    "    \n",
    "    print(bfilename)\n",
    "    \n",
    "    return clusterer.labels_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = [\"vgh\", \"vglogh\", \n",
    "         \"n_l1_vg_h\", \"n_l1_vg_logh\", \"n_l1_vgh\", \"n_l1_vglogh\", \n",
    "         \"n_l2_vg_h\", \"n_l2_vg_logh\", \"n_l2_vgh\", \"n_l2_vglogh\", \n",
    "         \"n_max_vg_h\", \"n_max_vg_logh\", \"n_max_vgh\", \"n_max_vglogh\", \n",
    "         \"std_vg_h\", \"std_vg_logh\", \"std_vgh\", \"std_vglogh\", \n",
    "         \"mm_vg_h\", \"mm_vg_logh\", \"mm_vgh\", \"mm_vglogh\", \n",
    "         \"pt_vg_h\", \"pt_vg_logh\", \"pt_vgh\", \"pt_vglogh\"\n",
    "        ]\n",
    "\n",
    "w=256\n",
    "h=256\n",
    "size = w*h\n",
    "\n",
    "all_val_grad_hist = []\n",
    "sparse_val_grad_hist = []\n",
    "#'VisMale_128x256x256','bonsai256X256X256B', 'Carp_256x256x512','XMasTree-LO_256x249x256' \n",
    "for dataset in ['VisMale_128x256x256'] :\n",
    "    with open('../volumeCache/%s.raw.2DHistogram.TextureCache'%(dataset), 'rb') as fp:\n",
    "        Histogram2DYMax = unpack('<f', fp.read(4))[0] #Max of Grad_mag\n",
    "        for i in range(h) :\n",
    "            for j in range(w):\n",
    "                readdata = unpack('<L', fp.read(4))[0]\n",
    "                all_val_grad_hist.append([i, j, readdata])\n",
    "                if readdata>=1 and i!=0:\n",
    "                    sparse_val_grad_hist.append([i, j, readdata])\n",
    "\n",
    "        np_all_val_grad_hist = np.array(all_val_grad_hist)\n",
    "        np_sparse_val_grad_hist = np.array(sparse_val_grad_hist)\n",
    "\n",
    "        ret_all_array = transform(np_all_val_grad_hist)\n",
    "        ret_sparse_array = transform(np_sparse_val_grad_hist)\n",
    "        \n",
    "        tsne_all_array = []\n",
    "        tsne_sparse_array = []\n",
    "        \n",
    "        ###########################################################################################################\n",
    "        \n",
    "        #for i in range(len(ret_all_array)):\n",
    "        #    print(\"%d_%s_%s\"% (i, dataset, index[i]), end='\\t')\n",
    "        #    tsne_all_array.append(_TSNE(100,ret_all_array[i]))\n",
    "        \n",
    "        #for i in range(len(tsne_all_array)):\n",
    "        #    save_tsne_result( tsne_all_array[i],  \"tsneCache\", \"%d_%s_%s\"% (i, dataset, index[i]) )        \n",
    "        \n",
    "        #for i in range(len(tsne_all_array)):\n",
    "        #    for _k in [10,15,20]:\n",
    "        #        kmeans(_k,  tsne_all_array[i], np_all_val_grad_hist, 256,256,  \"%d_%s_%s\"% (i, dataset, index[i]) )\n",
    "        #    \n",
    "        #    for _eps in [0.5, 1.0, 1.5, 3.0]:\n",
    "        #        dbscan(_eps, tsne_all_array[i], np_all_val_grad_hist, 256,256,  \"%d_%s_%s\"% (i, dataset, index[i]))\n",
    "        #    Hdbscan(300, 20, 1.0, tsne_all_array[i], np_all_val_grad_hist, 256,256,  \"%d_%s_%s\"% (i, dataset, index[i])  ) \n",
    "        \n",
    "        ###########################################################################################################\n",
    "        \n",
    "        for tsne_cnt in range(10):\n",
    "            tsne_sparse_array.append([])\n",
    "            for i in range(len(ret_sparse_array)):\n",
    "                print(\"%s_%d_sparse_%s\"% ( dataset, i,index[i]), end='\\t')\n",
    "                tsne_sparse_array[tsne_cnt].append(_TSNE(3000,ret_sparse_array[i]))\n",
    "\n",
    "            for i in range(len(tsne_sparse_array[tsne_cnt])):\n",
    "                save_tsne_result( tsne_sparse_array[tsne_cnt][i],  \"tsneCache\", \"%d_%s_%d_sparse_%s\"% (tsne_cnt, dataset,i,  index[i]) )     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for tsne_cnt in range(10):\n",
    "    for i in range(len(tsne_sparse_array[tsne_cnt])):\n",
    "        Hdbscan_dr(80,20,1.0, tsne_sparse_array[tsne_cnt][i], np_sparse_val_grad_hist, 256,256,  \"%d_%d_%s_%s\"% (tsne_cnt, i, dataset, index[i]), tsne_cnt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: hdbscan in c:\\users\\vislab_phy\\anaconda3\\envs\\py37_clone\\lib\\site-packages (0.8.27)\n",
      "Requirement already satisfied: scikit-learn>=0.20 in c:\\users\\vislab_phy\\anaconda3\\envs\\py37_clone\\lib\\site-packages (from hdbscan) (0.23.2)\n",
      "Requirement already satisfied: six in c:\\users\\vislab_phy\\anaconda3\\envs\\py37_clone\\lib\\site-packages (from hdbscan) (1.15.0)\n",
      "Requirement already satisfied: scipy>=1.0 in c:\\users\\vislab_phy\\anaconda3\\envs\\py37_clone\\lib\\site-packages (from hdbscan) (1.5.2)\n",
      "Requirement already satisfied: joblib>=1.0 in c:\\users\\vislab_phy\\anaconda3\\envs\\py37_clone\\lib\\site-packages (from hdbscan) (1.0.1)\n",
      "Requirement already satisfied: numpy>=1.16 in c:\\users\\vislab_phy\\anaconda3\\envs\\py37_clone\\lib\\site-packages (from hdbscan) (1.19.1)\n",
      "Collecting cython>=0.27\n",
      "  Using cached Cython-0.29.21-cp37-cp37m-win_amd64.whl (1.6 MB)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\vislab_phy\\anaconda3\\envs\\py37_clone\\lib\\site-packages (from scikit-learn>=0.20->hdbscan) (2.1.0)\n",
      "Installing collected packages: cython\n",
      "Successfully installed cython-0.29.21\n"
     ]
    }
   ],
   "source": [
    "!pip install hdbscan"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
