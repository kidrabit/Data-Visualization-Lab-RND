{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import BCEWithLogitsLoss\n",
    "from torch.utils.data import DataLoader\n",
    "from dgl import model_zoo,DGLGraph\n",
    "import math\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "from utils_merge import Meter, set_random_seed, collate, EarlyStopping,load_brl_dataset\n",
    "import argparse\n",
    "# from sklearn import svm, datasets\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# from sklearn.metrics import plot_confusion_matrix\n",
    "import seaborn as sn\n",
    "import pandas as pd\n",
    "from sklearn import metrics\n",
    "import collections\n",
    "import sys; sys.argv=['']; del sys\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from dgl.contrib.deprecation import deprecated\n",
    "from dgl.nn.pytorch import Set2Set, NNConv\n",
    "    \n",
    "class ImputationNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ImputationNet,self).__init__()\n",
    "        \n",
    "        self.fc1 = nn.Linear(6*7+7, 512)\n",
    "        self.fc2 = nn.Linear(512,512)\n",
    "        self.fc3 = nn.Linear(512,512)\n",
    "        self.fc_last = nn.Linear(512,359*7)\n",
    "        self.tanh=nn.Tanh()\n",
    "        \n",
    "    def forward(self,x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = self.tanh(self.fc_last(x))#self.fc_last(x)#\n",
    "        return x\n",
    "\n",
    "class MPNNModel(nn.Module):\n",
    "    @deprecated('Import MPNNPredictor from dgllife.model instead.', 'class')\n",
    "    def __init__(self,\n",
    "                 node_input_dim=15,\n",
    "                 edge_input_dim=5,\n",
    "                 #output_dim=12,\n",
    "                 node_hidden_dim=64,\n",
    "                 edge_hidden_dim=128,#64,#\n",
    "                 num_step_message_passing=6,#4,#\n",
    "                 num_step_set2set=6,#4,#\n",
    "                 num_layer_set2set=3):#2):#\n",
    "        super(MPNNModel, self).__init__()\n",
    "\n",
    "        self.num_step_message_passing = num_step_message_passing\n",
    "        self.lin0 = nn.Linear(node_input_dim, node_hidden_dim)\n",
    "        edge_network = nn.Sequential(\n",
    "            nn.Linear(edge_input_dim, edge_hidden_dim), nn.ReLU(),\n",
    "            nn.Linear(edge_hidden_dim, node_hidden_dim * node_hidden_dim))\n",
    "        self.conv = NNConv(in_feats=node_hidden_dim,\n",
    "                           out_feats=node_hidden_dim,\n",
    "                           edge_func=edge_network,\n",
    "                           aggregator_type='sum')\n",
    "        self.gru = nn.GRU(node_hidden_dim, node_hidden_dim)\n",
    "\n",
    "        self.set2set = Set2Set(node_hidden_dim, num_step_set2set, num_layer_set2set)\n",
    "        self.lin1 = nn.Linear(2 * node_hidden_dim, node_hidden_dim)\n",
    "        \n",
    "        self.lin2_1 = nn.Linear(node_hidden_dim, 121)\n",
    "        self.lin2_2 = nn.Linear(node_hidden_dim, 1)\n",
    "\n",
    "    def forward(self, g, n_feat, e_feat):\n",
    "        out = F.relu(self.lin0(n_feat))                 # (B1, H1)\n",
    "        h = out.unsqueeze(0)                            # (1, B1, H1)\n",
    "        \n",
    "        for i in range(self.num_step_message_passing):\n",
    "            m = F.relu(self.conv(g, out, e_feat))       # (B1, H1)\n",
    "            out, h = self.gru(m.unsqueeze(0), h)\n",
    "            out = out.squeeze(0)\n",
    "            \n",
    "        out = self.set2set(g, out)\n",
    "        out = F.relu(self.lin1(out))\n",
    "        \n",
    "        out1=self.lin2_1(out)#for classification\n",
    "        out2=self.lin2_2(out)#for regression\n",
    "        return [out1,out2]\n",
    "\n",
    "class ImPrNet(nn.Module):\n",
    "    def __init__(self, model_disp, model_multitask):\n",
    "        super(ImPrNet, self).__init__()\n",
    "        self.model_imputation = model_disp\n",
    "        self.model_multitask = model_multitask\n",
    "        \n",
    "    def forward(self, g,  x, e_feat):#x(n,6*7+7)-disp6,nf\n",
    "        disp_359 = self.model_imputation(x)#(n,359*7)-disp359\n",
    "        disp_359=disp_359.view(-1,359,7)\n",
    "        x=x[:,:6*7].view(-1,6,7)\n",
    "        \n",
    "        x=torch.cat((disp_359,x),axis=1)\n",
    "        x=x.view(len(x)*365,7)\n",
    "        #print('forwad',len(g),x.shape)\n",
    "        \n",
    "        #h = bg.ndata.pop('n_feat')\n",
    "        #e = bg.edata.pop('e_feat')\n",
    "        #h, e = h.to(args['device']), e.to(args['device'])\n",
    "        #for i in range(len(g)):g[i].ndata['n_feat'] =x[i]\n",
    "        g.ndata['n_feat'] =x\n",
    "        \n",
    "        output = self.model_multitask(g,  x, e_feat)\n",
    "        \n",
    "        return output\n",
    "    \n",
    "\n",
    "\n",
    "print(torch.cuda.current_device())\n",
    "# torch.cuda.set_device(1)\n",
    "# print(torch.cuda.current_device())\n",
    "\n",
    "\n",
    "parser = argparse.ArgumentParser(description='Molecule Regression')\n",
    "parser.add_argument('-m', '--model', type=str,default='MPNN',help='Model to use')#choices=['MPNN', 'SCHNET', 'MGCN', 'AttentiveFP'],\n",
    "#parser.add_argument('-d', '--dataset', type=str, default='bridge',help='Dataset to use')#choices=['Alchemy', 'Aromaticity'],                \n",
    "parser.add_argument('-p', '--pre-trained', action='store_true', default=False, help='Whether to skip training and use a pre-trained model')\n",
    "args = parser.parse_args().__dict__\n",
    "training_setting= {\n",
    "    'random_seed': 0,\n",
    "    'batch_size': 64,#64,#\n",
    "    'num_epochs': 2000,#900,\n",
    "    'node_in_feats': 7,\n",
    "    'edge_in_feats': 6,\n",
    "    #'output_dim': 120,\n",
    "    'lr': 0.00001,#0.001,#\n",
    "    'patience': 300,#20,\n",
    "    'metric_name': 'l1',#'roc_auc',#\n",
    "    'weight_decay': 0,\n",
    "    'n_task':41,\n",
    "}\n",
    "args.update(training_setting)\n",
    "args['device'] = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "args['data_path_x']='data/data_mlp/single/input6/'#'data/data_mpnn/'\n",
    "args['data_path_label']='data/label_multitask/'\n",
    "set_random_seed(args['random_seed'])\n",
    "\n",
    "\n",
    "train_loader,val_loader,test_loader=load_brl_dataset(args)#for batch_id, batch_data in enumerate(train_loader):bg, labels = batch_data;print(a)\n",
    "\n",
    "#model = load_model(args)\n",
    "model_i = ImputationNet()\n",
    "model_p = MPNNModel(node_input_dim=args['node_in_feats'],\n",
    "                  edge_input_dim=args['edge_in_feats'])\n",
    "\n",
    "# Load state dicts\n",
    "model_i.load_state_dict(torch.load('model_saved/mlp/single/imputation/last.pth')['model_state_dict'])\n",
    "model_p.load_state_dict(torch.load('model_saved/multitask/model2_5000/early_stop.pth')['model_state_dict'])\n",
    "\n",
    "model = ImPrNet(model_i, model_p)\n",
    "model.to(args['device'])\n",
    "\n",
    "loss_fn_cablenumber =nn.CrossEntropyLoss()\n",
    "loss_fn_area =nn.L1Loss(reduction='none')\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=args['lr'], weight_decay=args['weight_decay'])\n",
    "stopper = EarlyStopping(mode='lower', patience=args['patience'],\n",
    "                        filename='model_saved/finetuning/early_stop.pth')\n",
    "\n",
    "\n",
    "def run_a_train_epoch_cr(args, epoch, model, data_loader,\n",
    "                      loss_criterion_cablenumber,loss_criterion_area, optimizer):\n",
    "    model.train()\n",
    "    train_meter = Meter()\n",
    "    correct=0\n",
    "    total=0\n",
    "    for batch_id, batch_data in enumerate(data_loader):\n",
    "        bg, h, labels_cablenumber,labels_area = batch_data\n",
    "        labels_cablenumber = labels_cablenumber.to(args['device'])\n",
    "        labels_area = labels_area.to(args['device'])\n",
    "        labels=[labels_cablenumber,labels_area]\n",
    "        #prediction = regress(args, model, bg)\n",
    "        #h = bg.ndata.pop('n_feat')\n",
    "        e = bg.edata.pop('e_feat')\n",
    "        h, e = h.to(args['device']), e.to(args['device'])\n",
    "        prediction = model(bg, h, e)\n",
    "        \n",
    "        loss_cablenumber = (loss_criterion_cablenumber(prediction[0], labels_cablenumber).float()).mean()\n",
    "        loss_area = (loss_criterion_area(prediction[1],labels_area).float()).mean()\n",
    "        loss=(loss_cablenumber+loss_area)\n",
    "       \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_meter.update(prediction[1], labels_area)\n",
    "        \n",
    "        _, predicted = torch.max(prediction[0].data, 1)\n",
    "        total += labels_cablenumber.size(0)\n",
    "        correct += (predicted == labels_cablenumber).sum().item();del predicted\n",
    "        \n",
    "    total_score_acc=100-100 * correct / total\n",
    "    total_score = np.mean(train_meter.compute_metric(args['metric_name']))\n",
    "    print('epoch {:d}/{:d}, training {} {:.4f} / accuracy(%) {:.4f}'.format(\n",
    "        epoch + 1, args['num_epochs'], args['metric_name'], total_score, total_score_acc)) \n",
    "    #print('epoch {:d}/{:d}, training total_score_acc {:.4f}'.format(epoch + 1, args['num_epochs'], total_score_acc)) \n",
    "    \n",
    "def run_an_eval_epoch_cr(args, model, data_loader):\n",
    "    model.eval()\n",
    "    eval_meter = Meter()\n",
    "    correct=0\n",
    "    total=0\n",
    "    with torch.no_grad():\n",
    "        for batch_id, batch_data in enumerate(data_loader):\n",
    "            bg,h, labels_cablenumber,labels_area = batch_data\n",
    "            labels_cablenumber = labels_cablenumber.to(args['device'])\n",
    "            labels_area = labels_area.to(args['device'])\n",
    "            labels=[labels_cablenumber,labels_area]\n",
    "            #prediction = regress(args, model, bg)\n",
    "            #h = bg.ndata.pop('n_feat')\n",
    "            e = bg.edata.pop('e_feat')\n",
    "            h, e = h.to(args['device']), e.to(args['device'])\n",
    "            prediction = model(bg, h, e)\n",
    "            \n",
    "            \n",
    "            eval_meter.update(prediction[1], labels_area)\n",
    "            \n",
    "            _, predicted = torch.max(prediction[0].data, 1)\n",
    "            total += labels_cablenumber.size(0)\n",
    "            correct += (predicted == labels_cablenumber).sum().item();del predicted\n",
    "        total_score_acc=100-100 * correct / total\n",
    "        total_score = np.mean(eval_meter.compute_metric(args['metric_name']))\n",
    "    return total_score,total_score_acc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.save({'model_state_dict': model.state_dict(),\n",
    "#             'optimizer': optimizer.state_dict()}, 'model_saved/multitask/model2_5000/1.pth')\n",
    "state=torch.load('model_saved/finetuning/early_stop.pth') \n",
    "model.load_state_dict(state['model_state_dict'])\n",
    "optimizer.load_state_dict(state['optimizer'])\n",
    "for param_group in optimizer.param_groups: print(param_group['lr'])\n",
    "# for param_group in optimizer.param_groups: param_group['lr'] = param_group['lr']*0.1\n",
    "# for param_group in optimizer.param_groups: print(param_group['lr'])\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7276,) (7276, 1) (7276,) (7276, 1)\n",
      "acc:  0.9677020340846619\n",
      "acc mean 0.4802544765957446 0.49932442536571514\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "#eval_meter = Meter()\n",
    "cn_p=[];area_p=[];cn_t=[];area_t=[]\n",
    "with torch.no_grad():\n",
    "    for batch_id, batch_data in enumerate(train_loader):\n",
    "        bg,h, labels_cablenumber,labels_area = batch_data\n",
    "        \n",
    "        labels_cablenumber = labels_cablenumber.to(args['device'])\n",
    "        labels_area = labels_area.to(args['device'])\n",
    "        cn_t.append(labels_cablenumber.cpu().detach().numpy())\n",
    "        area_t.append(labels_area.cpu().detach().numpy())\n",
    "        \n",
    "        e = bg.edata.pop('e_feat')\n",
    "        h, e = h.to(args['device']), e.to(args['device'])\n",
    "        prediction = model(bg, h, e)\n",
    "        \n",
    "        \n",
    "        cn_p.append(prediction[0].cpu().detach().numpy())\n",
    "        area_p.append(prediction[1].cpu().detach().numpy())\n",
    "\n",
    "cn_p=np.concatenate(cn_p,axis=0)\n",
    "cn_p=np.argmax(cn_p,axis=1)\n",
    "area_p=np.concatenate(area_p,axis=0)\n",
    "\n",
    "cn_t=np.concatenate(cn_t,axis=0)\n",
    "area_t=np.concatenate(area_t,axis=0)\n",
    "\n",
    "print(cn_p.shape,area_p.shape,cn_t.shape,area_t.shape)\n",
    "\n",
    "acc=np.sum(cn_p==cn_t)/len(cn_p)\n",
    "print('acc: ',acc)\n",
    "#plt.plot(area_t,area_p,'.')\n",
    "\n",
    "w_idx=np.where(cn_p!=cn_t)\n",
    "c_idx=np.where(cn_p==cn_t)\n",
    "\n",
    "area=np.load('data/label_multitask/a_label_train.npz')['arr_0']\n",
    "print('acc mean',np.mean(area[w_idx]),np.mean(area[c_idx]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.metrics as metrics\n",
    "t_area=area_t.copy();p_area=area_p.copy()\n",
    "#plt.plot(p_area.flatten(),t_area.flatten(),'.',color='darkblue',alpha=0.2)\n",
    "plt.plot(t_area[c_idx].flatten(),p_area[c_idx].flatten(),'.',color='deepskyblue',alpha=0.3)\n",
    "plt.plot(t_area[w_idx].flatten(),p_area[w_idx].flatten(),'.',color='crimson',alpha=0.3)\n",
    "#plt.ylim(np.min([a_p,a_t]), np.max([a_p,a_t]))\n",
    "#plt.xlim(np.min([a_p,a_t]), np.max([a_p,a_t]))\n",
    "plt.ylim(-0.1,1.1)\n",
    "plt.xlim(-0.1,1.1)\n",
    "plt.xlabel('actual')\n",
    "plt.ylabel('pred')\n",
    "plt.grid()\n",
    "#plt.savefig('images/train_area.png', bbox_inches = 'tight')\n",
    "plt.show()\n",
    "\n",
    "y_t=area_t.copy();y_p=area_p.copy()\n",
    "print(y_t.shape,y_p.shape)#(\n",
    "import sklearn.metrics as metrics\n",
    "mae = metrics.mean_absolute_error(y_t, y_p)\n",
    "mse = metrics.mean_squared_error(y_t, y_p)\n",
    "rmse=np.sqrt(mse)\n",
    "print('mae', mae, '| rmse:',rmse)\n",
    "print('actual mean', np.mean(y_t),'| pred mean',np.mean(y_p))\n",
    "print(np.corrcoef(y_t.flatten(),y_p.flatten()))\n",
    "print('rmse/range',rmse/(np.max(y_t)-np.min(y_t)))\n",
    "print('mape',np.mean(np.abs(y_t-y_p)/y_t))\n",
    "iqr= np.subtract(*np.percentile(y_t, [75, 25]))\n",
    "print('rmse/iqr',rmse/iqr)\n",
    "print('rmse/mean',rmse/np.mean(y_t))\n",
    "print('actual min max', np.min(y_t),np.max(y_t))\n",
    "print('pred min max', np.min(y_p),np.max(y_p))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_area=area_t.copy();p_area=area_p.copy()\n",
    "#plt.plot(p_area.flatten(),t_area.flatten(),'.',color='darkblue',alpha=0.2)\n",
    "plt.plot(t_area[c_idx].flatten(),p_area[c_idx].flatten(),'.',color='deepskyblue',alpha=0.5)\n",
    "##plt.plot(t_area[w_idx].flatten(),p_area[w_idx].flatten(),'.',color='red',alpha=0.3)\n",
    "#plt.ylim(np.min([a_p,a_t]), np.max([a_p,a_t]))\n",
    "#plt.xlim(np.min([a_p,a_t]), np.max([a_p,a_t]))\n",
    "plt.ylim(-0.1,1.1)\n",
    "plt.xlim(-0.1,1.1)\n",
    "plt.xlabel('actual')\n",
    "plt.ylabel('pred')\n",
    "plt.grid()\n",
    "##plt.savefig('images/test_area_c.png', bbox_inches = 'tight')\n",
    "plt.show()\n",
    "\n",
    "y_t=area_t[c_idx].copy();y_p=area_p[c_idx].copy()\n",
    "print(y_t.shape,y_p.shape)#(\n",
    "import sklearn.metrics as metrics\n",
    "mae = metrics.mean_absolute_error(y_t, y_p)\n",
    "mse = metrics.mean_squared_error(y_t, y_p)\n",
    "rmse=np.sqrt(mse)\n",
    "print('mae', mae, '| rmse:',rmse)\n",
    "print('actual mean', np.mean(y_t),'| pred mean',np.mean(y_p))\n",
    "print(np.corrcoef(y_t.flatten(),y_p.flatten()))\n",
    "print('rmse/range',rmse/(np.max(y_t)-np.min(y_t)))\n",
    "print('mape',np.mean(np.abs(y_t-y_p)/y_t))\n",
    "iqr= np.subtract(*np.percentile(y_t, [75, 25]))\n",
    "print('rmse/iqr',rmse/iqr)\n",
    "print('rmse/mean',rmse/np.mean(y_t))\n",
    "print('actual min max', np.min(y_t),np.max(y_t))\n",
    "print('pred min max', np.min(y_p),np.max(y_p))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.metrics as metrics\n",
    "t_area=area_t.copy();p_area=area_p.copy()\n",
    "#plt.plot(p_area.flatten(),t_area.flatten(),'.',color='darkblue',alpha=0.2)\n",
    "##plt.plot(t_area[c_idx].flatten(),p_area[c_idx].flatten(),'.',color='aqua',alpha=0.3)\n",
    "plt.plot(t_area[w_idx].flatten(),p_area[w_idx].flatten(),'.',color='crimson',alpha=0.3)\n",
    "#plt.ylim(np.min([a_p,a_t]), np.max([a_p,a_t]))\n",
    "#plt.xlim(np.min([a_p,a_t]), np.max([a_p,a_t]))\n",
    "plt.ylim(-0.1,1.1)\n",
    "plt.xlim(-0.1,1.1)\n",
    "plt.xlabel('actual')\n",
    "plt.ylabel('pred')\n",
    "plt.grid()\n",
    "##plt.savefig('images/test_area_w.png', bbox_inches = 'tight')\n",
    "plt.show()\n",
    "\n",
    "y_t=area_t[w_idx].copy();y_p=area_p[w_idx].copy()\n",
    "print(y_t.shape,y_p.shape)#(\n",
    "import sklearn.metrics as metrics\n",
    "mae = metrics.mean_absolute_error(y_t, y_p)\n",
    "mse = metrics.mean_squared_error(y_t, y_p)\n",
    "rmse=np.sqrt(mse)\n",
    "print('mae', mae, '| rmse:',rmse)\n",
    "print('actual mean', np.mean(y_t),'| pred mean',np.mean(y_p))\n",
    "print(np.corrcoef(y_t.flatten(),y_p.flatten()))\n",
    "print('rmse/range',rmse/(np.max(y_t)-np.min(y_t)))\n",
    "print('mape',np.mean(np.abs(y_t-y_p)/y_t))\n",
    "iqr= np.subtract(*np.percentile(y_t, [75, 25]))\n",
    "print('rmse/iqr',rmse/iqr)\n",
    "print('rmse/mean',rmse/np.mean(y_t))\n",
    "print('actual min max', np.min(y_t),np.max(y_t))\n",
    "print('pred min max', np.min(y_p),np.max(y_p))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('min, max',np.min(area_t),np.max(area_t))\n",
    "break_case=1e-06\n",
    "damaged_list=np.arange(0.01,0.99,0.1)\n",
    "stable_case=0.9999\n",
    "\n",
    "area_idx=np.where(area_t==break_case)[0]\n",
    "acc_case=np.sum(cn_p[area_idx]==cn_t[area_idx])/len(cn_p[area_idx])\n",
    "print('break_case',acc_case)\n",
    "for i in damaged_list:\n",
    "    low_bound=round(i,3)\n",
    "    upper_bound=round(i+0.09,3)\n",
    "    if upper_bound==1: upper_bound=0.99\n",
    "    area_idx=np.where((area_t>=low_bound) & (area_t<=upper_bound))[0]\n",
    "    acc_case=np.sum(cn_p[area_idx]==cn_t[area_idx])/len(cn_p[area_idx])\n",
    "    print(low_bound,'~',upper_bound,':',acc_case)\n",
    "area_idx=np.where(area_t>=stable_case)[0]\n",
    "acc_case=np.sum(cn_p[area_idx]==cn_t[area_idx])/len(cn_p[area_idx])\n",
    "print('stable_case',acc_case)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "#eval_meter = Meter()\n",
    "cn_p=[];area_p=[];cn_t=[];area_t=[]\n",
    "with torch.no_grad():\n",
    "    for batch_id, batch_data in enumerate(test_loader):\n",
    "        bg,h, labels_cablenumber,labels_area = batch_data\n",
    "        \n",
    "        labels_cablenumber = labels_cablenumber.to(args['device'])\n",
    "        labels_area = labels_area.to(args['device'])\n",
    "        cn_t.append(labels_cablenumber.cpu().detach().numpy())\n",
    "        area_t.append(labels_area.cpu().detach().numpy())\n",
    "        \n",
    "        e = bg.edata.pop('e_feat')\n",
    "        h, e = h.to(args['device']), e.to(args['device'])\n",
    "        prediction = model(bg, h, e)\n",
    "        \n",
    "        \n",
    "        cn_p.append(prediction[0].cpu().detach().numpy())\n",
    "        area_p.append(prediction[1].cpu().detach().numpy())\n",
    "\n",
    "cn_p=np.concatenate(cn_p,axis=0)\n",
    "cn_p=np.argmax(cn_p,axis=1)\n",
    "area_p=np.concatenate(area_p,axis=0)\n",
    "\n",
    "cn_t=np.concatenate(cn_t,axis=0)\n",
    "area_t=np.concatenate(area_t,axis=0)\n",
    "\n",
    "print(cn_p.shape,area_p.shape,cn_t.shape,area_t.shape)\n",
    "\n",
    "acc=np.sum(cn_p==cn_t)/len(cn_p)\n",
    "print('acc: ',acc)\n",
    "#plt.plot(area_t,area_p,'.')\n",
    "\n",
    "w_idx=np.where(cn_p!=cn_t)\n",
    "c_idx=np.where(cn_p==cn_t)\n",
    "\n",
    "area=np.load('data/label_multitask/a_label_test.npz')['arr_0']\n",
    "print('acc mean',np.mean(area[w_idx]),np.mean(area[c_idx]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.metrics as metrics\n",
    "t_area=area_t.copy();p_area=area_p.copy()\n",
    "#plt.plot(p_area.flatten(),t_area.flatten(),'.',color='darkblue',alpha=0.2)\n",
    "plt.plot(t_area[c_idx].flatten(),p_area[c_idx].flatten(),'.',color='deepskyblue',alpha=0.5)\n",
    "plt.plot(t_area[w_idx].flatten(),p_area[w_idx].flatten(),'.',color='crimson',alpha=0.3)\n",
    "#plt.ylim(np.min([a_p,a_t]), np.max([a_p,a_t]))\n",
    "#plt.xlim(np.min([a_p,a_t]), np.max([a_p,a_t]))\n",
    "plt.ylim(-0.1,1.1)\n",
    "plt.xlim(-0.1,1.1)\n",
    "plt.xlabel('actual')\n",
    "plt.ylabel('pred')\n",
    "plt.grid()\n",
    "plt.savefig('images/test_area.png', bbox_inches = 'tight')\n",
    "plt.show()\n",
    "\n",
    "y_t=area_t.copy();y_p=area_p.copy()\n",
    "print(y_t.shape,y_p.shape)#(\n",
    "import sklearn.metrics as metrics\n",
    "mae = metrics.mean_absolute_error(y_t, y_p)\n",
    "mse = metrics.mean_squared_error(y_t, y_p)\n",
    "rmse=np.sqrt(mse)\n",
    "print('mae', mae, '| rmse:',rmse)\n",
    "print('actual mean', np.mean(y_t),'| pred mean',np.mean(y_p))\n",
    "print(np.corrcoef(y_t.flatten(),y_p.flatten()))\n",
    "print('rmse/range',rmse/(np.max(y_t)-np.min(y_t)))\n",
    "print('mape',np.mean(np.abs(y_t-y_p)/y_t))\n",
    "iqr= np.subtract(*np.percentile(y_t, [75, 25]))\n",
    "print('rmse/iqr',rmse/iqr)\n",
    "print('rmse/mean',rmse/np.mean(y_t))\n",
    "print('actual min max', np.min(y_t),np.max(y_t))\n",
    "print('pred min max', np.min(y_p),np.max(y_p))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_area=area_t.copy();p_area=area_p.copy()\n",
    "#plt.plot(p_area.flatten(),t_area.flatten(),'.',color='darkblue',alpha=0.2)\n",
    "plt.plot(t_area[c_idx].flatten(),p_area[c_idx].flatten(),'.',color='deepskyblue',alpha=0.5)\n",
    "##plt.plot(t_area[w_idx].flatten(),p_area[w_idx].flatten(),'.',color='red',alpha=0.3)\n",
    "#plt.ylim(np.min([a_p,a_t]), np.max([a_p,a_t]))\n",
    "#plt.xlim(np.min([a_p,a_t]), np.max([a_p,a_t]))\n",
    "plt.ylim(-0.1,1.1)\n",
    "plt.xlim(-0.1,1.1)\n",
    "plt.xlabel('actual')\n",
    "plt.ylabel('pred')\n",
    "plt.grid()\n",
    "plt.savefig('images/test_area_c.png', bbox_inches = 'tight')\n",
    "plt.show()\n",
    "\n",
    "y_t=area_t[c_idx].copy();y_p=area_p[c_idx].copy()\n",
    "print(y_t.shape,y_p.shape)#(\n",
    "import sklearn.metrics as metrics\n",
    "mae = metrics.mean_absolute_error(y_t, y_p)\n",
    "mse = metrics.mean_squared_error(y_t, y_p)\n",
    "rmse=np.sqrt(mse)\n",
    "print('mae', mae, '| rmse:',rmse)\n",
    "print('actual mean', np.mean(y_t),'| pred mean',np.mean(y_p))\n",
    "print(np.corrcoef(y_t.flatten(),y_p.flatten()))\n",
    "print('rmse/range',rmse/(np.max(y_t)-np.min(y_t)))\n",
    "print('mape',np.mean(np.abs(y_t-y_p)/y_t))\n",
    "iqr= np.subtract(*np.percentile(y_t, [75, 25]))\n",
    "print('rmse/iqr',rmse/iqr)\n",
    "print('rmse/mean',rmse/np.mean(y_t))\n",
    "print('actual min max', np.min(y_t),np.max(y_t))\n",
    "print('pred min max', np.min(y_p),np.max(y_p))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.metrics as metrics\n",
    "t_area=area_t.copy();p_area=area_p.copy()\n",
    "#plt.plot(p_area.flatten(),t_area.flatten(),'.',color='darkblue',alpha=0.2)\n",
    "##plt.plot(t_area[c_idx].flatten(),p_area[c_idx].flatten(),'.',color='aqua',alpha=0.3)\n",
    "plt.plot(t_area[w_idx].flatten(),p_area[w_idx].flatten(),'.',color='crimson',alpha=0.3)\n",
    "#plt.ylim(np.min([a_p,a_t]), np.max([a_p,a_t]))\n",
    "#plt.xlim(np.min([a_p,a_t]), np.max([a_p,a_t]))\n",
    "plt.ylim(-0.1,1.1)\n",
    "plt.xlim(-0.1,1.1)\n",
    "plt.xlabel('actual')\n",
    "plt.ylabel('pred')\n",
    "plt.grid()\n",
    "plt.savefig('images/test_area_w.png', bbox_inches = 'tight')\n",
    "plt.show()\n",
    "\n",
    "y_t=area_t[w_idx].copy();y_p=area_p[w_idx].copy()\n",
    "print(y_t.shape,y_p.shape)#(\n",
    "import sklearn.metrics as metrics\n",
    "mae = metrics.mean_absolute_error(y_t, y_p)\n",
    "mse = metrics.mean_squared_error(y_t, y_p)\n",
    "rmse=np.sqrt(mse)\n",
    "print('mae', mae, '| rmse:',rmse)\n",
    "print('actual mean', np.mean(y_t),'| pred mean',np.mean(y_p))\n",
    "print(np.corrcoef(y_t.flatten(),y_p.flatten()))\n",
    "print('rmse/range',rmse/(np.max(y_t)-np.min(y_t)))\n",
    "print('mape',np.mean(np.abs(y_t-y_p)/y_t))\n",
    "iqr= np.subtract(*np.percentile(y_t, [75, 25]))\n",
    "print('rmse/iqr',rmse/iqr)\n",
    "print('rmse/mean',rmse/np.mean(y_t))\n",
    "print('actual min max', np.min(y_t),np.max(y_t))\n",
    "print('pred min max', np.min(y_p),np.max(y_p))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.min(area_t),np.max(area_t))\n",
    "area_list=[1e-06]+list(np.round(np.linspace(0.01,0.99,99),2))+[0.9999]#np.min(area_t)\n",
    "acc_list=[]\n",
    "for i in area_list:\n",
    "    area_idx=np.where(area_t==i)[0]\n",
    "    acc_list.append(np.sum(cn_p[area_idx]==cn_t[area_idx])/len(cn_p[area_idx]))\n",
    "print(acc_list)\n",
    "\n",
    "plt.figure(figsize=(6,3))\n",
    "plt.plot(area_list,acc_list,'.-',c='darkblue')\n",
    "plt.grid()\n",
    "plt.xlabel('area')\n",
    "plt.ylabel('accuracy')\n",
    "plt.savefig('images/test_acc_list.png', bbox_inches = 'tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "area_idx=np.where(area_t<0.95)[0]\n",
    "np.sum(cn_p[area_idx]==cn_t[area_idx])/len(cn_p[area_idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "area_idx=np.where(area_t<0.90)[0]\n",
    "np.sum(cn_p[area_idx]==cn_t[area_idx])/len(cn_p[area_idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(np.arange(0.01,0.99,0.05))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "damaged_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('min, max',np.min(area_t),np.max(area_t))\n",
    "break_case=1e-06\n",
    "damaged_list=np.arange(0.01,0.99,0.05)\n",
    "stable_case=0.9999#exclude 100% -> it dosen't included in test set\n",
    "\n",
    "area_idx=np.where(area_t==break_case)[0]\n",
    "acc_case=np.sum(cn_p[area_idx]==cn_t[area_idx])/len(cn_p[area_idx])\n",
    "print('break_case',acc_case)\n",
    "for i in damaged_list:\n",
    "    low_bound=round(i,3)\n",
    "    upper_bound=round(i+0.04,3)\n",
    "    if upper_bound==1: upper_bound=0.99\n",
    "    area_idx=np.where((area_t>=low_bound) & (area_t<=upper_bound))[0]\n",
    "    acc_case=np.sum(cn_p[area_idx]==cn_t[area_idx])/len(cn_p[area_idx])\n",
    "    print(low_bound,'~',upper_bound,':',acc_case)\n",
    "area_idx=np.where(area_t==stable_case)[0]\n",
    "acc_case=np.sum(cn_p[area_idx]==cn_t[area_idx])/len(cn_p[area_idx])\n",
    "print('stable_case',acc_case)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.arange(0.01,0.99,0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('min, max',np.min(area_t),np.max(area_t))\n",
    "break_case=1e-06\n",
    "damaged_list=np.arange(0.01,0.99,0.1)\n",
    "stable_case=0.9999#exclude 100% -> it dosen't included in test set\n",
    "\n",
    "area_idx=np.where(area_t==break_case)[0]\n",
    "acc_case=np.sum(cn_p[area_idx]==cn_t[area_idx])/len(cn_p[area_idx])\n",
    "print('break_case',acc_case)\n",
    "for i in damaged_list:\n",
    "    low_bound=round(i,3)\n",
    "    upper_bound=round(i+0.09,3)\n",
    "    if upper_bound==1: upper_bound=0.99\n",
    "    area_idx=np.where((area_t>=low_bound) & (area_t<=upper_bound))[0]\n",
    "    acc_case=np.sum(cn_p[area_idx]==cn_t[area_idx])/len(cn_p[area_idx])\n",
    "    print(low_bound,'~',upper_bound,':',acc_case)\n",
    "area_idx=np.where(area_t==stable_case)[0]\n",
    "acc_case=np.sum(cn_p[area_idx]==cn_t[area_idx])/len(cn_p[area_idx])\n",
    "print('stable_case',acc_case)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sum(cn_t==0),sum(cn_p==0))\n",
    "print(np.where(cn_t==0))\n",
    "print(np.where(cn_p==0))\n",
    "\n",
    "idx_ns=np.setdiff1d(np.where(cn_p==0)[0],np.where(cn_t==0)[0])\n",
    "print(idx_ns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "area_t[idx_ns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sum(cn_t!=0),sum(cn_p!=0))\n",
    "print(np.where(cn_t!=0))\n",
    "print(np.where(cn_p!=0))\n",
    "\n",
    "idx_ns=np.intersect1d(np.where(cn_p!=0)[0],np.where(cn_t!=0)[0])\n",
    "print(len(idx_ns))\n",
    "area_t[idx_ns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(np.where(area_t[idx_ns]>=0.96)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(np.where(area_t[idx_ns]>=0.95)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.where(area_t[idx_ns]>0.95)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "area_t[idx_ns][area_t[idx_ns]>0.95]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm=metrics.confusion_matrix(cn_t,cn_p)\n",
    "df_cm = pd.DataFrame(cm, index = [i for i in range(121)],\n",
    "                  columns = [i for i in range(121)])\n",
    "plt.figure(figsize = (20,15))\n",
    "sn.heatmap(df_cm, cmap=\"Blues\")#, annot=True)\n",
    "plt.ylabel('Actual', fontsize = 15)\n",
    "plt.xlabel('Predicted', fontsize = 15)\n",
    "plt.savefig('images/confusion_test.png', bbox_inches = 'tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cn_list=list(range(1,121))\n",
    "mse_list=[]\n",
    "for i in cn_list:\n",
    "    cn_idx=np.where(cn_t==i)[0]\n",
    "    mse_list.append(metrics.mean_squared_error(area_t[cn_idx], area_p[cn_idx]))\n",
    "print(mse_list)\n",
    "\n",
    "plt.figure(figsize=(8,1.5))\n",
    "plt.bar(cn_list,mse_list,color='darkblue')\n",
    "plt.xlabel('cable')\n",
    "plt.ylabel('MSE')\n",
    "plt.savefig('images/mse_list_test.png', bbox_inches = 'tight')\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.where(np.array(mse_list)>0.03)[0]+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.where(np.array(mse_list)==np.min(mse_list))[0]+1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
