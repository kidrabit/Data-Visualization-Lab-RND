{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d54e588f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.layers import Dense, Conv1D, MaxPooling1D, Flatten, concatenate, Conv2D, MaxPooling2D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4f875d6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN(tf.keras.Model):\n",
    "    def __init__(self, state_size, current_size, mask_size, furniture_size, next_size):\n",
    "        super(DQN, self).__init__()\n",
    "        \n",
    "        self.grid_cnn = Conv2D(filters=5, kernel_size=3, activation='relu', padding='valid', input_shape=state_size)\n",
    "        self.grid_dnn = Dense(32, activation='relu')\n",
    "        \n",
    "        self.cur_grid_cnn = Conv2D(filters=5, kernel_size=3, activation='relu', padding='valid', input_shape=current_size)\n",
    "        self.cur_grid_dnn = Dense(32, activation='relu')\n",
    "        \n",
    "        self.mask_grid_cnn = Conv2D(filters=5, kernel_size=3, activation='relu', padding='valid', input_shape=mask_size)\n",
    "        self.mask_grid_dnn = Dense(32, activation='relu')\n",
    "        \n",
    "        self.size_grid_cnn = Conv2D(filters=5, kernel_size=3, activation='relu', padding='valid', input_shape=furniture_size)\n",
    "        self.size_grid_dnn = Dense(32, activation='relu')\n",
    "        \n",
    "        self.next_grid_cnn = Conv2D(filters=5, kernel_size=3, activation='relu', padding='valid', input_shape=next_size)\n",
    "        self.next_grid_dnn = Dense(32, activation='relu')\n",
    "        \n",
    "        self.f1 = Dense(256, activation='relu')\n",
    "        self.f2 = Dense(128, activation='relu')\n",
    "        self.f = Dense(1)\n",
    "        \n",
    "    def call(self, datas):\n",
    "        grid, cur_grid, mask_grid, size_grid, next_grid = datas[0], datas[1], datas[2], datas[3], datas[4]\n",
    "        \n",
    "        g = self.grid_cnn(g)\n",
    "        g = MaxPooling2D(pool_size=(2,2))(g)\n",
    "        g = Flatten()(g)\n",
    "        g = self.grid_dnn(g)\n",
    "        \n",
    "        c = self.cur_grid_cnn(c)\n",
    "        c = MaxPooling2D(pool_size=(2,2))(c)\n",
    "        c = Flatten()(c)\n",
    "        c = self.cur_grid_dnn(c)\n",
    "        \n",
    "        m = self.mask_grid_cnn(m)\n",
    "        m = MaxPooling2D(pool_size=(2,2))(m)\n",
    "        m = Flatten()(m)\n",
    "        m = self.mask_grid_dnn(m)\n",
    "        \n",
    "        s = self.size_grid_cnn(s)\n",
    "        s = MaxPooling2D(ppol_size=(2,2))(s)\n",
    "        s = Flatten()(s)\n",
    "        s = self.size_grid_dnn(s)\n",
    "        \n",
    "        n = self.next_grid_cnn(n)\n",
    "        n = MaxPooling2D(pool_size=(2,2))(n)\n",
    "        n = Flatten()(n)\n",
    "        n = self.next_grid_dnn(n)\n",
    "        \n",
    "        x = concatenate([g,c,m,s,n])     \n",
    "        x = self.f1(x)\n",
    "        x = self.f2(x)\n",
    "        q = self.f(x)\n",
    "        \n",
    "        return q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0b8f9c89",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from collections import deque"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4b9f152b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    def __init__(self, L=30, B=30, learning_rate=1e-8, exp_steps=500, train_st=200, batch_size=32, memory_len=500, \n",
    "                update_target_rate=30):\n",
    "        self.state_size = (L,B)\n",
    "        self.current_size = (L,B)\n",
    "        self.mask_size = (L,B)\n",
    "        self.furniture_size = (L,B)\n",
    "        self.next_size = (L,B)\n",
    "        \n",
    "        self.discount_factor = 0.99\n",
    "        self.learning_rate = learning_rate\n",
    "        self.epsilon = 1.\n",
    "        self.epsilon_start, self.epsilon_end = 1.0, 0.01\n",
    "        self.exploration_steps = exp_steps\n",
    "        self.epsilon_decay_step = self.epsilon_start - self.epsilon_end\n",
    "        self.epsilon_decay_step /= self.exploration_steps\n",
    "        self.batch_size = batch_size\n",
    "        self.train_start = train_st\n",
    "        self.update_target_rate = update_target_rate\n",
    "        \n",
    "        self.memory = deque(maxlen=memory_len)\n",
    "        self.gamma = 0.9\n",
    "    \n",
    "        self.model = DQN(self.state_size, self.current_size, self.mask_size, self.furniture_size, self.next_size)\n",
    "        self.target_model = DQN(self.state_size, self.current_size, self.mask_size, self.furniture_size, self.next_size)\n",
    "        \n",
    "        self.optimizer = Adam(self.learning_rate)\n",
    "        self.update_target_model()\n",
    "        self.avg_q_max, self.avg_loss = 0,0\n",
    "        self.model_path = os.path.join(os.getcwd(), 'save_mode', 'model_3d')\n",
    "    \n",
    "    def update_target_model(self):\n",
    "        self.target_model.set_weights(self.model.get_weights())\n",
    "        \n",
    "    def get_action(self, state, current, mask, f_size):\n",
    "        if np.random.rand() < self.epsilon:\n",
    "            random_action = random.randrange(len(state))\n",
    "            return random_action\n",
    "        else:\n",
    "            q_values = self.model([state, current, mask, f_size])\n",
    "            argmax_idx = np.where(q_values == tf.math.reduce_max(q_values))\n",
    "            action_idx = argmax_idx[0][0]\n",
    "            return action_idx\n",
    "        \n",
    "    def append_sample(self, history, current, mask, f_size, next_state, reward, last, \n",
    "                      t_history, t_current, t_mask, t_f_size, t_next_state):\n",
    "        self.memory.append((history, current, mask, f_size, next_state, reward, last, \n",
    "                            t_history, t_current, t_mask, t_f_size, t_next_state))\n",
    "        \n",
    "    def train_model(self):\n",
    "        batch = random.sample(self.memory, self.batch_size)\n",
    "        \n",
    "        history = np.array([sample[0] for sample in batch])\n",
    "        current = np.array([sample[1] for sample in batch])\n",
    "        mask = np.array([sample[2] for sample in batch])\n",
    "        f_size = np.array([sample[3] for sample in batch])\n",
    "        next_staet = np.array([sample[4] for sample in batch])\n",
    "        reward = np.array([sample[5] for sample in batch])\n",
    "        last = np.array([sample[6] for sample in batch])\n",
    "        t_history = np.array([sample[7] for sample in batch])\n",
    "        t_current = np.array([sample[8] for sample in batch])\n",
    "        t_mask = np.array([sample[9] for sample in batch])\n",
    "        t_f_size = np.array([sample[10] for sample in batch])\n",
    "        t_next_state = np.array([sample[11] for sample in batch])\n",
    "        \n",
    "        model_params = self.model.trainable_variables\n",
    "        with tf.GradientTape() as tape:\n",
    "            pred = self.model([history, current, mask, f_size, next_state])\n",
    "            targets = []\n",
    "            for i in range(self.batch_size):\n",
    "                t_q = self.target_model([t_history[i], t_current[i], t_mask[i], t_f_size[i], t_next_state[i]])\n",
    "                t_max_q = tf.math.reduce_max(t_q)\n",
    "                targets.append([(1-0.75)*reward[i] + (1-last[i])*0.75*t_max_q])\n",
    "            targets = np.array(targets)\n",
    "            error = tf.abs(targets-predicts)\n",
    "            quadratic_part = tf.clip_by_value(error, 0.0, 1.0)\n",
    "            linear_part = error-quadratic_part\n",
    "            loss = tf.reduce_mean(0.5 * tf.square(quadratic_part) + linear_part)\n",
    "            self.avg_loss += loss.numpy()\n",
    "        \n",
    "        grads = tape.gradient(loss, model_params)\n",
    "        self.optimizer.apply_gradients(zip(grads, model_params))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a533b5d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05818746",
   "metadata": {},
   "source": [
    "## 데이터 로드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c51ba612",
   "metadata": {},
   "outputs": [],
   "source": [
    "+with open('data/ikea.json', 'r') as ikea_json:\n",
    "    ikea_python = json.load(ikea_json)\n",
    "    \n",
    "with open('data/make.json', 'r') as make_json:\n",
    "    make_python = json.load(make_json)\n",
    "    \n",
    "ikea = json.loads(ikea_python)\n",
    "make = json.loads(make_python)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01272bbb",
   "metadata": {},
   "source": [
    "## 환경 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fa1f3441",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = Agent(L=30, B=30, learning_rate=1e-4, exp_steps=9000, train_st=1000, memory_len=1000, batch_size=32,\n",
    "             update_target_rate=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2be3980a",
   "metadata": {},
   "source": [
    "## 반복"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aa4ebef",
   "metadata": {},
   "outputs": [],
   "source": [
    "for e in range(num_episode):\n",
    "    st = time.time()\n",
    "    step = 0 \n",
    "    \n",
    "    if e > 9000:\n",
    "        agent.epsilon = 0\n",
    "    if agent.epsilon > agent.epsilon_end and len(agent.memory) >= agent.train_start:\n",
    "        agent.epsilon -= agent.epsilon_decay_step\n",
    "        \n",
    "    [] [] [] [] []\n",
    "    \n",
    "    for ~\n",
    "        done = False\n",
    "        \n",
    "        append ~\n",
    "        while not done:\n",
    "            state = \n",
    "            current_state = \n",
    "            step += 1\n",
    "            \n",
    "            mask = \n",
    "            \n",
    "            furniture = \n",
    "            furniture_size = \n",
    "            \n",
    "            action_list = []\n",
    "            next_states = state에 action을 하나씩 넣어서 리스트로 만들기\n",
    "            \n",
    "            state, current_state, mask, furniture_size = *len(action_list)\n",
    "            \n",
    "            action_idx = agent.get_action(state, current_state, mask, furniture_size)\n",
    "            "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "learning",
   "language": "python",
   "name": "learning"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
