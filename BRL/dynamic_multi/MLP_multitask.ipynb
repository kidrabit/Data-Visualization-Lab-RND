{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using backend: pytorch\n",
      "/home/shs/anaconda3/lib/python3.7/site-packages/statsmodels/tools/_testing.py:19: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n",
      "  import pandas.util.testing as tm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from dgl import model_zoo,DGLGraph\n",
    "import math\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "#from utils_mpnn import Meter, set_random_seed, collate, EarlyStopping, load_model,load_brl_dataset,regress,run_a_train_epoch,run_an_eval_epoch\n",
    "import argparse\n",
    "# from sklearn import svm, datasets\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# from sklearn.metrics import plot_confusion_matrix\n",
    "import seaborn as sn\n",
    "import pandas as pd\n",
    "from sklearn import metrics\n",
    "import collections\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from utils_mlp import Meter\n",
    "\n",
    "# define NN architecture\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net,self).__init__()\n",
    "        \n",
    "        self.fc1 = nn.Linear(365*7+7, 4096)\n",
    "        self.fc2 = nn.Linear(4096,4096)\n",
    "        self.fc3 = nn.Linear(4096,4096)\n",
    "        self.fc4 = nn.Linear(4096,4096)\n",
    "        self.fc5_1 = nn.Linear(4096,4096)\n",
    "        self.fc5_2 = nn.Linear(4096,4096)\n",
    "        self.fc_ouput1 = nn.Linear(4096,121)\n",
    "        self.fc_output2 = nn.Linear(4096,120)\n",
    "        self.droput = nn.Dropout(0.1)\n",
    "        self.sig=nn.Sigmoid()\n",
    "        \n",
    "    def forward(self,x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.droput(x)\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.droput(x)\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = self.droput(x)\n",
    "        x = F.relu(self.fc4(x))\n",
    "        x = self.droput(x)\n",
    "        \n",
    "        x1 = F.relu(self.fc5_1(x))\n",
    "        x2 = F.relu(self.fc5_2(x))\n",
    "        \n",
    "        x1 = self.fc_ouput1(x1)\n",
    "        x2 = self.sig(self.fc_output2(x2))#x2 = self.fc4_2(x)\n",
    "        return [x1,x2]#x\n",
    "# initialize the NN\n",
    "model = Net()\n",
    "\n",
    "print(torch.cuda.current_device())\n",
    "torch.cuda.set_device(1)\n",
    "print(torch.cuda.current_device())\n",
    "\n",
    "loss_fn_cn =nn.BCEWithLogitsLoss(reduction='none')\n",
    "loss_fn_a =nn.L1Loss(reduction='none')\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "#stopper = EarlyStopping(mode='lower', patience=500, filename='model_saved/kld/early_stop.pth')\n",
    "args={};args['device']=torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "model.to(args['device'])\n",
    "\n",
    "class MyDataset(Dataset):\n",
    "    def __init__(self, data, target_cn, target_a,transform=None):\n",
    "        self.data = torch.from_numpy(data).float()\n",
    "        self.target_cn = torch.from_numpy(target_cn).float()#.long()\n",
    "        self.target_a = torch.from_numpy(target_a).float()\n",
    "        self.transform = transform\n",
    "    def __getitem__(self, index):\n",
    "        x = self.data[index]\n",
    "        y_cn = self.target_cn[index]\n",
    "        y_a = self.target_a[index]\n",
    "        if self.transform:\n",
    "            x = self.transform(x)\n",
    "        return x, y_cn, y_a\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "data_path_x='data/data_mlp/input365/'\n",
    "data_path_label_c='data/data_mlp/label_multitask_bce/';data_path_label_a='data/data_mlp/label/'\n",
    "mode='train'\n",
    "dataset = MyDataset(np.load(data_path_x+'features_'+mode+'.npz')['arr_0'], \n",
    "                    np.load(data_path_label_c+'c_label_'+mode+'.npz')['arr_0'],\n",
    "                    np.load(data_path_label_a+'label_'+mode+'.npz')['arr_0'])\n",
    "train_loader = torch.utils.data.DataLoader(dataset, batch_size=512, shuffle=True,pin_memory=True)\n",
    "\n",
    "mode='val'\n",
    "dataset = MyDataset(np.load(data_path_x+'features_'+mode+'.npz')['arr_0'], \n",
    "                    np.load(data_path_label_c+'c_label_'+mode+'.npz')['arr_0'], \n",
    "                    np.load(data_path_label_a+'label_'+mode+'.npz')['arr_0'])\n",
    "val_loader = torch.utils.data.DataLoader(dataset, batch_size=512, shuffle=False,pin_memory=True)\n",
    "\n",
    "mode='test'\n",
    "dataset = MyDataset(np.load(data_path_x+'features_'+mode+'.npz')['arr_0'],\n",
    "                    np.load(data_path_label_c+'c_label_'+mode+'.npz')['arr_0'],\n",
    "                    np.load(data_path_label_a+'label_'+mode+'.npz')['arr_0'])\n",
    "test_loader = torch.utils.data.DataLoader(dataset, batch_size=512, shuffle=False,pin_memory=True)\n",
    "\n",
    "# initialize tracker for minimum validation loss\n",
    "valid_loss_min = np.Inf  # set initial \"min\" to infinity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 \tTraining Loss: 99.188858 \tacc: 98.398760\tValidation Loss: 38.934124, time5.4\n",
      "Validation loss decreased (inf --> 98.398760).  Saving model ...\n",
      "Epoch: 2 \tTraining Loss: 99.007117 \tacc: 98.679982\tValidation Loss: 38.534096, time5.2\n",
      "Epoch: 3 \tTraining Loss: 99.072161 \tacc: 99.276860\tValidation Loss: 37.878101, time5.2\n",
      "Epoch: 4 \tTraining Loss: 99.075987 \tacc: 98.886593\tValidation Loss: 39.178919, time5.3\n",
      "Epoch: 5 \tTraining Loss: 98.976507 \tacc: 98.949725\tValidation Loss: 38.675041, time5.2\n",
      "Epoch: 6 \tTraining Loss: 98.930594 \tacc: 99.265381\tValidation Loss: 40.346725, time5.3\n",
      "Epoch: 7 \tTraining Loss: 98.833027 \tacc: 98.966942\tValidation Loss: 34.509078, time5.3\n",
      "Epoch: 8 \tTraining Loss: 98.616850 \tacc: 98.329890\tValidation Loss: 39.044914, time5.3\n",
      "Validation loss decreased (98.398760 --> 98.329890).  Saving model ...\n",
      "Epoch: 9 \tTraining Loss: 98.551806 \tacc: 98.433196\tValidation Loss: 35.877952, time5.2\n",
      "Epoch: 10 \tTraining Loss: 98.490588 \tacc: 98.364325\tValidation Loss: 38.608404, time5.3\n",
      "Epoch: 11 \tTraining Loss: 98.588154 \tacc: 98.352847\tValidation Loss: 43.627558, time5.2\n",
      "Epoch: 12 \tTraining Loss: 98.482935 \tacc: 97.928145\tValidation Loss: 37.469348, time5.3\n",
      "Validation loss decreased (98.329890 --> 97.928145).  Saving model ...\n",
      "Epoch: 13 \tTraining Loss: 98.241889 \tacc: 98.467631\tValidation Loss: 36.696334, time5.3\n",
      "Epoch: 14 \tTraining Loss: 98.314585 \tacc: 98.559458\tValidation Loss: 39.965520, time5.3\n",
      "Epoch: 15 \tTraining Loss: 98.027625 \tacc: 98.031451\tValidation Loss: 36.470746, time5.3\n",
      "Epoch: 16 \tTraining Loss: 98.056321 \tacc: 97.669881\tValidation Loss: 36.077223, time5.3\n",
      "Validation loss decreased (97.928145 --> 97.669881).  Saving model ...\n",
      "Epoch: 17 \tTraining Loss: 98.004668 \tacc: 98.008494\tValidation Loss: 37.336856, time5.3\n",
      "Epoch: 18 \tTraining Loss: 97.910927 \tacc: 97.761708\tValidation Loss: 39.883421, time5.3\n",
      "Epoch: 19 \tTraining Loss: 97.761708 \tacc: 97.371442\tValidation Loss: 37.609456, time5.3\n",
      "Validation loss decreased (97.669881 --> 97.371442).  Saving model ...\n",
      "Epoch: 20 \tTraining Loss: 97.752143 \tacc: 98.060147\tValidation Loss: 36.373099, time5.3\n",
      "Epoch: 21 \tTraining Loss: 97.635445 \tacc: 97.939624\tValidation Loss: 37.721915, time5.3\n",
      "Epoch: 22 \tTraining Loss: 97.425008 \tacc: 97.469008\tValidation Loss: 38.534979, time5.3\n",
      "Epoch: 23 \tTraining Loss: 97.262397 \tacc: 97.560836\tValidation Loss: 38.318063, time5.3\n",
      "Epoch: 24 \tTraining Loss: 97.428834 \tacc: 97.296832\tValidation Loss: 37.141764, time5.2\n",
      "Validation loss decreased (97.371442 --> 97.296832).  Saving model ...\n",
      "Epoch: 25 \tTraining Loss: 97.042394 \tacc: 97.044307\tValidation Loss: 39.404899, time5.3\n",
      "Validation loss decreased (97.296832 --> 97.044307).  Saving model ...\n",
      "Epoch: 26 \tTraining Loss: 97.027089 \tacc: 96.992654\tValidation Loss: 36.063496, time5.4\n",
      "Validation loss decreased (97.044307 --> 96.992654).  Saving model ...\n",
      "Epoch: 27 \tTraining Loss: 97.029002 \tacc: 97.239440\tValidation Loss: 37.960622, time5.3\n",
      "Epoch: 28 \tTraining Loss: 97.023263 \tacc: 97.182048\tValidation Loss: 39.795562, time5.3\n",
      "Epoch: 29 \tTraining Loss: 97.004132 \tacc: 97.095960\tValidation Loss: 38.738419, time5.2\n",
      "Epoch: 30 \tTraining Loss: 96.761172 \tacc: 96.734389\tValidation Loss: 37.878859, time5.3\n",
      "Validation loss decreased (96.992654 --> 96.734389).  Saving model ...\n",
      "Epoch: 31 \tTraining Loss: 96.619605 \tacc: 96.608127\tValidation Loss: 38.869061, time5.3\n",
      "Validation loss decreased (96.734389 --> 96.608127).  Saving model ...\n",
      "Epoch: 32 \tTraining Loss: 96.489516 \tacc: 96.900826\tValidation Loss: 39.478169, time5.3\n",
      "Epoch: 33 \tTraining Loss: 96.608127 \tacc: 95.976814\tValidation Loss: 38.163617, time5.3\n",
      "Validation loss decreased (96.608127 --> 95.976814).  Saving model ...\n",
      "Epoch: 34 \tTraining Loss: 96.418733 \tacc: 96.332645\tValidation Loss: 38.217067, time5.2\n",
      "Epoch: 35 \tTraining Loss: 96.150903 \tacc: 95.873508\tValidation Loss: 39.232176, time5.3\n",
      "Validation loss decreased (95.976814 --> 95.873508).  Saving model ...\n",
      "Epoch: 36 \tTraining Loss: 96.196817 \tacc: 96.051423\tValidation Loss: 39.422526, time5.3\n",
      "Epoch: 37 \tTraining Loss: 96.026553 \tacc: 96.372819\tValidation Loss: 39.322050, time5.2\n",
      "Epoch: 38 \tTraining Loss: 95.856290 \tacc: 95.873508\tValidation Loss: 39.095864, time5.3\n",
      "Validation loss decreased (95.873508 --> 95.873508).  Saving model ...\n",
      "Epoch: 39 \tTraining Loss: 95.752984 \tacc: 96.166208\tValidation Loss: 38.719148, time5.3\n",
      "Epoch: 40 \tTraining Loss: 95.712810 \tacc: 95.930900\tValidation Loss: 39.390028, time5.3\n",
      "Epoch: 41 \tTraining Loss: 95.586547 \tacc: 95.115932\tValidation Loss: 39.071880, time5.3\n",
      "Validation loss decreased (95.873508 --> 95.115932).  Saving model ...\n",
      "Epoch: 42 \tTraining Loss: 95.121671 \tacc: 95.173324\tValidation Loss: 39.298729, time5.3\n",
      "Epoch: 43 \tTraining Loss: 95.242195 \tacc: 94.943756\tValidation Loss: 37.811767, time5.3\n",
      "Validation loss decreased (95.115932 --> 94.943756).  Saving model ...\n",
      "Epoch: 44 \tTraining Loss: 95.012626 \tacc: 95.494720\tValidation Loss: 39.114782, time5.2\n",
      "Epoch: 45 \tTraining Loss: 94.916973 \tacc: 95.592287\tValidation Loss: 38.569040, time5.3\n",
      "Epoch: 46 \tTraining Loss: 94.859581 \tacc: 94.932277\tValidation Loss: 39.356668, time5.3\n",
      "Validation loss decreased (94.943756 --> 94.932277).  Saving model ...\n",
      "Epoch: 47 \tTraining Loss: 94.432966 \tacc: 94.628099\tValidation Loss: 38.484385, time5.3\n",
      "Validation loss decreased (94.932277 --> 94.628099).  Saving model ...\n",
      "Epoch: 48 \tTraining Loss: 94.285660 \tacc: 94.364096\tValidation Loss: 38.418383, time5.3\n",
      "Validation loss decreased (94.628099 --> 94.364096).  Saving model ...\n",
      "Epoch: 49 \tTraining Loss: 94.306703 \tacc: 94.117309\tValidation Loss: 38.722147, time5.3\n",
      "Validation loss decreased (94.364096 --> 94.117309).  Saving model ...\n",
      "Epoch: 50 \tTraining Loss: 94.348791 \tacc: 93.950872\tValidation Loss: 40.338781, time5.3\n",
      "Validation loss decreased (94.117309 --> 93.950872).  Saving model ...\n",
      "Epoch: 51 \tTraining Loss: 93.992960 \tacc: 94.151745\tValidation Loss: 38.582316, time5.2\n",
      "Epoch: 52 \tTraining Loss: 93.769131 \tacc: 93.675390\tValidation Loss: 38.224404, time5.3\n",
      "Validation loss decreased (93.950872 --> 93.675390).  Saving model ...\n",
      "Epoch: 53 \tTraining Loss: 93.518519 \tacc: 93.388430\tValidation Loss: 38.835601, time5.3\n",
      "Validation loss decreased (93.675390 --> 93.388430).  Saving model ...\n",
      "Epoch: 54 \tTraining Loss: 93.386517 \tacc: 93.440083\tValidation Loss: 38.762101, time5.3\n",
      "Epoch: 55 \tTraining Loss: 93.424778 \tacc: 93.750000\tValidation Loss: 39.519304, time5.3\n",
      "Epoch: 56 \tTraining Loss: 93.193297 \tacc: 93.600781\tValidation Loss: 38.618842, time5.3\n",
      "Epoch: 57 \tTraining Loss: 92.848944 \tacc: 93.704086\tValidation Loss: 38.924802, time5.2\n",
      "Epoch: 58 \tTraining Loss: 92.847031 \tacc: 93.164601\tValidation Loss: 39.368125, time5.3\n",
      "Validation loss decreased (93.388430 --> 93.164601).  Saving model ...\n",
      "Epoch: 59 \tTraining Loss: 92.657637 \tacc: 93.342516\tValidation Loss: 37.555465, time5.3\n",
      "Epoch: 60 \tTraining Loss: 92.602158 \tacc: 92.567723\tValidation Loss: 40.049362, time5.2\n",
      "Validation loss decreased (93.164601 --> 92.567723).  Saving model ...\n",
      "Epoch: 61 \tTraining Loss: 92.085629 \tacc: 92.912075\tValidation Loss: 38.177609, time5.3\n",
      "Epoch: 62 \tTraining Loss: 91.991889 \tacc: 91.879017\tValidation Loss: 38.889381, time5.3\n",
      "Validation loss decreased (92.567723 --> 91.879017).  Saving model ...\n",
      "Epoch: 63 \tTraining Loss: 91.850321 \tacc: 91.333792\tValidation Loss: 38.670950, time5.3\n",
      "Validation loss decreased (91.879017 --> 91.333792).  Saving model ...\n",
      "Epoch: 64 \tTraining Loss: 91.557622 \tacc: 92.097107\tValidation Loss: 39.411667, time5.3\n",
      "Epoch: 65 \tTraining Loss: 91.364402 \tacc: 91.477273\tValidation Loss: 38.966346, time5.3\n",
      "Epoch: 66 \tTraining Loss: 91.352923 \tacc: 92.280762\tValidation Loss: 38.593371, time5.5\n",
      "Epoch: 67 \tTraining Loss: 91.316575 \tacc: 91.758494\tValidation Loss: 40.183561, time5.5\n",
      "Epoch: 68 \tTraining Loss: 90.949265 \tacc: 91.500230\tValidation Loss: 38.643998, time5.6\n",
      "Epoch: 69 \tTraining Loss: 90.956918 \tacc: 91.683884\tValidation Loss: 38.551016, time5.6\n",
      "Epoch: 70 \tTraining Loss: 90.792394 \tacc: 91.764233\tValidation Loss: 39.590292, time5.6\n",
      "Epoch: 71 \tTraining Loss: 90.914830 \tacc: 90.495868\tValidation Loss: 39.857318, time5.5\n",
      "Validation loss decreased (91.333792 --> 90.495868).  Saving model ...\n",
      "Epoch: 72 \tTraining Loss: 90.484389 \tacc: 92.372590\tValidation Loss: 38.941418, time5.4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 73 \tTraining Loss: 90.524564 \tacc: 90.570478\tValidation Loss: 38.788853, time5.5\n",
      "Epoch: 74 \tTraining Loss: 90.260560 \tacc: 91.018136\tValidation Loss: 40.045322, time5.4\n",
      "Epoch: 75 \tTraining Loss: 90.241429 \tacc: 90.639348\tValidation Loss: 40.075926, time5.5\n",
      "Epoch: 76 \tTraining Loss: 90.008035 \tacc: 90.633609\tValidation Loss: 39.281137, time5.5\n",
      "Epoch: 77 \tTraining Loss: 89.952556 \tacc: 90.576217\tValidation Loss: 39.386281, time5.5\n",
      "Epoch: 78 \tTraining Loss: 89.862642 \tacc: 89.726814\tValidation Loss: 40.540902, time5.5\n",
      "Validation loss decreased (90.495868 --> 89.726814).  Saving model ...\n",
      "Epoch: 79 \tTraining Loss: 89.778466 \tacc: 90.249082\tValidation Loss: 39.854123, time5.3\n",
      "Epoch: 80 \tTraining Loss: 89.506811 \tacc: 90.185950\tValidation Loss: 39.095801, time5.3\n",
      "Epoch: 81 \tTraining Loss: 89.590986 \tacc: 90.518825\tValidation Loss: 39.737841, time5.3\n",
      "Epoch: 82 \tTraining Loss: 89.353765 \tacc: 90.082645\tValidation Loss: 39.931571, time5.3\n",
      "Epoch: 83 \tTraining Loss: 89.198806 \tacc: 89.508724\tValidation Loss: 39.927050, time5.3\n",
      "Validation loss decreased (89.726814 --> 89.508724).  Saving model ...\n",
      "Epoch: 84 \tTraining Loss: 89.271503 \tacc: 89.675161\tValidation Loss: 39.764268, time5.3\n",
      "Epoch: 85 \tTraining Loss: 89.149066 \tacc: 89.818641\tValidation Loss: 39.669669, time5.3\n",
      "Epoch: 86 \tTraining Loss: 89.028543 \tacc: 90.800046\tValidation Loss: 40.869612, time5.2\n",
      "Epoch: 87 \tTraining Loss: 89.089761 \tacc: 89.566116\tValidation Loss: 40.839692, time5.3\n",
      "Epoch: 88 \tTraining Loss: 88.835323 \tacc: 89.376722\tValidation Loss: 40.813769, time5.2\n",
      "Validation loss decreased (89.508724 --> 89.376722).  Saving model ...\n",
      "Epoch: 89 \tTraining Loss: 88.881237 \tacc: 89.204545\tValidation Loss: 41.022549, time5.3\n",
      "Validation loss decreased (89.376722 --> 89.204545).  Saving model ...\n",
      "Epoch: 90 \tTraining Loss: 88.697582 \tacc: 89.129936\tValidation Loss: 41.388915, time5.2\n",
      "Validation loss decreased (89.204545 --> 89.129936).  Saving model ...\n",
      "Epoch: 91 \tTraining Loss: 88.483318 \tacc: 89.284894\tValidation Loss: 40.847751, time5.3\n",
      "Epoch: 92 \tTraining Loss: 88.795148 \tacc: 89.152893\tValidation Loss: 41.230352, time5.2\n",
      "Epoch: 93 \tTraining Loss: 88.303489 \tacc: 89.663682\tValidation Loss: 40.754670, time5.3\n",
      "Epoch: 94 \tTraining Loss: 88.182966 \tacc: 89.313590\tValidation Loss: 41.630531, time5.3\n",
      "Epoch: 95 \tTraining Loss: 88.146618 \tacc: 89.502984\tValidation Loss: 41.210935, time5.3\n",
      "Epoch: 96 \tTraining Loss: 87.984007 \tacc: 89.147153\tValidation Loss: 42.193960, time5.3\n",
      "Epoch: 97 \tTraining Loss: 87.855831 \tacc: 89.422635\tValidation Loss: 41.346158, time5.3\n",
      "Epoch: 98 \tTraining Loss: 87.934267 \tacc: 89.147153\tValidation Loss: 41.337854, time5.3\n",
      "Epoch: 99 \tTraining Loss: 87.760178 \tacc: 89.359504\tValidation Loss: 42.031930, time5.3\n",
      "Epoch: 100 \tTraining Loss: 87.750612 \tacc: 89.279155\tValidation Loss: 42.189217, time5.3\n",
      "Epoch: 101 \tTraining Loss: 87.551653 \tacc: 89.302112\tValidation Loss: 42.067684, time5.2\n",
      "Epoch: 102 \tTraining Loss: 87.526783 \tacc: 88.412534\tValidation Loss: 41.751477, time5.3\n",
      "Validation loss decreased (89.129936 --> 88.412534).  Saving model ...\n",
      "Epoch: 103 \tTraining Loss: 87.515305 \tacc: 89.749770\tValidation Loss: 42.081649, time5.3\n",
      "Epoch: 104 \tTraining Loss: 87.304867 \tacc: 88.314968\tValidation Loss: 43.066649, time5.3\n",
      "Validation loss decreased (88.412534 --> 88.314968).  Saving model ...\n",
      "Epoch: 105 \tTraining Loss: 87.081038 \tacc: 88.510101\tValidation Loss: 42.871171, time5.2\n",
      "Epoch: 106 \tTraining Loss: 87.027472 \tacc: 88.533058\tValidation Loss: 43.127210, time5.3\n",
      "Epoch: 107 \tTraining Loss: 86.964340 \tacc: 89.393939\tValidation Loss: 43.359870, time5.3\n",
      "Epoch: 108 \tTraining Loss: 86.906948 \tacc: 88.888889\tValidation Loss: 43.614775, time5.3\n",
      "Epoch: 109 \tTraining Loss: 87.031298 \tacc: 89.181589\tValidation Loss: 44.041724, time5.2\n",
      "Epoch: 110 \tTraining Loss: 86.906948 \tacc: 88.888889\tValidation Loss: 43.551280, time5.3\n",
      "Epoch: 111 \tTraining Loss: 86.627640 \tacc: 88.148531\tValidation Loss: 43.682377, time5.2\n",
      "Validation loss decreased (88.314968 --> 88.148531).  Saving model ...\n",
      "Epoch: 112 \tTraining Loss: 86.487986 \tacc: 88.418274\tValidation Loss: 43.667089, time5.3\n",
      "Epoch: 113 \tTraining Loss: 86.109198 \tacc: 89.497245\tValidation Loss: 44.319219, time5.3\n",
      "Epoch: 114 \tTraining Loss: 86.457377 \tacc: 88.303489\tValidation Loss: 44.851297, time5.3\n",
      "Epoch: 115 \tTraining Loss: 86.072850 \tacc: 88.475666\tValidation Loss: 45.062517, time5.3\n",
      "Epoch: 116 \tTraining Loss: 85.967631 \tacc: 88.797062\tValidation Loss: 44.457923, time5.3\n",
      "Epoch: 117 \tTraining Loss: 85.831803 \tacc: 88.653581\tValidation Loss: 44.895783, time5.3\n",
      "Epoch: 118 \tTraining Loss: 85.858586 \tacc: 88.854454\tValidation Loss: 45.331312, time5.3\n",
      "Epoch: 119 \tTraining Loss: 85.510407 \tacc: 88.814279\tValidation Loss: 45.549353, time5.3\n",
      "Epoch: 120 \tTraining Loss: 85.391797 \tacc: 88.406795\tValidation Loss: 45.970575, time5.3\n",
      "Epoch: 121 \tTraining Loss: 85.345883 \tacc: 89.129936\tValidation Loss: 46.016784, time5.3\n",
      "Epoch: 122 \tTraining Loss: 85.330579 \tacc: 88.332185\tValidation Loss: 46.247927, time5.3\n",
      "Epoch: 123 \tTraining Loss: 85.034053 \tacc: 88.636364\tValidation Loss: 46.390290, time5.3\n",
      "Epoch: 124 \tTraining Loss: 84.907790 \tacc: 88.355142\tValidation Loss: 47.119037, time5.4\n",
      "Epoch: 125 \tTraining Loss: 84.936486 \tacc: 88.630624\tValidation Loss: 46.826143, time5.5\n",
      "Epoch: 126 \tTraining Loss: 84.990052 \tacc: 88.498623\tValidation Loss: 47.120199, time5.5\n",
      "Epoch: 127 \tTraining Loss: 84.923095 \tacc: 88.923324\tValidation Loss: 47.752882, time5.4\n",
      "Epoch: 128 \tTraining Loss: 84.613177 \tacc: 88.005051\tValidation Loss: 48.072426, time5.5\n",
      "Validation loss decreased (88.148531 --> 88.005051).  Saving model ...\n",
      "Epoch: 129 \tTraining Loss: 84.515611 \tacc: 88.550275\tValidation Loss: 48.036618, time5.5\n",
      "Epoch: 130 \tTraining Loss: 84.314738 \tacc: 88.584711\tValidation Loss: 47.853531, time5.4\n",
      "Epoch: 131 \tTraining Loss: 84.175084 \tacc: 88.292011\tValidation Loss: 49.073509, time5.5\n",
      "Epoch: 132 \tTraining Loss: 84.018212 \tacc: 87.953398\tValidation Loss: 48.358023, time5.4\n",
      "Validation loss decreased (88.005051 --> 87.953398).  Saving model ...\n",
      "Epoch: 133 \tTraining Loss: 84.050735 \tacc: 88.688017\tValidation Loss: 48.748942, time5.5\n",
      "Epoch: 134 \tTraining Loss: 84.259259 \tacc: 88.096878\tValidation Loss: 49.435918, time5.5\n",
      "Epoch: 135 \tTraining Loss: 83.941690 \tacc: 89.204545\tValidation Loss: 49.554902, time5.5\n",
      "Epoch: 136 \tTraining Loss: 83.794383 \tacc: 88.286272\tValidation Loss: 50.084138, time5.5\n",
      "Epoch: 137 \tTraining Loss: 83.476814 \tacc: 88.142792\tValidation Loss: 50.250053, time5.5\n",
      "Epoch: 138 \tTraining Loss: 83.455770 \tacc: 87.609045\tValidation Loss: 49.750262, time5.5\n",
      "Validation loss decreased (87.953398 --> 87.609045).  Saving model ...\n",
      "Epoch: 139 \tTraining Loss: 83.377334 \tacc: 88.412534\tValidation Loss: 50.736834, time5.4\n",
      "Epoch: 140 \tTraining Loss: 83.333333 \tacc: 88.412534\tValidation Loss: 51.363180, time5.5\n",
      "Epoch: 141 \tTraining Loss: 83.130548 \tacc: 89.732553\tValidation Loss: 50.623290, time5.4\n",
      "Epoch: 142 \tTraining Loss: 83.166896 \tacc: 87.614784\tValidation Loss: 51.446667, time5.5\n",
      "Epoch: 143 \tTraining Loss: 83.333333 \tacc: 87.953398\tValidation Loss: 51.335532, time5.4\n",
      "Epoch: 144 \tTraining Loss: 82.935415 \tacc: 87.804178\tValidation Loss: 51.530681, time5.4\n",
      "Epoch: 145 \tTraining Loss: 82.881849 \tacc: 88.297750\tValidation Loss: 51.784108, time5.5\n",
      "Epoch: 146 \tTraining Loss: 82.399755 \tacc: 88.211662\tValidation Loss: 51.954231, time5.4\n",
      "Epoch: 147 \tTraining Loss: 82.394016 \tacc: 87.936180\tValidation Loss: 52.361903, time5.5\n",
      "Epoch: 148 \tTraining Loss: 82.248623 \tacc: 88.578972\tValidation Loss: 53.183519, time5.4\n",
      "Epoch: 149 \tTraining Loss: 82.124273 \tacc: 87.844353\tValidation Loss: 53.517138, time5.5\n",
      "Epoch: 150 \tTraining Loss: 81.925314 \tacc: 88.429752\tValidation Loss: 53.927102, time5.4\n",
      "Epoch: 151 \tTraining Loss: 82.260101 \tacc: 88.481405\tValidation Loss: 53.449457, time5.5\n",
      "Epoch: 152 \tTraining Loss: 81.936792 \tacc: 88.888889\tValidation Loss: 53.703125, time5.4\n",
      "Epoch: 153 \tTraining Loss: 81.898531 \tacc: 88.137052\tValidation Loss: 54.312206, time5.5\n",
      "Epoch: 154 \tTraining Loss: 81.808616 \tacc: 88.217401\tValidation Loss: 54.603565, time5.4\n",
      "Epoch: 155 \tTraining Loss: 81.852617 \tacc: 88.578972\tValidation Loss: 54.698236, time5.5\n",
      "Epoch: 156 \tTraining Loss: 81.431742 \tacc: 87.884527\tValidation Loss: 54.508237, time5.4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 157 \tTraining Loss: 81.330349 \tacc: 87.913223\tValidation Loss: 54.967624, time5.5\n",
      "Epoch: 158 \tTraining Loss: 81.047215 \tacc: 88.871671\tValidation Loss: 55.089102, time5.4\n",
      "Epoch: 159 \tTraining Loss: 81.219391 \tacc: 87.396694\tValidation Loss: 56.180354, time5.4\n",
      "Validation loss decreased (87.609045 --> 87.396694).  Saving model ...\n",
      "Epoch: 160 \tTraining Loss: 81.073998 \tacc: 88.297750\tValidation Loss: 56.098585, time5.6\n",
      "Epoch: 161 \tTraining Loss: 80.842516 \tacc: 88.653581\tValidation Loss: 56.634966, time5.5\n",
      "Epoch: 162 \tTraining Loss: 80.915213 \tacc: 87.947658\tValidation Loss: 55.964241, time5.5\n",
      "Epoch: 163 \tTraining Loss: 80.637818 \tacc: 87.620523\tValidation Loss: 56.857614, time5.5\n",
      "Epoch: 164 \tTraining Loss: 80.586165 \tacc: 88.487144\tValidation Loss: 57.082041, time5.5\n",
      "Epoch: 165 \tTraining Loss: 80.679905 \tacc: 87.844353\tValidation Loss: 57.246802, time5.5\n",
      "Epoch: 166 \tTraining Loss: 80.511555 \tacc: 87.551653\tValidation Loss: 57.724228, time5.5\n",
      "Epoch: 167 \tTraining Loss: 80.157637 \tacc: 88.601928\tValidation Loss: 57.967764, time5.6\n",
      "Epoch: 168 \tTraining Loss: 80.597643 \tacc: 87.947658\tValidation Loss: 58.375019, time5.6\n",
      "Epoch: 169 \tTraining Loss: 79.987374 \tacc: 87.379477\tValidation Loss: 58.378288, time5.6\n",
      "Validation loss decreased (87.396694 --> 87.379477).  Saving model ...\n",
      "Epoch: 170 \tTraining Loss: 80.377640 \tacc: 87.345041\tValidation Loss: 58.631695, time5.4\n",
      "Validation loss decreased (87.379477 --> 87.345041).  Saving model ...\n",
      "Epoch: 171 \tTraining Loss: 79.809458 \tacc: 87.568871\tValidation Loss: 58.517325, time5.4\n",
      "Epoch: 172 \tTraining Loss: 79.947199 \tacc: 88.039486\tValidation Loss: 59.242994, time5.5\n",
      "Epoch: 173 \tTraining Loss: 79.736762 \tacc: 89.485767\tValidation Loss: 59.794216, time5.4\n",
      "Epoch: 174 \tTraining Loss: 79.775023 \tacc: 88.177227\tValidation Loss: 59.553028, time5.5\n",
      "Epoch: 175 \tTraining Loss: 79.537802 \tacc: 88.148531\tValidation Loss: 59.828793, time5.4\n",
      "Epoch: 176 \tTraining Loss: 79.350321 \tacc: 88.096878\tValidation Loss: 60.432877, time5.4\n",
      "Epoch: 177 \tTraining Loss: 79.308234 \tacc: 88.573232\tValidation Loss: 59.684301, time5.4\n",
      "Epoch: 178 \tTraining Loss: 79.195363 \tacc: 87.729568\tValidation Loss: 61.059776, time5.5\n",
      "Epoch: 179 \tTraining Loss: 79.317799 \tacc: 87.798439\tValidation Loss: 60.865900, time5.4\n",
      "Epoch: 180 \tTraining Loss: 79.208754 \tacc: 88.188705\tValidation Loss: 61.562869, time5.5\n",
      "Epoch: 181 \tTraining Loss: 78.812749 \tacc: 88.142792\tValidation Loss: 61.377382, time5.4\n",
      "Epoch: 182 \tTraining Loss: 78.686486 \tacc: 87.855831\tValidation Loss: 62.019342, time5.5\n",
      "Epoch: 183 \tTraining Loss: 78.774487 \tacc: 88.435491\tValidation Loss: 62.884691, time5.4\n",
      "Epoch: 184 \tTraining Loss: 78.634833 \tacc: 88.131313\tValidation Loss: 61.855355, time5.4\n",
      "Epoch: 185 \tTraining Loss: 78.707530 \tacc: 87.511478\tValidation Loss: 62.234416, time5.5\n",
      "Epoch: 186 \tTraining Loss: 78.556397 \tacc: 88.148531\tValidation Loss: 63.146242, time5.4\n",
      "Epoch: 187 \tTraining Loss: 78.552571 \tacc: 88.693756\tValidation Loss: 62.914654, time5.5\n",
      "Epoch: 188 \tTraining Loss: 78.133609 \tacc: 88.194444\tValidation Loss: 63.613402, time5.4\n",
      "Epoch: 189 \tTraining Loss: 78.382308 \tacc: 88.762626\tValidation Loss: 63.768142, time5.5\n",
      "Epoch: 190 \tTraining Loss: 78.114478 \tacc: 87.591827\tValidation Loss: 63.691088, time5.4\n",
      "Epoch: 191 \tTraining Loss: 78.055173 \tacc: 87.557392\tValidation Loss: 64.571692, time5.5\n",
      "Epoch: 192 \tTraining Loss: 78.108739 \tacc: 87.913223\tValidation Loss: 63.366603, time5.4\n",
      "Epoch: 193 \tTraining Loss: 77.846648 \tacc: 87.425390\tValidation Loss: 64.356867, time5.5\n",
      "Epoch: 194 \tTraining Loss: 77.861953 \tacc: 87.706612\tValidation Loss: 65.486906, time5.4\n",
      "Epoch: 195 \tTraining Loss: 77.898301 \tacc: 88.406795\tValidation Loss: 64.944541, time5.5\n",
      "Epoch: 196 \tTraining Loss: 77.785430 \tacc: 87.844353\tValidation Loss: 64.917930, time5.4\n",
      "Epoch: 197 \tTraining Loss: 77.494643 \tacc: 88.125574\tValidation Loss: 65.580068, time5.4\n",
      "Epoch: 198 \tTraining Loss: 77.548209 \tacc: 88.056703\tValidation Loss: 65.176361, time5.5\n",
      "Epoch: 199 \tTraining Loss: 77.576905 \tacc: 88.005051\tValidation Loss: 64.916154, time5.4\n",
      "Epoch: 200 \tTraining Loss: 78.148913 \tacc: 88.791322\tValidation Loss: 64.433973, time5.5\n",
      "Epoch: 201 \tTraining Loss: 77.680211 \tacc: 87.913223\tValidation Loss: 66.889648, time5.4\n",
      "Epoch: 202 \tTraining Loss: 77.265075 \tacc: 87.786961\tValidation Loss: 66.838775, time5.5\n",
      "Epoch: 203 \tTraining Loss: 77.090986 \tacc: 87.689394\tValidation Loss: 67.116763, time5.4\n",
      "Epoch: 204 \tTraining Loss: 76.725589 \tacc: 88.653581\tValidation Loss: 67.266857, time5.5\n",
      "Epoch: 205 \tTraining Loss: 77.037420 \tacc: 88.372360\tValidation Loss: 67.410181, time5.4\n",
      "Epoch: 206 \tTraining Loss: 77.301423 \tacc: 87.884527\tValidation Loss: 66.605179, time5.5\n",
      "Epoch: 207 \tTraining Loss: 76.805938 \tacc: 88.274793\tValidation Loss: 66.930886, time5.4\n",
      "Epoch: 208 \tTraining Loss: 76.622283 \tacc: 88.177227\tValidation Loss: 67.355137, time5.5\n",
      "Epoch: 209 \tTraining Loss: 76.480716 \tacc: 88.016529\tValidation Loss: 68.732002, time5.4\n",
      "Epoch: 210 \tTraining Loss: 76.740894 \tacc: 88.114096\tValidation Loss: 68.332010, time5.4\n",
      "Epoch: 211 \tTraining Loss: 76.406107 \tacc: 88.205923\tValidation Loss: 68.118458, time5.5\n",
      "Epoch: 212 \tTraining Loss: 76.364019 \tacc: 88.085399\tValidation Loss: 69.082903, time5.4\n",
      "Epoch: 213 \tTraining Loss: 76.396541 \tacc: 89.129936\tValidation Loss: 69.202356, time5.5\n",
      "Epoch: 214 \tTraining Loss: 76.371671 \tacc: 88.114096\tValidation Loss: 69.111072, time5.4\n",
      "Epoch: 215 \tTraining Loss: 76.109581 \tacc: 87.815657\tValidation Loss: 69.294709, time5.5\n",
      "Epoch: 216 \tTraining Loss: 76.346801 \tacc: 88.429752\tValidation Loss: 69.280734, time5.4\n",
      "Epoch: 217 \tTraining Loss: 75.757576 \tacc: 88.607668\tValidation Loss: 70.438291, time5.5\n",
      "Epoch: 218 \tTraining Loss: 76.012014 \tacc: 87.746786\tValidation Loss: 69.730962, time5.4\n",
      "Epoch: 219 \tTraining Loss: 75.629400 \tacc: 87.890266\tValidation Loss: 69.668705, time5.5\n",
      "Epoch: 220 \tTraining Loss: 75.839838 \tacc: 88.240358\tValidation Loss: 71.757127, time5.4\n",
      "Epoch: 221 \tTraining Loss: 75.480181 \tacc: 87.901745\tValidation Loss: 70.096999, time5.5\n",
      "Epoch: 222 \tTraining Loss: 75.725054 \tacc: 88.079660\tValidation Loss: 70.975817, time5.4\n",
      "Epoch: 223 \tTraining Loss: 75.361570 \tacc: 88.108356\tValidation Loss: 71.388288, time5.5\n",
      "Epoch: 224 \tTraining Loss: 75.464876 \tacc: 88.016529\tValidation Loss: 70.772521, time5.4\n",
      "Epoch: 225 \tTraining Loss: 75.531833 \tacc: 88.957759\tValidation Loss: 71.007925, time5.4\n",
      "Epoch: 226 \tTraining Loss: 75.401745 \tacc: 88.177227\tValidation Loss: 71.394209, time5.5\n",
      "Epoch: 227 \tTraining Loss: 75.470615 \tacc: 88.085399\tValidation Loss: 71.572945, time5.4\n",
      "Epoch: 228 \tTraining Loss: 75.522268 \tacc: 87.867309\tValidation Loss: 73.028086, time5.5\n",
      "Epoch: 229 \tTraining Loss: 75.837925 \tacc: 88.515840\tValidation Loss: 71.459459, time5.4\n",
      "Epoch: 230 \tTraining Loss: 75.231481 \tacc: 88.171488\tValidation Loss: 72.637188, time5.5\n",
      "Epoch: 231 \tTraining Loss: 74.680517 \tacc: 87.844353\tValidation Loss: 73.574697, time5.4\n",
      "Epoch: 232 \tTraining Loss: 74.998087 \tacc: 88.412534\tValidation Loss: 73.966145, time5.5\n",
      "Epoch: 233 \tTraining Loss: 74.701561 \tacc: 87.792700\tValidation Loss: 73.460346, time5.4\n",
      "Epoch: 234 \tTraining Loss: 74.711126 \tacc: 87.654959\tValidation Loss: 74.209926, time5.5\n",
      "Epoch: 235 \tTraining Loss: 74.816345 \tacc: 88.837236\tValidation Loss: 70.879623, time5.4\n",
      "Epoch: 236 \tTraining Loss: 74.649908 \tacc: 88.205923\tValidation Loss: 75.325761, time5.5\n",
      "Epoch: 237 \tTraining Loss: 74.600168 \tacc: 87.976354\tValidation Loss: 74.224820, time5.4\n",
      "Epoch: 238 \tTraining Loss: 74.707300 \tacc: 87.844353\tValidation Loss: 74.509213, time5.4\n",
      "Epoch: 239 \tTraining Loss: 74.529385 \tacc: 89.416896\tValidation Loss: 73.854499, time5.5\n",
      "Epoch: 240 \tTraining Loss: 74.544689 \tacc: 88.263315\tValidation Loss: 74.779341, time5.4\n",
      "Epoch: 241 \tTraining Loss: 74.213728 \tacc: 88.355142\tValidation Loss: 76.089005, time5.5\n",
      "Epoch: 242 \tTraining Loss: 74.242424 \tacc: 88.079660\tValidation Loss: 75.753103, time5.4\n",
      "Epoch: 243 \tTraining Loss: 74.257729 \tacc: 87.838613\tValidation Loss: 75.475356, time5.5\n",
      "Epoch: 244 \tTraining Loss: 73.965029 \tacc: 89.066804\tValidation Loss: 75.026053, time5.4\n",
      "Epoch: 245 \tTraining Loss: 73.825375 \tacc: 89.187328\tValidation Loss: 75.907628, time5.5\n",
      "Epoch: 246 \tTraining Loss: 73.641720 \tacc: 88.050964\tValidation Loss: 76.559102, time5.4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 247 \tTraining Loss: 73.953551 \tacc: 87.511478\tValidation Loss: 77.404719, time5.5\n",
      "Epoch: 248 \tTraining Loss: 73.953551 \tacc: 87.798439\tValidation Loss: 76.019036, time5.4\n",
      "Epoch: 249 \tTraining Loss: 73.649373 \tacc: 87.729568\tValidation Loss: 75.765388, time5.5\n",
      "Epoch: 250 \tTraining Loss: 73.622590 \tacc: 88.842975\tValidation Loss: 77.182660, time5.4\n",
      "Epoch: 251 \tTraining Loss: 73.660851 \tacc: 89.124197\tValidation Loss: 75.927355, time5.4\n",
      "Epoch: 252 \tTraining Loss: 73.704852 \tacc: 88.154270\tValidation Loss: 77.454641, time5.5\n",
      "Epoch: 253 \tTraining Loss: 73.408326 \tacc: 88.974977\tValidation Loss: 77.494792, time5.4\n",
      "Epoch: 254 \tTraining Loss: 73.551806 \tacc: 88.607668\tValidation Loss: 77.370843, time5.5\n",
      "Epoch: 255 \tTraining Loss: 73.503979 \tacc: 87.591827\tValidation Loss: 78.611247, time5.4\n",
      "Epoch: 256 \tTraining Loss: 73.182583 \tacc: 88.653581\tValidation Loss: 79.466225, time5.5\n",
      "Epoch: 257 \tTraining Loss: 73.433196 \tacc: 87.660698\tValidation Loss: 78.548445, time5.4\n",
      "Epoch: 258 \tTraining Loss: 73.368151 \tacc: 87.632002\tValidation Loss: 77.515271, time5.5\n",
      "Epoch: 259 \tTraining Loss: 73.171105 \tacc: 88.383838\tValidation Loss: 79.297571, time5.4\n",
      "Epoch: 260 \tTraining Loss: 73.220845 \tacc: 87.580349\tValidation Loss: 77.600784, time5.5\n",
      "Epoch: 261 \tTraining Loss: 72.960667 \tacc: 88.114096\tValidation Loss: 79.554055, time5.4\n",
      "Epoch: 262 \tTraining Loss: 72.897536 \tacc: 87.947658\tValidation Loss: 79.564806, time5.5\n",
      "Epoch: 263 \tTraining Loss: 72.880318 \tacc: 88.039486\tValidation Loss: 80.664364, time5.4\n",
      "Epoch: 264 \tTraining Loss: 72.947276 \tacc: 88.234619\tValidation Loss: 78.808819, time5.4\n",
      "Epoch: 265 \tTraining Loss: 73.077365 \tacc: 87.746786\tValidation Loss: 78.024901, time5.5\n",
      "Epoch: 266 \tTraining Loss: 73.157713 \tacc: 88.464187\tValidation Loss: 78.472558, time5.4\n",
      "Epoch: 267 \tTraining Loss: 72.604836 \tacc: 88.877410\tValidation Loss: 80.211260, time5.5\n",
      "Epoch: 268 \tTraining Loss: 73.207453 \tacc: 87.815657\tValidation Loss: 79.725573, time5.4\n",
      "Epoch: 269 \tTraining Loss: 72.543618 \tacc: 88.429752\tValidation Loss: 79.244841, time5.5\n",
      "Epoch: 270 \tTraining Loss: 72.790404 \tacc: 88.412534\tValidation Loss: 81.011673, time5.4\n",
      "Epoch: 271 \tTraining Loss: 72.723447 \tacc: 88.481405\tValidation Loss: 80.659244, time5.5\n",
      "Epoch: 272 \tTraining Loss: 72.308310 \tacc: 88.142792\tValidation Loss: 81.022114, time5.4\n",
      "Epoch: 273 \tTraining Loss: 72.681359 \tacc: 88.401056\tValidation Loss: 80.902676, time5.5\n",
      "Epoch: 274 \tTraining Loss: 72.660315 \tacc: 87.873049\tValidation Loss: 81.292635, time5.4\n",
      "Epoch: 275 \tTraining Loss: 72.338919 \tacc: 88.986455\tValidation Loss: 82.046140, time5.5\n",
      "Epoch: 276 \tTraining Loss: 72.224135 \tacc: 88.441230\tValidation Loss: 81.571905, time5.4\n",
      "Epoch: 277 \tTraining Loss: 72.145699 \tacc: 88.791322\tValidation Loss: 81.993010, time5.4\n",
      "Epoch: 278 \tTraining Loss: 71.707606 \tacc: 88.877410\tValidation Loss: 82.082357, time5.5\n",
      "Epoch: 279 \tTraining Loss: 72.537879 \tacc: 88.619146\tValidation Loss: 81.147277, time5.4\n",
      "Epoch: 280 \tTraining Loss: 72.384833 \tacc: 88.360882\tValidation Loss: 81.774712, time5.5\n",
      "Epoch: 281 \tTraining Loss: 72.478574 \tacc: 88.056703\tValidation Loss: 82.894623, time5.4\n",
      "Epoch: 282 \tTraining Loss: 71.743955 \tacc: 87.695133\tValidation Loss: 81.851335, time5.5\n",
      "Epoch: 283 \tTraining Loss: 72.017524 \tacc: 88.292011\tValidation Loss: 82.918185, time5.4\n",
      "Epoch: 284 \tTraining Loss: 71.807086 \tacc: 88.378099\tValidation Loss: 80.664933, time5.5\n",
      "Epoch: 285 \tTraining Loss: 71.858739 \tacc: 87.792700\tValidation Loss: 81.945181, time5.4\n",
      "Epoch: 286 \tTraining Loss: 71.388124 \tacc: 88.251837\tValidation Loss: 83.943380, time5.5\n",
      "Epoch: 287 \tTraining Loss: 71.353688 \tacc: 89.187328\tValidation Loss: 82.846710, time5.3\n",
      "Epoch: 288 \tTraining Loss: 71.753520 \tacc: 87.918962\tValidation Loss: 83.294074, time5.3\n",
      "Epoch: 289 \tTraining Loss: 70.982553 \tacc: 88.142792\tValidation Loss: 84.798279, time5.3\n",
      "Epoch: 290 \tTraining Loss: 71.290557 \tacc: 88.309229\tValidation Loss: 84.052525, time5.2\n",
      "Epoch: 291 \tTraining Loss: 71.282905 \tacc: 89.606290\tValidation Loss: 83.413961, time5.3\n",
      "Epoch: 292 \tTraining Loss: 71.801347 \tacc: 88.785583\tValidation Loss: 84.350622, time5.3\n",
      "Epoch: 293 \tTraining Loss: 71.361341 \tacc: 88.165748\tValidation Loss: 84.731398, time5.3\n",
      "Epoch: 294 \tTraining Loss: 71.116468 \tacc: 88.188705\tValidation Loss: 83.943512, time5.2\n",
      "Epoch: 295 \tTraining Loss: 70.907943 \tacc: 88.068182\tValidation Loss: 86.422335, time5.3\n",
      "Epoch: 296 \tTraining Loss: 71.160468 \tacc: 87.580349\tValidation Loss: 84.967389, time5.2\n",
      "Epoch: 297 \tTraining Loss: 70.921335 \tacc: 87.821396\tValidation Loss: 86.939634, time5.3\n",
      "Epoch: 298 \tTraining Loss: 71.390037 \tacc: 88.613407\tValidation Loss: 85.381028, time5.3\n",
      "Epoch: 299 \tTraining Loss: 70.626722 \tacc: 88.688017\tValidation Loss: 84.624585, time5.3\n",
      "Epoch: 300 \tTraining Loss: 71.173860 \tacc: 88.280533\tValidation Loss: 86.302353, time5.2\n",
      "Epoch: 301 \tTraining Loss: 71.022727 \tacc: 88.056703\tValidation Loss: 86.071565, time5.3\n",
      "Epoch: 302 \tTraining Loss: 70.165672 \tacc: 88.274793\tValidation Loss: 86.718292, time5.2\n",
      "Epoch: 303 \tTraining Loss: 70.598026 \tacc: 88.642103\tValidation Loss: 86.512856, time5.3\n",
      "Epoch: 304 \tTraining Loss: 71.129859 \tacc: 88.079660\tValidation Loss: 87.086146, time5.2\n",
      "Epoch: 305 \tTraining Loss: 70.578895 \tacc: 87.815657\tValidation Loss: 87.029290, time5.2\n",
      "Epoch: 306 \tTraining Loss: 70.582721 \tacc: 88.802801\tValidation Loss: 87.206963, time5.3\n",
      "Epoch: 307 \tTraining Loss: 70.515764 \tacc: 88.820018\tValidation Loss: 86.963833, time5.3\n",
      "Epoch: 308 \tTraining Loss: 70.689853 \tacc: 88.332185\tValidation Loss: 87.727762, time5.3\n",
      "Epoch: 309 \tTraining Loss: 70.506198 \tacc: 88.073921\tValidation Loss: 87.160663, time5.2\n",
      "Epoch: 310 \tTraining Loss: 70.045148 \tacc: 87.660698\tValidation Loss: 89.552459, time5.3\n",
      "Epoch: 311 \tTraining Loss: 70.360805 \tacc: 88.309229\tValidation Loss: 88.967750, time5.3\n",
      "Epoch: 312 \tTraining Loss: 70.131237 \tacc: 88.091139\tValidation Loss: 88.008625, time5.3\n",
      "Epoch: 313 \tTraining Loss: 70.590373 \tacc: 88.119835\tValidation Loss: 87.216896, time5.2\n",
      "Epoch: 314 \tTraining Loss: 70.268978 \tacc: 88.217401\tValidation Loss: 88.806051, time5.3\n",
      "Epoch: 315 \tTraining Loss: 70.395240 \tacc: 88.263315\tValidation Loss: 88.696517, time5.2\n",
      "Epoch: 316 \tTraining Loss: 70.079584 \tacc: 87.970615\tValidation Loss: 89.702564, time5.3\n",
      "Epoch: 317 \tTraining Loss: 69.884451 \tacc: 88.521579\tValidation Loss: 88.794364, time5.2\n",
      "Epoch: 318 \tTraining Loss: 70.508111 \tacc: 88.504362\tValidation Loss: 87.754979, time5.2\n",
      "Epoch: 319 \tTraining Loss: 70.198194 \tacc: 88.435491\tValidation Loss: 88.938209, time5.3\n",
      "Epoch: 320 \tTraining Loss: 69.960973 \tacc: 88.360882\tValidation Loss: 89.931725, time5.2\n",
      "Epoch: 321 \tTraining Loss: 69.337313 \tacc: 87.815657\tValidation Loss: 90.869100, time5.3\n",
      "Epoch: 322 \tTraining Loss: 69.932277 \tacc: 87.850092\tValidation Loss: 88.426439, time5.2\n",
      "Epoch: 323 \tTraining Loss: 69.207224 \tacc: 88.292011\tValidation Loss: 88.866307, time5.3\n",
      "Epoch: 324 \tTraining Loss: 69.769666 \tacc: 88.653581\tValidation Loss: 89.322914, time5.2\n",
      "Epoch: 325 \tTraining Loss: 69.559229 \tacc: 88.458448\tValidation Loss: 90.105780, time5.3\n",
      "Epoch: 326 \tTraining Loss: 69.339226 \tacc: 87.723829\tValidation Loss: 90.493818, time5.2\n",
      "Epoch: 327 \tTraining Loss: 69.582185 \tacc: 88.349403\tValidation Loss: 91.379253, time5.3\n",
      "Epoch: 328 \tTraining Loss: 69.318182 \tacc: 88.647842\tValidation Loss: 91.346896, time5.2\n",
      "Epoch: 329 \tTraining Loss: 69.452097 \tacc: 88.332185\tValidation Loss: 90.977678, time5.3\n",
      "Epoch: 330 \tTraining Loss: 69.325834 \tacc: 88.997934\tValidation Loss: 91.119500, time5.2\n",
      "Epoch: 331 \tTraining Loss: 70.200107 \tacc: 89.170110\tValidation Loss: 89.554431, time5.2\n",
      "Epoch: 332 \tTraining Loss: 69.281833 \tacc: 88.073921\tValidation Loss: 90.577635, time5.3\n",
      "Epoch: 333 \tTraining Loss: 69.115396 \tacc: 89.101240\tValidation Loss: 91.975353, time5.2\n",
      "Epoch: 334 \tTraining Loss: 69.255051 \tacc: 87.993572\tValidation Loss: 92.975817, time5.3\n",
      "Epoch: 335 \tTraining Loss: 69.186180 \tacc: 88.349403\tValidation Loss: 92.178866, time5.2\n",
      "Epoch: 336 \tTraining Loss: 69.343052 \tacc: 88.865932\tValidation Loss: 91.766079, time5.3\n",
      "Epoch: 337 \tTraining Loss: 69.115396 \tacc: 88.165748\tValidation Loss: 92.642359, time5.2\n",
      "Epoch: 338 \tTraining Loss: 69.038874 \tacc: 88.016529\tValidation Loss: 93.705290, time5.3\n",
      "Epoch: 339 \tTraining Loss: 69.547750 \tacc: 88.205923\tValidation Loss: 93.695011, time5.2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 340 \tTraining Loss: 68.973829 \tacc: 88.223140\tValidation Loss: 92.457582, time5.4\n",
      "Epoch: 341 \tTraining Loss: 68.700260 \tacc: 87.970615\tValidation Loss: 93.943517, time5.2\n",
      "Epoch: 342 \tTraining Loss: 69.197658 \tacc: 88.050964\tValidation Loss: 93.731002, time5.3\n",
      "Epoch: 343 \tTraining Loss: 68.878176 \tacc: 88.630624\tValidation Loss: 93.359552, time5.2\n",
      "Epoch: 344 \tTraining Loss: 68.436257 \tacc: 88.125574\tValidation Loss: 93.285453, time5.2\n",
      "Epoch: 345 \tTraining Loss: 68.593128 \tacc: 88.521579\tValidation Loss: 92.499657, time5.3\n",
      "Epoch: 346 \tTraining Loss: 68.906872 \tacc: 88.710973\tValidation Loss: 93.421481, time5.2\n",
      "Epoch: 347 \tTraining Loss: 68.675390 \tacc: 88.332185\tValidation Loss: 95.079787, time5.3\n",
      "Epoch: 348 \tTraining Loss: 68.784435 \tacc: 89.101240\tValidation Loss: 92.901893, time5.2\n",
      "Epoch: 349 \tTraining Loss: 68.849480 \tacc: 88.860193\tValidation Loss: 93.934056, time5.3\n",
      "Epoch: 350 \tTraining Loss: 68.126339 \tacc: 88.865932\tValidation Loss: 94.136438, time5.2\n",
      "Epoch: 351 \tTraining Loss: 68.485996 \tacc: 87.586088\tValidation Loss: 93.788056, time5.3\n",
      "Epoch: 352 \tTraining Loss: 68.155035 \tacc: 88.314968\tValidation Loss: 94.385740, time5.2\n",
      "Epoch: 353 \tTraining Loss: 68.598867 \tacc: 88.079660\tValidation Loss: 93.479004, time5.3\n",
      "Epoch: 354 \tTraining Loss: 68.432430 \tacc: 88.980716\tValidation Loss: 93.875358, time5.2\n",
      "Epoch: 355 \tTraining Loss: 68.476431 \tacc: 88.911846\tValidation Loss: 94.906093, time5.3\n",
      "Epoch: 356 \tTraining Loss: 68.067034 \tacc: 87.626263\tValidation Loss: 94.525437, time5.2\n",
      "Epoch: 357 \tTraining Loss: 67.919728 \tacc: 88.251837\tValidation Loss: 97.267515, time5.2\n",
      "Epoch: 358 \tTraining Loss: 68.147383 \tacc: 88.527319\tValidation Loss: 95.124064, time5.3\n",
      "Epoch: 359 \tTraining Loss: 68.040251 \tacc: 88.165748\tValidation Loss: 95.230227, time5.2\n",
      "Epoch: 360 \tTraining Loss: 67.913988 \tacc: 88.360882\tValidation Loss: 96.043665, time5.3\n",
      "Epoch: 361 \tTraining Loss: 68.072773 \tacc: 87.672176\tValidation Loss: 97.359682, time5.2\n",
      "Epoch: 362 \tTraining Loss: 68.275559 \tacc: 88.200184\tValidation Loss: 95.181348, time5.3\n",
      "Epoch: 363 \tTraining Loss: 68.422865 \tacc: 88.119835\tValidation Loss: 96.287863, time5.3\n",
      "Epoch: 364 \tTraining Loss: 68.208601 \tacc: 88.412534\tValidation Loss: 96.826799, time5.3\n",
      "Epoch: 365 \tTraining Loss: 67.946511 \tacc: 88.234619\tValidation Loss: 96.942147, time5.2\n",
      "Epoch: 366 \tTraining Loss: 67.762856 \tacc: 88.079660\tValidation Loss: 96.055471, time5.3\n",
      "Epoch: 367 \tTraining Loss: 67.741812 \tacc: 88.234619\tValidation Loss: 98.621251, time5.2\n",
      "Epoch: 368 \tTraining Loss: 67.544766 \tacc: 88.860193\tValidation Loss: 95.483159, time5.3\n",
      "Epoch: 369 \tTraining Loss: 67.936945 \tacc: 88.716713\tValidation Loss: 96.083389, time5.2\n",
      "Epoch: 370 \tTraining Loss: 67.535200 \tacc: 88.280533\tValidation Loss: 98.388410, time5.3\n",
      "Epoch: 371 \tTraining Loss: 67.996250 \tacc: 88.028007\tValidation Loss: 97.135120, time5.2\n",
      "Epoch: 372 \tTraining Loss: 68.164601 \tacc: 87.970615\tValidation Loss: 97.546859, time5.2\n",
      "Epoch: 373 \tTraining Loss: 67.498852 \tacc: 87.918962\tValidation Loss: 98.476219, time5.3\n",
      "Epoch: 374 \tTraining Loss: 67.414677 \tacc: 88.584711\tValidation Loss: 95.136565, time5.2\n",
      "Epoch: 375 \tTraining Loss: 68.049816 \tacc: 89.382461\tValidation Loss: 95.075099, time5.3\n",
      "Epoch: 376 \tTraining Loss: 67.877640 \tacc: 88.188705\tValidation Loss: 97.609086, time5.2\n",
      "Epoch: 377 \tTraining Loss: 67.711203 \tacc: 87.654959\tValidation Loss: 97.898937, time5.3\n",
      "Epoch: 378 \tTraining Loss: 67.638506 \tacc: 87.913223\tValidation Loss: 98.494879, time5.2\n",
      "Epoch: 379 \tTraining Loss: 67.391720 \tacc: 89.606290\tValidation Loss: 97.839073, time5.3\n",
      "Epoch: 380 \tTraining Loss: 67.847031 \tacc: 88.320707\tValidation Loss: 98.021812, time5.3\n",
      "Epoch: 381 \tTraining Loss: 66.794842 \tacc: 88.831497\tValidation Loss: 100.132261, time5.3\n",
      "Epoch: 382 \tTraining Loss: 67.655724 \tacc: 87.936180\tValidation Loss: 98.928369, time5.2\n",
      "Epoch: 383 \tTraining Loss: 67.628941 \tacc: 87.620523\tValidation Loss: 99.054204, time5.3\n",
      "Epoch: 384 \tTraining Loss: 66.842669 \tacc: 88.820018\tValidation Loss: 99.971633, time5.2\n",
      "Epoch: 385 \tTraining Loss: 67.152586 \tacc: 88.716713\tValidation Loss: 98.610313, time5.2\n",
      "Epoch: 386 \tTraining Loss: 67.152586 \tacc: 88.636364\tValidation Loss: 97.633767, time5.3\n",
      "Epoch: 387 \tTraining Loss: 67.688246 \tacc: 87.970615\tValidation Loss: 98.130083, time5.2\n",
      "Epoch: 388 \tTraining Loss: 66.894322 \tacc: 87.735308\tValidation Loss: 99.862148, time5.3\n",
      "Epoch: 389 \tTraining Loss: 67.502678 \tacc: 87.809917\tValidation Loss: 99.583046, time5.2\n",
      "Epoch: 390 \tTraining Loss: 66.938323 \tacc: 88.682277\tValidation Loss: 100.290541, time5.3\n",
      "Epoch: 391 \tTraining Loss: 67.020585 \tacc: 87.402433\tValidation Loss: 100.300046, time5.2\n",
      "Epoch: 392 \tTraining Loss: 67.026324 \tacc: 88.050964\tValidation Loss: 98.998522, time5.3\n",
      "Epoch: 393 \tTraining Loss: 66.976584 \tacc: 87.987833\tValidation Loss: 102.056167, time5.2\n",
      "Epoch: 394 \tTraining Loss: 66.743189 \tacc: 87.918962\tValidation Loss: 101.930807, time5.3\n",
      "Epoch: 395 \tTraining Loss: 66.557622 \tacc: 87.959137\tValidation Loss: 101.494646, time5.3\n",
      "Epoch: 396 \tTraining Loss: 67.332415 \tacc: 87.792700\tValidation Loss: 99.539091, time5.3\n",
      "Epoch: 397 \tTraining Loss: 66.343358 \tacc: 88.963499\tValidation Loss: 101.073574, time5.2\n",
      "Epoch: 398 \tTraining Loss: 66.513621 \tacc: 88.039486\tValidation Loss: 102.935238, time5.2\n",
      "Epoch: 399 \tTraining Loss: 66.406489 \tacc: 88.251837\tValidation Loss: 100.538486, time5.3\n",
      "Epoch: 400 \tTraining Loss: 66.777625 \tacc: 88.366621\tValidation Loss: 102.413967, time5.2\n",
      "Epoch: 401 \tTraining Loss: 66.544230 \tacc: 88.883150\tValidation Loss: 100.594908, time5.3\n",
      "Epoch: 402 \tTraining Loss: 66.863713 \tacc: 88.263315\tValidation Loss: 102.159137, time5.2\n",
      "Epoch: 403 \tTraining Loss: 65.937787 \tacc: 89.353765\tValidation Loss: 103.424077, time5.3\n",
      "Epoch: 404 \tTraining Loss: 66.724059 \tacc: 89.336547\tValidation Loss: 102.006837, time5.2\n",
      "Epoch: 405 \tTraining Loss: 66.343358 \tacc: 88.504362\tValidation Loss: 102.561659, time5.3\n",
      "Epoch: 406 \tTraining Loss: 66.722146 \tacc: 88.544536\tValidation Loss: 102.222463, time5.2\n",
      "Epoch: 407 \tTraining Loss: 66.245791 \tacc: 87.861570\tValidation Loss: 103.173733, time5.3\n",
      "Epoch: 408 \tTraining Loss: 66.318488 \tacc: 88.992195\tValidation Loss: 101.762655, time5.2\n",
      "Epoch: 409 \tTraining Loss: 67.206152 \tacc: 88.533058\tValidation Loss: 102.238151, time5.3\n",
      "Epoch: 410 \tTraining Loss: 67.064585 \tacc: 88.280533\tValidation Loss: 102.057321, time5.2\n",
      "Epoch: 411 \tTraining Loss: 66.896235 \tacc: 87.781221\tValidation Loss: 101.499115, time5.2\n",
      "Epoch: 412 \tTraining Loss: 66.496403 \tacc: 88.429752\tValidation Loss: 101.107063, time5.3\n",
      "Epoch: 413 \tTraining Loss: 66.831191 \tacc: 87.896006\tValidation Loss: 103.077587, time5.2\n",
      "Epoch: 414 \tTraining Loss: 66.081267 \tacc: 88.441230\tValidation Loss: 102.130866, time5.3\n",
      "Epoch: 415 \tTraining Loss: 66.393098 \tacc: 88.303489\tValidation Loss: 102.441626, time5.3\n",
      "Epoch: 416 \tTraining Loss: 65.809611 \tacc: 89.256198\tValidation Loss: 103.795984, time5.3\n",
      "Epoch: 417 \tTraining Loss: 66.255357 \tacc: 88.791322\tValidation Loss: 105.222165, time5.2\n",
      "Epoch: 418 \tTraining Loss: 66.264922 \tacc: 87.689394\tValidation Loss: 103.257795, time5.3\n",
      "Epoch: 419 \tTraining Loss: 65.576217 \tacc: 89.101240\tValidation Loss: 104.852635, time5.2\n",
      "Epoch: 420 \tTraining Loss: 66.299357 \tacc: 87.666437\tValidation Loss: 103.451632, time5.3\n",
      "Epoch: 421 \tTraining Loss: 65.637435 \tacc: 88.607668\tValidation Loss: 103.516412, time5.2\n",
      "Epoch: 422 \tTraining Loss: 66.213269 \tacc: 88.085399\tValidation Loss: 104.509816, time5.3\n",
      "Epoch: 423 \tTraining Loss: 66.354836 \tacc: 88.607668\tValidation Loss: 102.977278, time5.3\n",
      "Epoch: 424 \tTraining Loss: 65.746480 \tacc: 88.062443\tValidation Loss: 104.024547, time5.2\n",
      "Epoch: 425 \tTraining Loss: 66.002831 \tacc: 88.556015\tValidation Loss: 103.806444, time5.3\n",
      "Epoch: 426 \tTraining Loss: 65.918656 \tacc: 88.056703\tValidation Loss: 104.985871, time5.2\n",
      "Epoch: 427 \tTraining Loss: 65.568564 \tacc: 88.223140\tValidation Loss: 105.284509, time5.3\n",
      "Epoch: 428 \tTraining Loss: 65.962657 \tacc: 88.401056\tValidation Loss: 105.181653, time5.2\n",
      "Epoch: 429 \tTraining Loss: 65.386823 \tacc: 88.280533\tValidation Loss: 104.642570, time5.3\n",
      "Epoch: 430 \tTraining Loss: 65.513085 \tacc: 88.441230\tValidation Loss: 105.026289, time5.2\n",
      "Epoch: 431 \tTraining Loss: 66.111876 \tacc: 88.234619\tValidation Loss: 104.438321, time5.3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 432 \tTraining Loss: 65.930135 \tacc: 88.814279\tValidation Loss: 103.652130, time5.3\n",
      "Epoch: 433 \tTraining Loss: 65.769437 \tacc: 88.550275\tValidation Loss: 103.323879, time5.3\n",
      "Epoch: 434 \tTraining Loss: 66.018136 \tacc: 87.907484\tValidation Loss: 105.531981, time5.2\n",
      "Epoch: 435 \tTraining Loss: 65.298822 \tacc: 88.360882\tValidation Loss: 105.478004, time5.3\n",
      "Epoch: 436 \tTraining Loss: 65.786654 \tacc: 88.797062\tValidation Loss: 104.713591, time5.3\n",
      "Epoch: 437 \tTraining Loss: 65.118993 \tacc: 87.964876\tValidation Loss: 105.582696, time5.2\n",
      "Epoch: 438 \tTraining Loss: 65.991353 \tacc: 87.798439\tValidation Loss: 105.836448, time5.3\n",
      "Epoch: 439 \tTraining Loss: 65.162994 \tacc: 88.567493\tValidation Loss: 106.526159, time5.2\n",
      "Epoch: 440 \tTraining Loss: 64.956382 \tacc: 88.125574\tValidation Loss: 106.479146, time5.3\n",
      "Epoch: 441 \tTraining Loss: 65.084558 \tacc: 87.815657\tValidation Loss: 108.379413, time5.2\n",
      "Epoch: 442 \tTraining Loss: 64.962121 \tacc: 87.804178\tValidation Loss: 106.346310, time5.3\n",
      "Epoch: 443 \tTraining Loss: 65.566651 \tacc: 88.045225\tValidation Loss: 108.418742, time5.2\n",
      "Epoch: 444 \tTraining Loss: 65.266299 \tacc: 88.045225\tValidation Loss: 106.841967, time5.3\n",
      "Epoch: 445 \tTraining Loss: 65.017600 \tacc: 87.953398\tValidation Loss: 107.848996, time5.2\n",
      "Epoch: 446 \tTraining Loss: 65.185950 \tacc: 88.647842\tValidation Loss: 107.773602, time5.3\n",
      "Epoch: 447 \tTraining Loss: 64.803336 \tacc: 88.665060\tValidation Loss: 107.914163, time5.3\n",
      "Epoch: 448 \tTraining Loss: 65.184037 \tacc: 88.332185\tValidation Loss: 106.967378, time5.3\n",
      "Epoch: 449 \tTraining Loss: 65.453780 \tacc: 87.746786\tValidation Loss: 106.269480, time5.2\n",
      "Epoch: 450 \tTraining Loss: 65.421258 \tacc: 88.160009\tValidation Loss: 106.319951, time5.3\n",
      "Epoch: 451 \tTraining Loss: 65.652740 \tacc: 87.970615\tValidation Loss: 106.999049, time5.2\n",
      "Epoch: 452 \tTraining Loss: 65.117080 \tacc: 88.733930\tValidation Loss: 106.780369, time5.2\n",
      "Epoch: 453 \tTraining Loss: 65.671870 \tacc: 87.901745\tValidation Loss: 108.116123, time5.3\n",
      "Epoch: 454 \tTraining Loss: 65.229951 \tacc: 88.309229\tValidation Loss: 107.771398, time5.3\n",
      "Epoch: 455 \tTraining Loss: 64.904729 \tacc: 87.672176\tValidation Loss: 108.581146, time5.3\n",
      "Epoch: 456 \tTraining Loss: 64.897077 \tacc: 87.982094\tValidation Loss: 108.990922, time5.2\n",
      "Epoch: 457 \tTraining Loss: 65.042470 \tacc: 87.976354\tValidation Loss: 108.963792, time5.3\n",
      "Epoch: 458 \tTraining Loss: 65.052036 \tacc: 88.630624\tValidation Loss: 107.938337, time5.2\n",
      "Epoch: 459 \tTraining Loss: 65.490129 \tacc: 88.676538\tValidation Loss: 108.238231, time5.3\n",
      "Epoch: 460 \tTraining Loss: 65.547521 \tacc: 88.412534\tValidation Loss: 108.313897, time5.2\n",
      "Epoch: 461 \tTraining Loss: 64.627334 \tacc: 88.096878\tValidation Loss: 108.477500, time5.3\n",
      "Epoch: 462 \tTraining Loss: 64.820554 \tacc: 87.385216\tValidation Loss: 109.854022, time5.2\n",
      "Epoch: 463 \tTraining Loss: 64.810989 \tacc: 87.304867\tValidation Loss: 110.394566, time5.3\n",
      "Validation loss decreased (87.345041 --> 87.304867).  Saving model ...\n",
      "Epoch: 464 \tTraining Loss: 64.623508 \tacc: 88.200184\tValidation Loss: 110.337214, time5.2\n",
      "Epoch: 465 \tTraining Loss: 65.107515 \tacc: 87.775482\tValidation Loss: 108.549034, time5.3\n",
      "Epoch: 466 \tTraining Loss: 64.633073 \tacc: 87.976354\tValidation Loss: 108.698165, time5.3\n",
      "Epoch: 467 \tTraining Loss: 65.074992 \tacc: 88.125574\tValidation Loss: 110.211124, time5.3\n",
      "Epoch: 468 \tTraining Loss: 64.893251 \tacc: 87.586088\tValidation Loss: 108.629393, time5.3\n",
      "Epoch: 469 \tTraining Loss: 64.545072 \tacc: 88.900367\tValidation Loss: 108.794628, time5.3\n",
      "Epoch: 470 \tTraining Loss: 65.046296 \tacc: 88.854454\tValidation Loss: 108.048466, time5.3\n",
      "Epoch: 471 \tTraining Loss: 64.967860 \tacc: 88.091139\tValidation Loss: 109.217008, time5.3\n",
      "Epoch: 472 \tTraining Loss: 64.990817 \tacc: 87.959137\tValidation Loss: 108.793962, time5.3\n",
      "Epoch: 473 \tTraining Loss: 64.112718 \tacc: 87.752525\tValidation Loss: 107.612109, time5.3\n",
      "Epoch: 474 \tTraining Loss: 65.094123 \tacc: 88.527319\tValidation Loss: 109.379679, time5.3\n",
      "Epoch: 475 \tTraining Loss: 64.175849 \tacc: 87.936180\tValidation Loss: 110.455351, time5.2\n",
      "Epoch: 476 \tTraining Loss: 64.990817 \tacc: 88.303489\tValidation Loss: 109.363922, time5.3\n",
      "Epoch: 477 \tTraining Loss: 64.596725 \tacc: 88.108356\tValidation Loss: 109.881289, time5.3\n",
      "Epoch: 478 \tTraining Loss: 64.640725 \tacc: 88.688017\tValidation Loss: 112.026427, time5.3\n",
      "Epoch: 479 \tTraining Loss: 64.005586 \tacc: 87.718090\tValidation Loss: 112.693532, time5.3\n",
      "Epoch: 480 \tTraining Loss: 64.227502 \tacc: 87.936180\tValidation Loss: 111.972920, time5.3\n",
      "Epoch: 481 \tTraining Loss: 64.332721 \tacc: 88.693756\tValidation Loss: 110.156913, time5.3\n",
      "Epoch: 482 \tTraining Loss: 64.700031 \tacc: 87.959137\tValidation Loss: 108.786950, time5.3\n",
      "Epoch: 483 \tTraining Loss: 64.053413 \tacc: 88.028007\tValidation Loss: 109.354493, time5.3\n",
      "Epoch: 484 \tTraining Loss: 64.441766 \tacc: 87.505739\tValidation Loss: 111.110299, time5.3\n",
      "Epoch: 485 \tTraining Loss: 63.867845 \tacc: 87.964876\tValidation Loss: 112.041517, time5.2\n",
      "Epoch: 486 \tTraining Loss: 64.642639 \tacc: 87.821396\tValidation Loss: 110.417130, time5.3\n",
      "Epoch: 487 \tTraining Loss: 64.223676 \tacc: 88.096878\tValidation Loss: 111.916199, time5.3\n",
      "Epoch: 488 \tTraining Loss: 64.099327 \tacc: 88.137052\tValidation Loss: 115.097026, time5.3\n",
      "Epoch: 489 \tTraining Loss: 64.196893 \tacc: 88.676538\tValidation Loss: 111.287869, time5.3\n",
      "Epoch: 490 \tTraining Loss: 64.139501 \tacc: 87.896006\tValidation Loss: 116.462253, time5.3\n",
      "Epoch: 491 \tTraining Loss: 63.569406 \tacc: 87.540174\tValidation Loss: 112.569628, time5.3\n",
      "Epoch: 492 \tTraining Loss: 64.003673 \tacc: 87.769743\tValidation Loss: 111.473021, time5.3\n",
      "Epoch: 493 \tTraining Loss: 64.145240 \tacc: 88.360882\tValidation Loss: 110.692257, time5.3\n",
      "Epoch: 494 \tTraining Loss: 63.760713 \tacc: 88.556015\tValidation Loss: 111.929276, time5.3\n",
      "Epoch: 495 \tTraining Loss: 64.359504 \tacc: 87.769743\tValidation Loss: 110.456620, time5.3\n",
      "Epoch: 496 \tTraining Loss: 64.242807 \tacc: 90.490129\tValidation Loss: 112.179230, time5.3\n",
      "Epoch: 497 \tTraining Loss: 65.134298 \tacc: 88.366621\tValidation Loss: 110.109719, time5.3\n",
      "Epoch: 498 \tTraining Loss: 64.780380 \tacc: 87.976354\tValidation Loss: 111.824382, time5.3\n",
      "Epoch: 499 \tTraining Loss: 64.414983 \tacc: 87.574610\tValidation Loss: 110.552927, time5.3\n",
      "Epoch: 500 \tTraining Loss: 64.248546 \tacc: 89.003673\tValidation Loss: 112.031301, time5.3\n",
      "Epoch: 501 \tTraining Loss: 64.759336 \tacc: 88.039486\tValidation Loss: 110.112758, time5.3\n",
      "Epoch: 502 \tTraining Loss: 64.311677 \tacc: 87.999311\tValidation Loss: 111.787408, time5.3\n",
      "Epoch: 503 \tTraining Loss: 64.084022 \tacc: 88.481405\tValidation Loss: 109.590616, time5.3\n",
      "Epoch: 504 \tTraining Loss: 63.938629 \tacc: 87.591827\tValidation Loss: 114.120996, time5.3\n",
      "Epoch: 505 \tTraining Loss: 64.273416 \tacc: 87.568871\tValidation Loss: 110.908962, time5.3\n",
      "Epoch: 506 \tTraining Loss: 64.805249 \tacc: 88.280533\tValidation Loss: 112.047464, time5.3\n",
      "Epoch: 507 \tTraining Loss: 64.164371 \tacc: 87.723829\tValidation Loss: 113.265277, time5.2\n",
      "Epoch: 508 \tTraining Loss: 64.305938 \tacc: 87.941919\tValidation Loss: 112.073064, time5.3\n",
      "Epoch: 509 \tTraining Loss: 63.603841 \tacc: 87.700872\tValidation Loss: 114.158196, time5.3\n",
      "Epoch: 510 \tTraining Loss: 64.313590 \tacc: 88.234619\tValidation Loss: 114.326374, time5.3\n",
      "Epoch: 511 \tTraining Loss: 63.858280 \tacc: 87.729568\tValidation Loss: 112.946467, time5.3\n",
      "Epoch: 512 \tTraining Loss: 64.290634 \tacc: 87.385216\tValidation Loss: 113.095953, time5.3\n",
      "Epoch: 513 \tTraining Loss: 64.074457 \tacc: 88.102617\tValidation Loss: 113.544927, time5.3\n",
      "Epoch: 514 \tTraining Loss: 63.544536 \tacc: 87.568871\tValidation Loss: 114.199514, time5.3\n",
      "Epoch: 515 \tTraining Loss: 63.733930 \tacc: 87.505739\tValidation Loss: 115.623267, time5.3\n",
      "Epoch: 516 \tTraining Loss: 63.959672 \tacc: 87.970615\tValidation Loss: 115.709926, time5.3\n",
      "Epoch: 517 \tTraining Loss: 63.079660 \tacc: 88.797062\tValidation Loss: 115.631719, time5.3\n",
      "Epoch: 518 \tTraining Loss: 64.139501 \tacc: 88.045225\tValidation Loss: 113.516967, time5.2\n",
      "Epoch: 519 \tTraining Loss: 63.636364 \tacc: 87.522957\tValidation Loss: 113.546396, time5.3\n",
      "Epoch: 520 \tTraining Loss: 63.496710 \tacc: 88.297750\tValidation Loss: 115.777248, time5.3\n",
      "Epoch: 521 \tTraining Loss: 63.800888 \tacc: 87.637741\tValidation Loss: 113.056017, time5.3\n",
      "Epoch: 522 \tTraining Loss: 63.904193 \tacc: 87.356520\tValidation Loss: 114.333495, time5.3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 523 \tTraining Loss: 63.418274 \tacc: 87.907484\tValidation Loss: 113.138125, time5.3\n",
      "Epoch: 524 \tTraining Loss: 64.365243 \tacc: 87.947658\tValidation Loss: 114.423532, time5.3\n",
      "Epoch: 525 \tTraining Loss: 63.265228 \tacc: 87.568871\tValidation Loss: 115.090551, time5.3\n",
      "Epoch: 526 \tTraining Loss: 63.655494 \tacc: 88.722452\tValidation Loss: 111.877335, time5.3\n",
      "Epoch: 527 \tTraining Loss: 64.372896 \tacc: 87.718090\tValidation Loss: 115.768393, time5.3\n",
      "Epoch: 528 \tTraining Loss: 63.280533 \tacc: 87.718090\tValidation Loss: 115.357876, time5.3\n",
      "Epoch: 529 \tTraining Loss: 64.055326 \tacc: 87.494261\tValidation Loss: 114.579076, time5.3\n",
      "Epoch: 530 \tTraining Loss: 63.485231 \tacc: 87.850092\tValidation Loss: 114.082229, time5.3\n",
      "Epoch: 531 \tTraining Loss: 63.601928 \tacc: 87.505739\tValidation Loss: 113.248017, time5.3\n",
      "Epoch: 532 \tTraining Loss: 63.611494 \tacc: 88.337925\tValidation Loss: 112.189540, time5.3\n",
      "Epoch: 533 \tTraining Loss: 63.974977 \tacc: 88.699495\tValidation Loss: 114.494201, time5.3\n",
      "Epoch: 534 \tTraining Loss: 63.073921 \tacc: 89.795684\tValidation Loss: 116.562864, time5.3\n",
      "Epoch: 535 \tTraining Loss: 63.640190 \tacc: 87.873049\tValidation Loss: 116.566795, time5.3\n",
      "Epoch: 536 \tTraining Loss: 63.523493 \tacc: 87.798439\tValidation Loss: 115.432112, time5.3\n",
      "Epoch: 537 \tTraining Loss: 62.957224 \tacc: 88.251837\tValidation Loss: 116.912755, time5.3\n",
      "Epoch: 538 \tTraining Loss: 63.177227 \tacc: 87.454086\tValidation Loss: 114.471831, time5.3\n",
      "Epoch: 539 \tTraining Loss: 62.492348 \tacc: 87.677916\tValidation Loss: 116.777012, time5.2\n",
      "Epoch: 540 \tTraining Loss: 63.506275 \tacc: 87.511478\tValidation Loss: 117.057433, time5.3\n",
      "Epoch: 541 \tTraining Loss: 62.936180 \tacc: 88.573232\tValidation Loss: 117.558182, time5.3\n",
      "Epoch: 542 \tTraining Loss: 63.228880 \tacc: 87.167126\tValidation Loss: 117.057606, time5.3\n",
      "Validation loss decreased (87.304867 --> 87.167126).  Saving model ...\n",
      "Epoch: 543 \tTraining Loss: 63.041399 \tacc: 87.947658\tValidation Loss: 117.804685, time5.3\n",
      "Epoch: 544 \tTraining Loss: 62.658785 \tacc: 87.832874\tValidation Loss: 116.015308, time5.2\n",
      "Epoch: 545 \tTraining Loss: 63.274793 \tacc: 87.511478\tValidation Loss: 118.523240, time5.3\n",
      "Epoch: 546 \tTraining Loss: 63.249923 \tacc: 87.706612\tValidation Loss: 115.856216, time5.2\n",
      "Epoch: 547 \tTraining Loss: 62.811830 \tacc: 88.573232\tValidation Loss: 117.611353, time5.3\n",
      "Epoch: 548 \tTraining Loss: 63.309229 \tacc: 88.182966\tValidation Loss: 115.425225, time5.2\n",
      "Epoch: 549 \tTraining Loss: 62.836700 \tacc: 88.475666\tValidation Loss: 115.158797, time5.2\n",
      "Epoch: 550 \tTraining Loss: 62.679829 \tacc: 87.459826\tValidation Loss: 117.929140, time5.3\n",
      "Epoch: 551 \tTraining Loss: 63.049051 \tacc: 87.964876\tValidation Loss: 114.969296, time5.2\n",
      "Epoch: 552 \tTraining Loss: 62.626263 \tacc: 88.705234\tValidation Loss: 117.034754, time5.3\n",
      "Epoch: 553 \tTraining Loss: 63.070095 \tacc: 87.356520\tValidation Loss: 117.401172, time5.2\n",
      "Epoch: 554 \tTraining Loss: 63.156183 \tacc: 89.032369\tValidation Loss: 117.756079, time5.3\n",
      "Epoch: 555 \tTraining Loss: 63.293924 \tacc: 87.850092\tValidation Loss: 117.385384, time5.2\n",
      "Epoch: 556 \tTraining Loss: 62.352694 \tacc: 87.764004\tValidation Loss: 117.626217, time5.3\n",
      "Epoch: 557 \tTraining Loss: 62.938093 \tacc: 87.677916\tValidation Loss: 118.461707, time5.2\n",
      "Epoch: 558 \tTraining Loss: 62.603306 \tacc: 87.293388\tValidation Loss: 117.196809, time5.3\n",
      "Epoch: 559 \tTraining Loss: 63.016529 \tacc: 88.435491\tValidation Loss: 118.554977, time5.3\n",
      "Epoch: 560 \tTraining Loss: 63.049051 \tacc: 87.735308\tValidation Loss: 117.760478, time5.3\n",
      "Epoch: 561 \tTraining Loss: 62.463652 \tacc: 88.389578\tValidation Loss: 118.756538, time5.2\n",
      "Epoch: 562 \tTraining Loss: 62.819483 \tacc: 87.838613\tValidation Loss: 120.061655, time5.2\n",
      "Epoch: 563 \tTraining Loss: 62.708525 \tacc: 87.471304\tValidation Loss: 120.048659, time5.3\n",
      "Epoch: 564 \tTraining Loss: 62.172865 \tacc: 88.556015\tValidation Loss: 120.628341, time5.2\n",
      "Epoch: 565 \tTraining Loss: 62.511478 \tacc: 87.178604\tValidation Loss: 120.060049, time5.3\n",
      "Epoch: 566 \tTraining Loss: 62.377564 \tacc: 87.855831\tValidation Loss: 118.215524, time5.2\n",
      "Epoch: 567 \tTraining Loss: 62.706612 \tacc: 87.322084\tValidation Loss: 120.045384, time5.3\n",
      "Epoch: 568 \tTraining Loss: 63.054790 \tacc: 88.286272\tValidation Loss: 117.846176, time5.2\n",
      "Epoch: 569 \tTraining Loss: 63.056703 \tacc: 87.861570\tValidation Loss: 120.650348, time5.3\n",
      "Epoch: 570 \tTraining Loss: 62.354607 \tacc: 87.654959\tValidation Loss: 119.224634, time5.2\n",
      "Epoch: 571 \tTraining Loss: 63.144705 \tacc: 87.758264\tValidation Loss: 119.927074, time5.3\n",
      "Epoch: 572 \tTraining Loss: 62.322084 \tacc: 88.096878\tValidation Loss: 119.266476, time5.2\n",
      "Epoch: 573 \tTraining Loss: 62.352694 \tacc: 88.561754\tValidation Loss: 120.731194, time5.3\n",
      "Epoch: 574 \tTraining Loss: 62.343128 \tacc: 87.632002\tValidation Loss: 119.555892, time5.2\n",
      "Epoch: 575 \tTraining Loss: 61.947123 \tacc: 88.877410\tValidation Loss: 118.874465, time5.2\n",
      "Epoch: 576 \tTraining Loss: 62.262779 \tacc: 87.413912\tValidation Loss: 120.129579, time5.3\n",
      "Epoch: 577 \tTraining Loss: 62.599480 \tacc: 87.609045\tValidation Loss: 117.477234, time5.2\n",
      "Epoch: 578 \tTraining Loss: 63.133226 \tacc: 87.431129\tValidation Loss: 118.583427, time5.3\n",
      "Epoch: 579 \tTraining Loss: 62.306780 \tacc: 87.809917\tValidation Loss: 119.300691, time5.2\n",
      "Epoch: 580 \tTraining Loss: 62.339302 \tacc: 88.492883\tValidation Loss: 120.104257, time5.3\n",
      "Epoch: 581 \tTraining Loss: 62.932354 \tacc: 88.424013\tValidation Loss: 119.230210, time5.2\n",
      "Epoch: 582 \tTraining Loss: 62.243649 \tacc: 87.522957\tValidation Loss: 118.225098, time5.3\n",
      "Epoch: 583 \tTraining Loss: 62.859657 \tacc: 88.050964\tValidation Loss: 119.578639, time5.2\n",
      "Epoch: 584 \tTraining Loss: 62.373737 \tacc: 87.896006\tValidation Loss: 119.211331, time5.3\n",
      "Epoch: 585 \tTraining Loss: 61.853382 \tacc: 87.913223\tValidation Loss: 119.832540, time5.2\n",
      "Epoch: 586 \tTraining Loss: 62.846266 \tacc: 87.402433\tValidation Loss: 119.993981, time5.3\n",
      "Epoch: 587 \tTraining Loss: 61.417202 \tacc: 87.253214\tValidation Loss: 124.329724, time5.2\n",
      "Epoch: 588 \tTraining Loss: 62.182430 \tacc: 88.487144\tValidation Loss: 122.134336, time5.2\n",
      "Epoch: 589 \tTraining Loss: 62.369911 \tacc: 87.706612\tValidation Loss: 121.508508, time5.3\n",
      "Epoch: 590 \tTraining Loss: 61.996863 \tacc: 87.190083\tValidation Loss: 121.615464, time5.2\n",
      "Epoch: 591 \tTraining Loss: 61.606596 \tacc: 87.896006\tValidation Loss: 121.618689, time5.3\n",
      "Epoch: 592 \tTraining Loss: 61.348332 \tacc: 87.649219\tValidation Loss: 123.299307, time5.2\n",
      "Epoch: 593 \tTraining Loss: 62.538261 \tacc: 87.798439\tValidation Loss: 123.051420, time5.3\n",
      "Epoch: 594 \tTraining Loss: 62.622436 \tacc: 87.511478\tValidation Loss: 120.969958, time5.2\n",
      "Epoch: 595 \tTraining Loss: 61.782599 \tacc: 87.459826\tValidation Loss: 122.699834, time5.3\n",
      "Epoch: 596 \tTraining Loss: 61.950949 \tacc: 87.683655\tValidation Loss: 122.873290, time5.2\n",
      "Epoch: 597 \tTraining Loss: 62.205387 \tacc: 87.235996\tValidation Loss: 122.897560, time5.3\n",
      "Epoch: 598 \tTraining Loss: 61.805556 \tacc: 87.672176\tValidation Loss: 123.684997, time5.2\n",
      "Epoch: 599 \tTraining Loss: 62.017906 \tacc: 87.775482\tValidation Loss: 121.860660, time5.3\n",
      "Epoch: 600 \tTraining Loss: 61.872513 \tacc: 87.844353\tValidation Loss: 121.087571, time5.2\n",
      "Epoch: 601 \tTraining Loss: 62.593740 \tacc: 87.310606\tValidation Loss: 122.481452, time5.2\n",
      "Epoch: 602 \tTraining Loss: 61.874426 \tacc: 87.431129\tValidation Loss: 121.194870, time5.3\n",
      "Epoch: 603 \tTraining Loss: 61.795990 \tacc: 87.586088\tValidation Loss: 123.020285, time5.2\n",
      "Epoch: 604 \tTraining Loss: 61.669728 \tacc: 88.292011\tValidation Loss: 123.071489, time5.3\n",
      "Epoch: 605 \tTraining Loss: 61.547291 \tacc: 87.609045\tValidation Loss: 123.520649, time5.2\n",
      "Epoch: 606 \tTraining Loss: 61.294766 \tacc: 87.075298\tValidation Loss: 125.002675, time5.3\n",
      "Validation loss decreased (87.167126 --> 87.075298).  Saving model ...\n",
      "Epoch: 607 \tTraining Loss: 62.079125 \tacc: 87.534435\tValidation Loss: 123.579465, time5.3\n",
      "Epoch: 608 \tTraining Loss: 61.489899 \tacc: 88.205923\tValidation Loss: 122.259888, time5.3\n",
      "Epoch: 609 \tTraining Loss: 62.216866 \tacc: 87.046602\tValidation Loss: 123.845026, time5.3\n",
      "Validation loss decreased (87.075298 --> 87.046602).  Saving model ...\n",
      "Epoch: 610 \tTraining Loss: 61.302418 \tacc: 88.314968\tValidation Loss: 122.634185, time5.2\n",
      "Epoch: 611 \tTraining Loss: 61.487986 \tacc: 87.442608\tValidation Loss: 123.627828, time5.3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 612 \tTraining Loss: 61.501377 \tacc: 87.821396\tValidation Loss: 123.226685, time5.2\n",
      "Epoch: 613 \tTraining Loss: 61.746250 \tacc: 87.241736\tValidation Loss: 122.219825, time5.2\n",
      "Epoch: 614 \tTraining Loss: 61.897383 \tacc: 89.428375\tValidation Loss: 122.682023, time5.3\n",
      "Epoch: 615 \tTraining Loss: 62.214953 \tacc: 86.977732\tValidation Loss: 125.275895, time5.2\n",
      "Validation loss decreased (87.046602 --> 86.977732).  Saving model ...\n",
      "Epoch: 616 \tTraining Loss: 61.248852 \tacc: 87.333563\tValidation Loss: 125.704118, time5.3\n",
      "Epoch: 617 \tTraining Loss: 61.455464 \tacc: 87.786961\tValidation Loss: 124.881565, time5.3\n",
      "Epoch: 618 \tTraining Loss: 61.388506 \tacc: 88.234619\tValidation Loss: 123.527932, time5.3\n",
      "Epoch: 619 \tTraining Loss: 61.419115 \tacc: 87.786961\tValidation Loss: 124.243107, time5.3\n",
      "Epoch: 620 \tTraining Loss: 61.185721 \tacc: 87.855831\tValidation Loss: 127.176146, time5.3\n",
      "Epoch: 621 \tTraining Loss: 61.589379 \tacc: 87.333563\tValidation Loss: 124.836811, time5.3\n",
      "Epoch: 622 \tTraining Loss: 61.629553 \tacc: 88.154270\tValidation Loss: 123.379300, time5.3\n",
      "Epoch: 623 \tTraining Loss: 61.160851 \tacc: 89.181589\tValidation Loss: 123.579900, time5.3\n",
      "Epoch: 624 \tTraining Loss: 62.962963 \tacc: 88.292011\tValidation Loss: 123.469315, time5.2\n",
      "Epoch: 625 \tTraining Loss: 61.530073 \tacc: 86.971993\tValidation Loss: 126.405122, time5.3\n",
      "Validation loss decreased (86.977732 --> 86.971993).  Saving model ...\n",
      "Epoch: 626 \tTraining Loss: 60.896847 \tacc: 88.257576\tValidation Loss: 125.953770, time5.3\n",
      "Epoch: 627 \tTraining Loss: 60.730410 \tacc: 87.488522\tValidation Loss: 126.710855, time5.2\n",
      "Epoch: 628 \tTraining Loss: 61.554943 \tacc: 87.454086\tValidation Loss: 127.281999, time5.3\n",
      "Epoch: 629 \tTraining Loss: 61.399985 \tacc: 87.545914\tValidation Loss: 123.884892, time5.2\n",
      "Epoch: 630 \tTraining Loss: 61.002066 \tacc: 88.619146\tValidation Loss: 124.924820, time5.3\n",
      "Epoch: 631 \tTraining Loss: 61.583639 \tacc: 88.171488\tValidation Loss: 125.122469, time5.2\n",
      "Epoch: 632 \tTraining Loss: 61.378941 \tacc: 87.103994\tValidation Loss: 125.081567, time5.3\n",
      "Epoch: 633 \tTraining Loss: 61.160851 \tacc: 87.155647\tValidation Loss: 126.325226, time5.2\n",
      "Epoch: 634 \tTraining Loss: 60.380318 \tacc: 87.333563\tValidation Loss: 127.381210, time5.3\n",
      "Epoch: 635 \tTraining Loss: 61.065197 \tacc: 87.299128\tValidation Loss: 125.834413, time5.2\n",
      "Epoch: 636 \tTraining Loss: 61.964340 \tacc: 87.264692\tValidation Loss: 126.753120, time5.3\n",
      "Epoch: 637 \tTraining Loss: 60.812672 \tacc: 87.947658\tValidation Loss: 126.234391, time5.2\n",
      "Epoch: 638 \tTraining Loss: 61.135981 \tacc: 88.050964\tValidation Loss: 127.265466, time5.2\n",
      "Epoch: 639 \tTraining Loss: 61.158938 \tacc: 87.953398\tValidation Loss: 126.203204, time5.3\n",
      "Epoch: 640 \tTraining Loss: 61.836165 \tacc: 87.677916\tValidation Loss: 125.245984, time5.2\n",
      "Epoch: 641 \tTraining Loss: 61.880165 \tacc: 87.218779\tValidation Loss: 125.188016, time5.3\n",
      "Epoch: 642 \tTraining Loss: 60.954239 \tacc: 88.389578\tValidation Loss: 124.463267, time5.2\n",
      "Epoch: 643 \tTraining Loss: 61.419115 \tacc: 87.941919\tValidation Loss: 124.383105, time5.3\n",
      "Epoch: 644 \tTraining Loss: 61.007805 \tacc: 87.000689\tValidation Loss: 126.302258, time5.2\n",
      "Epoch: 645 \tTraining Loss: 61.298592 \tacc: 87.695133\tValidation Loss: 123.920045, time5.3\n",
      "Epoch: 646 \tTraining Loss: 61.038414 \tacc: 88.556015\tValidation Loss: 126.686668, time5.2\n",
      "Epoch: 647 \tTraining Loss: 60.990588 \tacc: 87.528696\tValidation Loss: 126.995648, time5.3\n",
      "Epoch: 648 \tTraining Loss: 61.061371 \tacc: 87.195822\tValidation Loss: 126.222701, time5.2\n",
      "Epoch: 649 \tTraining Loss: 60.301882 \tacc: 88.498623\tValidation Loss: 129.055243, time5.3\n",
      "Epoch: 650 \tTraining Loss: 60.981022 \tacc: 88.016529\tValidation Loss: 125.863033, time5.2\n",
      "Epoch: 651 \tTraining Loss: 61.627640 \tacc: 88.056703\tValidation Loss: 124.596784, time5.2\n",
      "Epoch: 652 \tTraining Loss: 61.212504 \tacc: 86.966253\tValidation Loss: 128.915642, time5.3\n",
      "Validation loss decreased (86.971993 --> 86.966253).  Saving model ...\n",
      "Epoch: 653 \tTraining Loss: 61.275635 \tacc: 87.505739\tValidation Loss: 124.844501, time5.3\n",
      "Epoch: 654 \tTraining Loss: 61.292853 \tacc: 87.723829\tValidation Loss: 126.371441, time5.3\n",
      "Epoch: 655 \tTraining Loss: 61.197199 \tacc: 88.119835\tValidation Loss: 129.554372, time5.3\n",
      "Epoch: 656 \tTraining Loss: 60.596495 \tacc: 86.908861\tValidation Loss: 128.764844, time5.2\n",
      "Validation loss decreased (86.966253 --> 86.908861).  Saving model ...\n",
      "Epoch: 657 \tTraining Loss: 60.533364 \tacc: 87.718090\tValidation Loss: 128.468537, time5.2\n",
      "Epoch: 658 \tTraining Loss: 60.619452 \tacc: 88.022268\tValidation Loss: 127.377519, time5.2\n",
      "Epoch: 659 \tTraining Loss: 60.785889 \tacc: 87.626263\tValidation Loss: 130.811631, time5.3\n",
      "Epoch: 660 \tTraining Loss: 60.730410 \tacc: 87.821396\tValidation Loss: 128.760085, time5.2\n",
      "Epoch: 661 \tTraining Loss: 61.384680 \tacc: 89.118457\tValidation Loss: 122.342627, time5.3\n",
      "Epoch: 662 \tTraining Loss: 62.125038 \tacc: 87.758264\tValidation Loss: 123.712865, time5.2\n",
      "Epoch: 663 \tTraining Loss: 61.608509 \tacc: 87.023646\tValidation Loss: 128.070187, time5.2\n",
      "Epoch: 664 \tTraining Loss: 60.868151 \tacc: 87.167126\tValidation Loss: 129.014960, time5.2\n",
      "Epoch: 665 \tTraining Loss: 61.493725 \tacc: 87.040863\tValidation Loss: 126.418519, time5.3\n",
      "Epoch: 666 \tTraining Loss: 61.193373 \tacc: 87.155647\tValidation Loss: 128.550640, time5.2\n",
      "Epoch: 667 \tTraining Loss: 60.552495 \tacc: 87.115473\tValidation Loss: 128.776874, time5.3\n",
      "Epoch: 668 \tTraining Loss: 60.223447 \tacc: 87.138430\tValidation Loss: 130.521946, time5.2\n",
      "Epoch: 669 \tTraining Loss: 60.565886 \tacc: 87.241736\tValidation Loss: 128.513219, time5.3\n",
      "Epoch: 670 \tTraining Loss: 60.871977 \tacc: 86.759642\tValidation Loss: 127.712931, time5.2\n",
      "Validation loss decreased (86.908861 --> 86.759642).  Saving model ...\n",
      "Epoch: 671 \tTraining Loss: 61.248852 \tacc: 87.304867\tValidation Loss: 129.451955, time5.3\n",
      "Epoch: 672 \tTraining Loss: 60.376492 \tacc: 87.494261\tValidation Loss: 129.026398, time5.3\n",
      "Epoch: 673 \tTraining Loss: 60.671105 \tacc: 88.182966\tValidation Loss: 127.073839, time5.3\n",
      "Epoch: 674 \tTraining Loss: 60.611800 \tacc: 87.402433\tValidation Loss: 131.060342, time5.3\n",
      "Epoch: 675 \tTraining Loss: 60.233012 \tacc: 87.385216\tValidation Loss: 130.117019, time5.3\n",
      "Epoch: 676 \tTraining Loss: 60.198577 \tacc: 87.677916\tValidation Loss: 129.343454, time5.2\n",
      "Epoch: 677 \tTraining Loss: 60.451102 \tacc: 87.190083\tValidation Loss: 128.180532, time5.3\n",
      "Epoch: 678 \tTraining Loss: 60.548669 \tacc: 86.834252\tValidation Loss: 131.241052, time5.3\n",
      "Epoch: 679 \tTraining Loss: 61.321549 \tacc: 87.620523\tValidation Loss: 127.536050, time5.3\n",
      "Epoch: 680 \tTraining Loss: 59.683961 \tacc: 86.960514\tValidation Loss: 130.743169, time5.3\n",
      "Epoch: 681 \tTraining Loss: 60.376492 \tacc: 86.776860\tValidation Loss: 130.357023, time5.3\n",
      "Epoch: 682 \tTraining Loss: 60.518059 \tacc: 86.914601\tValidation Loss: 129.635855, time5.3\n",
      "Epoch: 683 \tTraining Loss: 60.789715 \tacc: 86.874426\tValidation Loss: 128.790589, time5.3\n",
      "Epoch: 684 \tTraining Loss: 60.309535 \tacc: 87.012167\tValidation Loss: 130.625445, time5.3\n",
      "Epoch: 685 \tTraining Loss: 60.345883 \tacc: 87.167126\tValidation Loss: 131.142683, time5.3\n",
      "Epoch: 686 \tTraining Loss: 60.060836 \tacc: 86.794077\tValidation Loss: 130.054939, time5.3\n",
      "Epoch: 687 \tTraining Loss: 60.988675 \tacc: 87.213039\tValidation Loss: 128.305127, time5.2\n",
      "Epoch: 688 \tTraining Loss: 60.854760 \tacc: 88.223140\tValidation Loss: 128.061217, time5.3\n",
      "Epoch: 689 \tTraining Loss: 61.487986 \tacc: 88.504362\tValidation Loss: 126.473567, time5.3\n",
      "Epoch: 690 \tTraining Loss: 60.286578 \tacc: 86.834252\tValidation Loss: 131.653444, time5.3\n",
      "Epoch: 691 \tTraining Loss: 59.783440 \tacc: 86.794077\tValidation Loss: 130.659383, time5.3\n",
      "Epoch: 692 \tTraining Loss: 60.179446 \tacc: 87.184343\tValidation Loss: 132.817967, time5.3\n",
      "Epoch: 693 \tTraining Loss: 60.053183 \tacc: 87.362259\tValidation Loss: 129.377429, time5.3\n",
      "Epoch: 694 \tTraining Loss: 61.116850 \tacc: 88.274793\tValidation Loss: 131.140329, time5.3\n",
      "Epoch: 695 \tTraining Loss: 60.173707 \tacc: 87.000689\tValidation Loss: 130.562994, time5.3\n",
      "Epoch: 696 \tTraining Loss: 59.724135 \tacc: 87.109734\tValidation Loss: 130.499879, time5.3\n",
      "Epoch: 697 \tTraining Loss: 59.791093 \tacc: 87.023646\tValidation Loss: 130.455349, time5.3\n",
      "Epoch: 698 \tTraining Loss: 60.055096 \tacc: 86.937557\tValidation Loss: 133.073015, time5.2\n",
      "Epoch: 699 \tTraining Loss: 60.973370 \tacc: 87.442608\tValidation Loss: 127.978705, time5.3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 700 \tTraining Loss: 60.613713 \tacc: 87.517218\tValidation Loss: 129.797356, time5.3\n",
      "Epoch: 701 \tTraining Loss: 60.321013 \tacc: 87.918962\tValidation Loss: 132.555692, time5.3\n",
      "Epoch: 702 \tTraining Loss: 60.275099 \tacc: 87.500000\tValidation Loss: 129.685764, time5.3\n",
      "Epoch: 703 \tTraining Loss: 59.861876 \tacc: 87.103994\tValidation Loss: 133.406745, time5.3\n",
      "Epoch: 704 \tTraining Loss: 59.509871 \tacc: 86.908861\tValidation Loss: 132.645691, time5.3\n",
      "Epoch: 705 \tTraining Loss: 60.014922 \tacc: 87.218779\tValidation Loss: 130.062271, time5.3\n",
      "Epoch: 706 \tTraining Loss: 60.020661 \tacc: 88.177227\tValidation Loss: 131.096500, time5.3\n",
      "Epoch: 707 \tTraining Loss: 60.053183 \tacc: 87.649219\tValidation Loss: 131.433604, time5.3\n",
      "Epoch: 708 \tTraining Loss: 60.053183 \tacc: 86.983471\tValidation Loss: 129.967165, time5.3\n",
      "Epoch: 709 \tTraining Loss: 60.079966 \tacc: 87.373737\tValidation Loss: 131.393944, time5.2\n",
      "Epoch: 710 \tTraining Loss: 59.609351 \tacc: 87.207300\tValidation Loss: 131.998995, time5.3\n",
      "Epoch: 711 \tTraining Loss: 59.898225 \tacc: 88.050964\tValidation Loss: 129.708219, time5.3\n",
      "Epoch: 712 \tTraining Loss: 60.079966 \tacc: 87.913223\tValidation Loss: 132.529999, time5.3\n",
      "Epoch: 713 \tTraining Loss: 60.701714 \tacc: 88.624885\tValidation Loss: 132.054728, time5.3\n",
      "Epoch: 714 \tTraining Loss: 59.733701 \tacc: 87.568871\tValidation Loss: 131.313572, time5.3\n",
      "Epoch: 715 \tTraining Loss: 60.441537 \tacc: 87.821396\tValidation Loss: 133.954717, time5.3\n",
      "Epoch: 716 \tTraining Loss: 59.781527 \tacc: 86.897383\tValidation Loss: 130.898562, time5.3\n",
      "Epoch: 717 \tTraining Loss: 59.588307 \tacc: 89.313590\tValidation Loss: 131.568107, time5.3\n",
      "Epoch: 718 \tTraining Loss: 60.475972 \tacc: 86.484160\tValidation Loss: 130.160427, time5.3\n",
      "Validation loss decreased (86.759642 --> 86.484160).  Saving model ...\n",
      "Epoch: 719 \tTraining Loss: 59.647612 \tacc: 87.408173\tValidation Loss: 132.570035, time5.2\n",
      "Epoch: 720 \tTraining Loss: 60.064662 \tacc: 87.184343\tValidation Loss: 134.281850, time5.3\n",
      "Epoch: 721 \tTraining Loss: 59.856137 \tacc: 88.022268\tValidation Loss: 131.232337, time5.2\n",
      "Epoch: 722 \tTraining Loss: 60.175620 \tacc: 86.943297\tValidation Loss: 133.740745, time5.2\n",
      "Epoch: 723 \tTraining Loss: 59.779614 \tacc: 87.557392\tValidation Loss: 131.700805, time5.3\n",
      "Epoch: 724 \tTraining Loss: 59.712657 \tacc: 87.976354\tValidation Loss: 132.051067, time5.2\n",
      "Epoch: 725 \tTraining Loss: 59.668656 \tacc: 87.155647\tValidation Loss: 133.194356, time5.3\n",
      "Epoch: 726 \tTraining Loss: 59.163606 \tacc: 89.279155\tValidation Loss: 134.743246, time5.2\n",
      "Epoch: 727 \tTraining Loss: 60.625191 \tacc: 88.762626\tValidation Loss: 133.403906, time5.3\n",
      "Epoch: 728 \tTraining Loss: 60.443450 \tacc: 86.966253\tValidation Loss: 131.776060, time5.2\n",
      "Epoch: 729 \tTraining Loss: 59.783440 \tacc: 87.144169\tValidation Loss: 133.185562, time5.3\n",
      "Epoch: 730 \tTraining Loss: 59.387435 \tacc: 87.098255\tValidation Loss: 132.332600, time5.2\n",
      "Epoch: 731 \tTraining Loss: 59.421870 \tacc: 87.029385\tValidation Loss: 135.799085, time5.3\n",
      "Epoch: 732 \tTraining Loss: 59.718396 \tacc: 86.725207\tValidation Loss: 133.500338, time5.2\n",
      "Epoch: 733 \tTraining Loss: 58.991429 \tacc: 86.834252\tValidation Loss: 135.498857, time5.3\n",
      "Epoch: 734 \tTraining Loss: 59.414218 \tacc: 87.517218\tValidation Loss: 135.217267, time5.2\n",
      "Epoch: 735 \tTraining Loss: 59.615090 \tacc: 86.880165\tValidation Loss: 135.771394, time5.2\n",
      "Epoch: 736 \tTraining Loss: 59.345347 \tacc: 88.228880\tValidation Loss: 133.482078, time5.3\n",
      "Epoch: 737 \tTraining Loss: 60.648148 \tacc: 87.379477\tValidation Loss: 131.314846, time5.2\n",
      "Epoch: 738 \tTraining Loss: 60.129706 \tacc: 87.511478\tValidation Loss: 132.181104, time5.3\n",
      "Epoch: 739 \tTraining Loss: 59.724135 \tacc: 86.742424\tValidation Loss: 131.461616, time5.2\n",
      "Epoch: 740 \tTraining Loss: 59.481175 \tacc: 88.527319\tValidation Loss: 129.526798, time5.3\n",
      "Epoch: 741 \tTraining Loss: 60.330579 \tacc: 86.466942\tValidation Loss: 134.169209, time5.2\n",
      "Validation loss decreased (86.484160 --> 86.466942).  Saving model ...\n",
      "Epoch: 742 \tTraining Loss: 59.527089 \tacc: 87.161387\tValidation Loss: 135.438539, time5.3\n",
      "Epoch: 743 \tTraining Loss: 59.444827 \tacc: 86.920340\tValidation Loss: 136.557275, time5.3\n",
      "Epoch: 744 \tTraining Loss: 59.050735 \tacc: 87.339302\tValidation Loss: 136.753537, time5.3\n",
      "Epoch: 745 \tTraining Loss: 59.651439 \tacc: 87.132691\tValidation Loss: 134.595282, time5.3\n",
      "Epoch: 746 \tTraining Loss: 59.230563 \tacc: 87.666437\tValidation Loss: 132.403061, time5.3\n",
      "Epoch: 747 \tTraining Loss: 59.027778 \tacc: 86.707989\tValidation Loss: 135.502567, time5.3\n",
      "Epoch: 748 \tTraining Loss: 58.675773 \tacc: 88.601928\tValidation Loss: 137.285093, time5.3\n",
      "Epoch: 749 \tTraining Loss: 60.682583 \tacc: 87.086777\tValidation Loss: 133.956163, time5.2\n",
      "Epoch: 750 \tTraining Loss: 59.917355 \tacc: 87.190083\tValidation Loss: 134.538475, time5.3\n",
      "Epoch: 751 \tTraining Loss: 60.935109 \tacc: 87.637741\tValidation Loss: 134.295388, time5.3\n",
      "Epoch: 752 \tTraining Loss: 60.257882 \tacc: 88.745409\tValidation Loss: 134.524400, time5.3\n",
      "Epoch: 753 \tTraining Loss: 60.709366 \tacc: 87.413912\tValidation Loss: 134.169343, time5.3\n",
      "Epoch: 754 \tTraining Loss: 59.647612 \tacc: 87.574610\tValidation Loss: 133.822436, time5.3\n",
      "Epoch: 755 \tTraining Loss: 59.291781 \tacc: 87.230257\tValidation Loss: 134.204432, time5.3\n",
      "Epoch: 756 \tTraining Loss: 59.509871 \tacc: 86.908861\tValidation Loss: 135.499063, time5.3\n",
      "Epoch: 757 \tTraining Loss: 59.835093 \tacc: 88.940542\tValidation Loss: 136.654335, time5.3\n",
      "Epoch: 758 \tTraining Loss: 60.070401 \tacc: 88.406795\tValidation Loss: 133.582740, time5.3\n",
      "Epoch: 759 \tTraining Loss: 59.764310 \tacc: 87.241736\tValidation Loss: 134.228531, time5.3\n",
      "Epoch: 760 \tTraining Loss: 59.278390 \tacc: 87.333563\tValidation Loss: 137.005468, time5.2\n",
      "Epoch: 761 \tTraining Loss: 59.278390 \tacc: 87.167126\tValidation Loss: 135.327157, time5.3\n",
      "Epoch: 762 \tTraining Loss: 59.278390 \tacc: 87.620523\tValidation Loss: 136.064599, time5.3\n",
      "Epoch: 763 \tTraining Loss: 59.701178 \tacc: 87.109734\tValidation Loss: 137.190543, time5.3\n",
      "Epoch: 764 \tTraining Loss: 59.714570 \tacc: 87.769743\tValidation Loss: 134.479799, time5.3\n",
      "Epoch: 765 \tTraining Loss: 59.959443 \tacc: 89.634986\tValidation Loss: 135.092022, time5.3\n",
      "Epoch: 766 \tTraining Loss: 59.940312 \tacc: 86.530073\tValidation Loss: 135.125817, time5.3\n",
      "Epoch: 767 \tTraining Loss: 59.551959 \tacc: 88.045225\tValidation Loss: 135.990340, time5.3\n",
      "Epoch: 768 \tTraining Loss: 59.603612 \tacc: 87.511478\tValidation Loss: 135.971401, time5.3\n",
      "Epoch: 769 \tTraining Loss: 58.823079 \tacc: 87.413912\tValidation Loss: 138.897196, time5.3\n",
      "Epoch: 770 \tTraining Loss: 59.993878 \tacc: 86.891644\tValidation Loss: 136.491606, time5.3\n",
      "Epoch: 771 \tTraining Loss: 59.381696 \tacc: 86.857208\tValidation Loss: 137.026849, time5.2\n",
      "Epoch: 772 \tTraining Loss: 58.916820 \tacc: 86.644858\tValidation Loss: 137.532781, time5.3\n",
      "Epoch: 773 \tTraining Loss: 59.697352 \tacc: 88.148531\tValidation Loss: 136.056708, time5.3\n",
      "Epoch: 774 \tTraining Loss: 59.441001 \tacc: 86.730946\tValidation Loss: 139.814362, time5.3\n",
      "Epoch: 775 \tTraining Loss: 59.419957 \tacc: 87.643480\tValidation Loss: 135.864500, time5.3\n",
      "Epoch: 776 \tTraining Loss: 58.972299 \tacc: 87.626263\tValidation Loss: 137.319642, time5.3\n",
      "Epoch: 777 \tTraining Loss: 59.276477 \tacc: 87.069559\tValidation Loss: 136.626671, time5.3\n",
      "Epoch: 778 \tTraining Loss: 59.213346 \tacc: 87.075298\tValidation Loss: 138.843367, time5.3\n",
      "Epoch: 779 \tTraining Loss: 59.353000 \tacc: 86.868687\tValidation Loss: 138.186960, time5.3\n",
      "Epoch: 780 \tTraining Loss: 58.920646 \tacc: 86.667815\tValidation Loss: 139.918424, time5.3\n",
      "Epoch: 781 \tTraining Loss: 59.075605 \tacc: 86.598944\tValidation Loss: 137.189257, time5.3\n",
      "Epoch: 782 \tTraining Loss: 59.366391 \tacc: 86.914601\tValidation Loss: 137.079353, time5.2\n",
      "Epoch: 783 \tTraining Loss: 60.166054 \tacc: 86.656336\tValidation Loss: 134.956549, time5.3\n",
      "Epoch: 784 \tTraining Loss: 59.611264 \tacc: 87.207300\tValidation Loss: 131.844044, time5.3\n",
      "Epoch: 785 \tTraining Loss: 59.869529 \tacc: 86.501377\tValidation Loss: 136.602474, time5.3\n",
      "Epoch: 786 \tTraining Loss: 58.909167 \tacc: 86.977732\tValidation Loss: 139.097519, time5.3\n",
      "Epoch: 787 \tTraining Loss: 58.771426 \tacc: 87.086777\tValidation Loss: 137.315314, time5.3\n",
      "Epoch: 788 \tTraining Loss: 59.000995 \tacc: 88.005051\tValidation Loss: 136.950260, time5.3\n",
      "Epoch: 789 \tTraining Loss: 59.381696 \tacc: 86.449725\tValidation Loss: 139.093735, time5.3\n",
      "Validation loss decreased (86.466942 --> 86.449725).  Saving model ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 790 \tTraining Loss: 58.696817 \tacc: 87.195822\tValidation Loss: 137.718916, time5.2\n",
      "Epoch: 791 \tTraining Loss: 59.286042 \tacc: 87.637741\tValidation Loss: 140.237517, time5.3\n",
      "Epoch: 792 \tTraining Loss: 58.934037 \tacc: 87.488522\tValidation Loss: 137.782896, time5.2\n",
      "Epoch: 793 \tTraining Loss: 59.192302 \tacc: 87.586088\tValidation Loss: 141.944026, time5.3\n",
      "Epoch: 794 \tTraining Loss: 59.944138 \tacc: 86.765381\tValidation Loss: 137.566590, time5.2\n",
      "Epoch: 795 \tTraining Loss: 58.658555 \tacc: 87.161387\tValidation Loss: 138.984440, time5.2\n",
      "Epoch: 796 \tTraining Loss: 58.807775 \tacc: 87.804178\tValidation Loss: 137.890429, time5.2\n",
      "Epoch: 797 \tTraining Loss: 59.620830 \tacc: 87.213039\tValidation Loss: 137.884901, time5.2\n",
      "Epoch: 798 \tTraining Loss: 59.372130 \tacc: 86.817034\tValidation Loss: 139.104019, time5.2\n",
      "Epoch: 799 \tTraining Loss: 58.897689 \tacc: 88.395317\tValidation Loss: 136.125296, time5.2\n",
      "Epoch: 800 \tTraining Loss: 59.712657 \tacc: 87.459826\tValidation Loss: 136.784461, time5.3\n",
      "Epoch: 801 \tTraining Loss: 58.633685 \tacc: 86.817034\tValidation Loss: 139.328067, time5.2\n",
      "Epoch: 802 \tTraining Loss: 59.194215 \tacc: 87.322084\tValidation Loss: 137.402939, time5.2\n",
      "Epoch: 803 \tTraining Loss: 59.731788 \tacc: 86.817034\tValidation Loss: 135.870647, time5.2\n",
      "Epoch: 804 \tTraining Loss: 59.018212 \tacc: 87.379477\tValidation Loss: 138.059887, time5.3\n",
      "Epoch: 805 \tTraining Loss: 58.855601 \tacc: 87.264692\tValidation Loss: 137.522397, time5.2\n",
      "Epoch: 806 \tTraining Loss: 59.639960 \tacc: 86.736685\tValidation Loss: 140.633219, time5.2\n",
      "Epoch: 807 \tTraining Loss: 59.576829 \tacc: 87.614784\tValidation Loss: 138.268569, time5.2\n",
      "Epoch: 808 \tTraining Loss: 59.048822 \tacc: 86.891644\tValidation Loss: 139.883759, time5.3\n",
      "Epoch: 809 \tTraining Loss: 58.620294 \tacc: 87.075298\tValidation Loss: 139.392698, time5.2\n",
      "Epoch: 810 \tTraining Loss: 59.121518 \tacc: 87.408173\tValidation Loss: 139.741175, time5.2\n",
      "Epoch: 811 \tTraining Loss: 59.132997 \tacc: 86.639118\tValidation Loss: 140.655922, time5.2\n",
      "Epoch: 812 \tTraining Loss: 58.509336 \tacc: 86.828512\tValidation Loss: 139.658279, time5.2\n",
      "Epoch: 813 \tTraining Loss: 58.974212 \tacc: 86.478421\tValidation Loss: 142.703284, time5.3\n",
      "Epoch: 814 \tTraining Loss: 58.149679 \tacc: 88.510101\tValidation Loss: 139.352087, time5.2\n",
      "Epoch: 815 \tTraining Loss: 60.332492 \tacc: 86.926079\tValidation Loss: 136.791976, time5.3\n",
      "Epoch: 816 \tTraining Loss: 58.893863 \tacc: 86.817034\tValidation Loss: 138.805121, time5.2\n",
      "Epoch: 817 \tTraining Loss: 58.981864 \tacc: 87.809917\tValidation Loss: 139.331255, time5.3\n",
      "Epoch: 818 \tTraining Loss: 59.203780 \tacc: 87.040863\tValidation Loss: 139.074146, time5.2\n",
      "Epoch: 819 \tTraining Loss: 58.941690 \tacc: 87.603306\tValidation Loss: 138.206547, time5.3\n",
      "Epoch: 820 \tTraining Loss: 58.723600 \tacc: 88.119835\tValidation Loss: 141.636110, time5.2\n",
      "Epoch: 821 \tTraining Loss: 58.576293 \tacc: 87.609045\tValidation Loss: 138.553301, time5.2\n",
      "Epoch: 822 \tTraining Loss: 59.661004 \tacc: 86.673554\tValidation Loss: 137.608188, time5.2\n",
      "Epoch: 823 \tTraining Loss: 58.736991 \tacc: 86.903122\tValidation Loss: 142.019159, time5.2\n",
      "Epoch: 824 \tTraining Loss: 58.979951 \tacc: 87.373737\tValidation Loss: 137.458769, time5.3\n",
      "Epoch: 825 \tTraining Loss: 59.025865 \tacc: 86.977732\tValidation Loss: 137.739203, time5.2\n",
      "Epoch: 826 \tTraining Loss: 58.761861 \tacc: 87.075298\tValidation Loss: 140.765038, time5.3\n",
      "Epoch: 827 \tTraining Loss: 58.574380 \tacc: 86.765381\tValidation Loss: 141.835420, time5.3\n",
      "Epoch: 828 \tTraining Loss: 58.800122 \tacc: 87.075298\tValidation Loss: 138.505559, time5.3\n",
      "Epoch: 829 \tTraining Loss: 58.905341 \tacc: 86.530073\tValidation Loss: 139.511796, time5.3\n",
      "Epoch: 830 \tTraining Loss: 59.521350 \tacc: 87.511478\tValidation Loss: 140.787937, time5.3\n",
      "Epoch: 831 \tTraining Loss: 59.016299 \tacc: 86.753903\tValidation Loss: 142.520168, time5.2\n",
      "Epoch: 832 \tTraining Loss: 58.811601 \tacc: 87.316345\tValidation Loss: 140.103578, time5.3\n",
      "Epoch: 833 \tTraining Loss: 58.300811 \tacc: 87.270432\tValidation Loss: 138.803037, time5.2\n",
      "Epoch: 834 \tTraining Loss: 58.643251 \tacc: 86.897383\tValidation Loss: 141.599416, time5.3\n",
      "Epoch: 835 \tTraining Loss: 58.966560 \tacc: 87.563131\tValidation Loss: 141.106424, time5.2\n",
      "Epoch: 836 \tTraining Loss: 58.450031 \tacc: 92.229109\tValidation Loss: 138.614388, time5.2\n",
      "Epoch: 837 \tTraining Loss: 62.622436 \tacc: 87.046602\tValidation Loss: 137.485628, time5.3\n",
      "Epoch: 838 \tTraining Loss: 59.087083 \tacc: 87.235996\tValidation Loss: 139.086321, time5.2\n",
      "Epoch: 839 \tTraining Loss: 58.285507 \tacc: 86.845730\tValidation Loss: 140.943224, time5.3\n",
      "Epoch: 840 \tTraining Loss: 59.138736 \tacc: 87.144169\tValidation Loss: 138.511991, time5.2\n",
      "Epoch: 841 \tTraining Loss: 59.111953 \tacc: 87.115473\tValidation Loss: 140.090177, time5.3\n",
      "Epoch: 842 \tTraining Loss: 58.631772 \tacc: 87.677916\tValidation Loss: 137.707095, time5.2\n",
      "Epoch: 843 \tTraining Loss: 58.597337 \tacc: 87.413912\tValidation Loss: 139.327696, time5.3\n",
      "Epoch: 844 \tTraining Loss: 58.560989 \tacc: 87.115473\tValidation Loss: 141.350265, time5.2\n",
      "Epoch: 845 \tTraining Loss: 58.182201 \tacc: 86.742424\tValidation Loss: 139.717086, time5.3\n",
      "Epoch: 846 \tTraining Loss: 58.497857 \tacc: 87.345041\tValidation Loss: 142.517275, time5.2\n",
      "Epoch: 847 \tTraining Loss: 58.727426 \tacc: 87.454086\tValidation Loss: 142.383644, time5.3\n",
      "Epoch: 848 \tTraining Loss: 58.725513 \tacc: 87.413912\tValidation Loss: 140.167312, time5.2\n",
      "Epoch: 849 \tTraining Loss: 59.412305 \tacc: 87.253214\tValidation Loss: 140.503136, time5.3\n",
      "Epoch: 850 \tTraining Loss: 59.391261 \tacc: 87.184343\tValidation Loss: 140.556731, time5.2\n",
      "Epoch: 851 \tTraining Loss: 58.761861 \tacc: 86.570248\tValidation Loss: 143.260433, time5.2\n",
      "Epoch: 852 \tTraining Loss: 58.559076 \tacc: 87.677916\tValidation Loss: 137.364752, time5.3\n",
      "Epoch: 853 \tTraining Loss: 59.106214 \tacc: 86.811295\tValidation Loss: 141.678938, time5.2\n",
      "Epoch: 854 \tTraining Loss: 58.777166 \tacc: 86.805556\tValidation Loss: 140.408338, time5.3\n",
      "Epoch: 855 \tTraining Loss: 58.279767 \tacc: 89.273416\tValidation Loss: 137.057971, time5.2\n",
      "Epoch: 856 \tTraining Loss: 59.450566 \tacc: 87.035124\tValidation Loss: 138.571738, time5.3\n",
      "Epoch: 857 \tTraining Loss: 59.345347 \tacc: 87.253214\tValidation Loss: 137.705540, time5.2\n",
      "Epoch: 858 \tTraining Loss: 59.228650 \tacc: 86.685032\tValidation Loss: 137.494298, time5.3\n",
      "Epoch: 859 \tTraining Loss: 58.425161 \tacc: 86.794077\tValidation Loss: 139.914156, time5.2\n",
      "Epoch: 860 \tTraining Loss: 58.400291 \tacc: 86.799816\tValidation Loss: 138.576353, time5.3\n",
      "Epoch: 861 \tTraining Loss: 59.473523 \tacc: 86.874426\tValidation Loss: 141.394321, time5.2\n",
      "Epoch: 862 \tTraining Loss: 58.842210 \tacc: 87.029385\tValidation Loss: 140.507772, time5.3\n",
      "Epoch: 863 \tTraining Loss: 58.327594 \tacc: 86.857208\tValidation Loss: 140.404827, time5.2\n",
      "Epoch: 864 \tTraining Loss: 58.983777 \tacc: 86.880165\tValidation Loss: 139.737176, time5.3\n",
      "Epoch: 865 \tTraining Loss: 58.358203 \tacc: 87.873049\tValidation Loss: 142.050462, time5.3\n",
      "Epoch: 866 \tTraining Loss: 58.685338 \tacc: 87.213039\tValidation Loss: 143.116060, time5.2\n",
      "Epoch: 867 \tTraining Loss: 58.633685 \tacc: 86.748163\tValidation Loss: 144.360654, time5.3\n",
      "Epoch: 868 \tTraining Loss: 57.950719 \tacc: 86.466942\tValidation Loss: 142.728375, time5.2\n",
      "Epoch: 869 \tTraining Loss: 58.013851 \tacc: 86.931818\tValidation Loss: 146.726287, time5.3\n",
      "Epoch: 870 \tTraining Loss: 58.692991 \tacc: 87.844353\tValidation Loss: 142.742281, time5.2\n",
      "Epoch: 871 \tTraining Loss: 58.436639 \tacc: 86.937557\tValidation Loss: 143.614442, time5.3\n",
      "Epoch: 872 \tTraining Loss: 59.768136 \tacc: 87.729568\tValidation Loss: 138.713323, time5.2\n",
      "Epoch: 873 \tTraining Loss: 59.347260 \tacc: 87.930441\tValidation Loss: 140.025155, time5.3\n",
      "Epoch: 874 \tTraining Loss: 58.997169 \tacc: 87.052342\tValidation Loss: 143.021191, time5.2\n",
      "Epoch: 875 \tTraining Loss: 58.067417 \tacc: 87.258953\tValidation Loss: 140.569237, time5.3\n",
      "Epoch: 876 \tTraining Loss: 57.780456 \tacc: 86.891644\tValidation Loss: 141.501005, time5.2\n",
      "Epoch: 877 \tTraining Loss: 58.157331 \tacc: 86.839991\tValidation Loss: 142.822040, time5.2\n",
      "Epoch: 878 \tTraining Loss: 58.484466 \tacc: 86.386593\tValidation Loss: 144.409735, time5.3\n",
      "Validation loss decreased (86.449725 --> 86.386593).  Saving model ...\n",
      "Epoch: 879 \tTraining Loss: 58.189853 \tacc: 86.753903\tValidation Loss: 142.785818, time5.3\n",
      "Epoch: 880 \tTraining Loss: 58.226201 \tacc: 87.012167\tValidation Loss: 143.621759, time5.3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 881 \tTraining Loss: 58.207071 \tacc: 86.587466\tValidation Loss: 140.792081, time5.3\n",
      "Epoch: 882 \tTraining Loss: 58.352464 \tacc: 87.270432\tValidation Loss: 143.450864, time5.3\n",
      "Epoch: 883 \tTraining Loss: 58.834558 \tacc: 86.254591\tValidation Loss: 144.018628, time5.2\n",
      "Validation loss decreased (86.386593 --> 86.254591).  Saving model ...\n",
      "Epoch: 884 \tTraining Loss: 58.216636 \tacc: 87.385216\tValidation Loss: 145.643221, time5.3\n",
      "Epoch: 885 \tTraining Loss: 58.453857 \tacc: 87.081038\tValidation Loss: 145.360416, time5.2\n",
      "Epoch: 886 \tTraining Loss: 58.847949 \tacc: 87.540174\tValidation Loss: 143.052769, time5.3\n",
      "Epoch: 887 \tTraining Loss: 59.031604 \tacc: 86.587466\tValidation Loss: 141.757845, time5.2\n",
      "Epoch: 888 \tTraining Loss: 58.042547 \tacc: 86.323462\tValidation Loss: 142.696729, time5.3\n",
      "Epoch: 889 \tTraining Loss: 57.646541 \tacc: 88.435491\tValidation Loss: 142.919577, time5.2\n",
      "Epoch: 890 \tTraining Loss: 59.266912 \tacc: 86.920340\tValidation Loss: 143.074392, time5.3\n",
      "Epoch: 891 \tTraining Loss: 57.952632 \tacc: 87.161387\tValidation Loss: 141.865531, time5.2\n",
      "Epoch: 892 \tTraining Loss: 58.472987 \tacc: 87.287649\tValidation Loss: 143.142510, time5.3\n",
      "Epoch: 893 \tTraining Loss: 58.067417 \tacc: 86.920340\tValidation Loss: 145.886876, time5.3\n",
      "Epoch: 894 \tTraining Loss: 57.709672 \tacc: 88.188705\tValidation Loss: 144.449483, time5.3\n",
      "Epoch: 895 \tTraining Loss: 58.838384 \tacc: 87.023646\tValidation Loss: 144.070387, time5.2\n",
      "Epoch: 896 \tTraining Loss: 58.505510 \tacc: 87.322084\tValidation Loss: 142.092638, time5.3\n",
      "Epoch: 897 \tTraining Loss: 58.293159 \tacc: 88.251837\tValidation Loss: 143.694095, time5.2\n",
      "Epoch: 898 \tTraining Loss: 58.641338 \tacc: 87.431129\tValidation Loss: 141.583021, time5.2\n",
      "Epoch: 899 \tTraining Loss: 58.486379 \tacc: 86.748163\tValidation Loss: 142.382568, time5.3\n",
      "Epoch: 900 \tTraining Loss: 58.036807 \tacc: 87.781221\tValidation Loss: 142.383287, time5.2\n",
      "Epoch: 901 \tTraining Loss: 58.275941 \tacc: 88.160009\tValidation Loss: 142.951528, time5.3\n",
      "Epoch: 902 \tTraining Loss: 58.926385 \tacc: 87.149908\tValidation Loss: 140.940571, time5.2\n",
      "Epoch: 903 \tTraining Loss: 58.201331 \tacc: 86.874426\tValidation Loss: 144.185267, time5.3\n",
      "Epoch: 904 \tTraining Loss: 58.228114 \tacc: 86.507117\tValidation Loss: 142.343053, time5.2\n",
      "Epoch: 905 \tTraining Loss: 58.055938 \tacc: 87.367998\tValidation Loss: 144.739242, time5.3\n",
      "Epoch: 906 \tTraining Loss: 57.895240 \tacc: 86.960514\tValidation Loss: 142.504584, time5.2\n",
      "Epoch: 907 \tTraining Loss: 58.518901 \tacc: 86.742424\tValidation Loss: 143.661595, time5.3\n",
      "Epoch: 908 \tTraining Loss: 58.184114 \tacc: 87.241736\tValidation Loss: 140.162119, time5.2\n",
      "Epoch: 909 \tTraining Loss: 58.570554 \tacc: 87.035124\tValidation Loss: 145.470233, time5.3\n",
      "Epoch: 910 \tTraining Loss: 57.759412 \tacc: 87.103994\tValidation Loss: 143.369300, time5.2\n",
      "Epoch: 911 \tTraining Loss: 58.620294 \tacc: 86.472681\tValidation Loss: 141.861123, time5.2\n",
      "Epoch: 912 \tTraining Loss: 58.296985 \tacc: 87.017906\tValidation Loss: 142.192657, time5.3\n",
      "Epoch: 913 \tTraining Loss: 57.615932 \tacc: 86.949036\tValidation Loss: 146.282130, time5.2\n",
      "Epoch: 914 \tTraining Loss: 57.746021 \tacc: 87.276171\tValidation Loss: 144.884098, time5.3\n",
      "Epoch: 915 \tTraining Loss: 57.575758 \tacc: 86.541552\tValidation Loss: 146.003955, time5.2\n",
      "Epoch: 916 \tTraining Loss: 57.650367 \tacc: 86.839991\tValidation Loss: 145.041427, time5.3\n",
      "Epoch: 917 \tTraining Loss: 58.157331 \tacc: 86.937557\tValidation Loss: 143.920390, time5.2\n",
      "Epoch: 918 \tTraining Loss: 58.698730 \tacc: 86.989210\tValidation Loss: 147.084092, time5.3\n",
      "Epoch: 919 \tTraining Loss: 58.054025 \tacc: 87.040863\tValidation Loss: 142.069106, time5.2\n",
      "Epoch: 920 \tTraining Loss: 57.562366 \tacc: 87.494261\tValidation Loss: 144.410530, time5.3\n",
      "Epoch: 921 \tTraining Loss: 58.658555 \tacc: 88.068182\tValidation Loss: 142.598774, time5.2\n",
      "Epoch: 922 \tTraining Loss: 58.381160 \tacc: 87.281910\tValidation Loss: 142.131746, time5.3\n",
      "Epoch: 923 \tTraining Loss: 58.754209 \tacc: 86.960514\tValidation Loss: 144.048540, time5.2\n",
      "Epoch: 924 \tTraining Loss: 58.675773 \tacc: 88.383838\tValidation Loss: 141.096616, time5.3\n",
      "Epoch: 925 \tTraining Loss: 58.191766 \tacc: 87.402433\tValidation Loss: 142.226799, time5.3\n",
      "Epoch: 926 \tTraining Loss: 58.381160 \tacc: 86.719467\tValidation Loss: 144.846743, time5.2\n",
      "Epoch: 927 \tTraining Loss: 57.814891 \tacc: 86.575987\tValidation Loss: 145.679384, time5.3\n",
      "Epoch: 928 \tTraining Loss: 57.849327 \tacc: 86.891644\tValidation Loss: 145.572842, time5.2\n",
      "Epoch: 929 \tTraining Loss: 58.277854 \tacc: 86.432507\tValidation Loss: 148.161578, time5.3\n",
      "Epoch: 930 \tTraining Loss: 57.350015 \tacc: 86.593205\tValidation Loss: 145.211996, time5.2\n",
      "Epoch: 931 \tTraining Loss: 58.136287 \tacc: 88.211662\tValidation Loss: 144.480497, time5.3\n",
      "Epoch: 932 \tTraining Loss: 57.944980 \tacc: 86.868687\tValidation Loss: 147.662424, time5.2\n",
      "Epoch: 933 \tTraining Loss: 57.732629 \tacc: 86.776860\tValidation Loss: 144.590656, time5.3\n",
      "Epoch: 934 \tTraining Loss: 58.065504 \tacc: 86.524334\tValidation Loss: 149.276102, time5.2\n",
      "Epoch: 935 \tTraining Loss: 58.436639 \tacc: 88.039486\tValidation Loss: 145.890233, time5.3\n",
      "Epoch: 936 \tTraining Loss: 58.402204 \tacc: 86.656336\tValidation Loss: 147.389147, time5.2\n",
      "Epoch: 937 \tTraining Loss: 57.650367 \tacc: 86.989210\tValidation Loss: 145.376161, time5.2\n",
      "Epoch: 938 \tTraining Loss: 57.646541 \tacc: 87.063820\tValidation Loss: 146.434444, time5.3\n",
      "Epoch: 939 \tTraining Loss: 58.916820 \tacc: 87.253214\tValidation Loss: 144.755093, time5.2\n",
      "Epoch: 940 \tTraining Loss: 57.803413 \tacc: 87.580349\tValidation Loss: 147.168692, time5.3\n",
      "Epoch: 941 \tTraining Loss: 57.539409 \tacc: 86.845730\tValidation Loss: 148.494122, time5.2\n",
      "Epoch: 942 \tTraining Loss: 58.025329 \tacc: 88.005051\tValidation Loss: 147.156735, time5.3\n",
      "Epoch: 943 \tTraining Loss: 57.583410 \tacc: 87.448347\tValidation Loss: 147.330014, time5.2\n",
      "Epoch: 944 \tTraining Loss: 58.750383 \tacc: 86.937557\tValidation Loss: 145.627902, time5.3\n",
      "Epoch: 945 \tTraining Loss: 58.599250 \tacc: 87.000689\tValidation Loss: 144.920105, time5.2\n",
      "Epoch: 946 \tTraining Loss: 57.954545 \tacc: 86.185721\tValidation Loss: 148.865943, time5.3\n",
      "Validation loss decreased (86.254591 --> 86.185721).  Saving model ...\n",
      "Epoch: 947 \tTraining Loss: 56.978880 \tacc: 86.730946\tValidation Loss: 147.104793, time5.3\n",
      "Epoch: 948 \tTraining Loss: 57.914371 \tacc: 86.530073\tValidation Loss: 146.701123, time5.2\n",
      "Epoch: 949 \tTraining Loss: 57.849327 \tacc: 86.794077\tValidation Loss: 146.629914, time5.3\n",
      "Epoch: 950 \tTraining Loss: 58.516988 \tacc: 87.683655\tValidation Loss: 144.367566, time5.3\n",
      "Epoch: 951 \tTraining Loss: 58.652816 \tacc: 86.633379\tValidation Loss: 144.159342, time5.3\n",
      "Epoch: 952 \tTraining Loss: 58.677686 \tacc: 87.528696\tValidation Loss: 144.825938, time5.3\n",
      "Epoch: 953 \tTraining Loss: 58.545684 \tacc: 86.713728\tValidation Loss: 141.597622, time5.3\n",
      "Epoch: 954 \tTraining Loss: 58.174549 \tacc: 87.109734\tValidation Loss: 146.885168, time5.3\n",
      "Epoch: 955 \tTraining Loss: 58.124809 \tacc: 87.815657\tValidation Loss: 144.698381, time5.3\n",
      "Epoch: 956 \tTraining Loss: 58.639425 \tacc: 86.943297\tValidation Loss: 144.511839, time5.3\n",
      "Epoch: 957 \tTraining Loss: 57.851240 \tacc: 86.610422\tValidation Loss: 146.806827, time5.3\n",
      "Epoch: 958 \tTraining Loss: 57.503061 \tacc: 86.518595\tValidation Loss: 148.603973, time5.3\n",
      "Epoch: 959 \tTraining Loss: 57.973676 \tacc: 86.587466\tValidation Loss: 146.006697, time5.2\n",
      "Epoch: 960 \tTraining Loss: 57.522192 \tacc: 87.029385\tValidation Loss: 149.339724, time5.3\n",
      "Epoch: 961 \tTraining Loss: 57.271579 \tacc: 86.920340\tValidation Loss: 150.542848, time5.3\n",
      "Epoch: 962 \tTraining Loss: 57.730716 \tacc: 86.845730\tValidation Loss: 150.730959, time5.3\n",
      "Epoch: 963 \tTraining Loss: 58.078895 \tacc: 87.132691\tValidation Loss: 146.918941, time5.3\n",
      "Epoch: 964 \tTraining Loss: 57.281145 \tacc: 87.201561\tValidation Loss: 148.018510, time5.3\n",
      "Epoch: 965 \tTraining Loss: 58.189853 \tacc: 87.769743\tValidation Loss: 144.849422, time5.3\n",
      "Epoch: 966 \tTraining Loss: 57.713499 \tacc: 87.264692\tValidation Loss: 145.276081, time5.3\n",
      "Epoch: 967 \tTraining Loss: 58.166896 \tacc: 86.834252\tValidation Loss: 148.442276, time5.3\n",
      "Epoch: 968 \tTraining Loss: 57.181665 \tacc: 86.817034\tValidation Loss: 149.322317, time5.3\n",
      "Epoch: 969 \tTraining Loss: 57.661846 \tacc: 86.949036\tValidation Loss: 146.427930, time5.2\n",
      "Epoch: 970 \tTraining Loss: 57.283058 \tacc: 86.839991\tValidation Loss: 146.166576, time5.3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 971 \tTraining Loss: 58.467248 \tacc: 87.672176\tValidation Loss: 143.490758, time5.3\n",
      "Epoch: 972 \tTraining Loss: 58.008111 \tacc: 87.436869\tValidation Loss: 146.805652, time5.3\n",
      "Epoch: 973 \tTraining Loss: 58.348638 \tacc: 87.029385\tValidation Loss: 147.133504, time5.3\n",
      "Epoch: 974 \tTraining Loss: 58.027242 \tacc: 87.500000\tValidation Loss: 145.231386, time5.3\n",
      "Epoch: 975 \tTraining Loss: 58.442378 \tacc: 87.017906\tValidation Loss: 146.006739, time5.3\n",
      "Epoch: 976 \tTraining Loss: 57.883762 \tacc: 86.753903\tValidation Loss: 149.272361, time5.3\n",
      "Epoch: 977 \tTraining Loss: 57.638889 \tacc: 86.926079\tValidation Loss: 151.159062, time5.3\n",
      "Epoch: 978 \tTraining Loss: 57.839761 \tacc: 86.644858\tValidation Loss: 149.503201, time5.3\n",
      "Epoch: 979 \tTraining Loss: 57.738369 \tacc: 86.449725\tValidation Loss: 145.156883, time5.3\n",
      "Epoch: 980 \tTraining Loss: 57.174013 \tacc: 86.828512\tValidation Loss: 148.362862, time5.2\n",
      "Epoch: 981 \tTraining Loss: 57.694368 \tacc: 87.551653\tValidation Loss: 147.607537, time5.3\n",
      "Epoch: 982 \tTraining Loss: 57.981328 \tacc: 86.564509\tValidation Loss: 148.015752, time5.3\n",
      "Epoch: 983 \tTraining Loss: 57.680976 \tacc: 87.637741\tValidation Loss: 147.148971, time5.2\n",
      "Epoch: 984 \tTraining Loss: 57.732629 \tacc: 86.994949\tValidation Loss: 147.823388, time5.3\n",
      "Epoch: 985 \tTraining Loss: 57.468626 \tacc: 86.742424\tValidation Loss: 146.905437, time5.3\n",
      "Epoch: 986 \tTraining Loss: 57.288797 \tacc: 86.650597\tValidation Loss: 146.525534, time5.3\n",
      "Epoch: 987 \tTraining Loss: 57.497322 \tacc: 87.126951\tValidation Loss: 146.326780, time5.3\n",
      "Epoch: 988 \tTraining Loss: 57.822544 \tacc: 86.782599\tValidation Loss: 146.380287, time5.3\n",
      "Epoch: 989 \tTraining Loss: 57.063055 \tacc: 86.994949\tValidation Loss: 147.511780, time5.3\n",
      "Epoch: 990 \tTraining Loss: 57.080272 \tacc: 86.455464\tValidation Loss: 149.595639, time5.3\n",
      "Epoch: 991 \tTraining Loss: 56.942531 \tacc: 86.788338\tValidation Loss: 145.522674, time5.2\n",
      "Epoch: 992 \tTraining Loss: 57.713499 \tacc: 87.976354\tValidation Loss: 141.395848, time5.3\n",
      "Epoch: 993 \tTraining Loss: 58.725513 \tacc: 86.690771\tValidation Loss: 146.489411, time5.3\n",
      "Epoch: 994 \tTraining Loss: 58.078895 \tacc: 87.333563\tValidation Loss: 146.759107, time5.3\n",
      "Epoch: 995 \tTraining Loss: 57.784282 \tacc: 87.023646\tValidation Loss: 148.112801, time5.3\n",
      "Epoch: 996 \tTraining Loss: 57.998546 \tacc: 87.804178\tValidation Loss: 146.828873, time5.3\n",
      "Epoch: 997 \tTraining Loss: 57.730716 \tacc: 87.287649\tValidation Loss: 148.453150, time5.3\n",
      "Epoch: 998 \tTraining Loss: 56.984619 \tacc: 86.472681\tValidation Loss: 154.009447, time5.3\n",
      "Epoch: 999 \tTraining Loss: 57.290710 \tacc: 88.263315\tValidation Loss: 146.078165, time5.3\n",
      "Epoch: 1000 \tTraining Loss: 58.159244 \tacc: 89.193067\tValidation Loss: 147.214657, time5.3\n",
      "Epoch: 1001 \tTraining Loss: 58.075069 \tacc: 86.570248\tValidation Loss: 150.907518, time5.3\n",
      "Epoch: 1002 \tTraining Loss: 56.670875 \tacc: 87.121212\tValidation Loss: 151.192856, time5.2\n",
      "Epoch: 1003 \tTraining Loss: 57.874197 \tacc: 86.512856\tValidation Loss: 150.996976, time5.3\n",
      "Epoch: 1004 \tTraining Loss: 57.623584 \tacc: 88.200184\tValidation Loss: 145.703098, time5.3\n",
      "Epoch: 1005 \tTraining Loss: 57.445669 \tacc: 86.805556\tValidation Loss: 150.882018, time5.3\n",
      "Epoch: 1006 \tTraining Loss: 57.395929 \tacc: 87.448347\tValidation Loss: 150.725215, time5.3\n",
      "Epoch: 1007 \tTraining Loss: 57.703933 \tacc: 88.125574\tValidation Loss: 149.931009, time5.3\n",
      "Epoch: 1008 \tTraining Loss: 58.417508 \tacc: 86.667815\tValidation Loss: 151.851967, time5.3\n",
      "Epoch: 1009 \tTraining Loss: 57.198883 \tacc: 87.293388\tValidation Loss: 150.909500, time5.3\n",
      "Epoch: 1010 \tTraining Loss: 57.941154 \tacc: 87.115473\tValidation Loss: 151.551775, time5.3\n",
      "Epoch: 1011 \tTraining Loss: 57.294536 \tacc: 87.098255\tValidation Loss: 149.348007, time5.3\n",
      "Epoch: 1012 \tTraining Loss: 57.807239 \tacc: 86.736685\tValidation Loss: 146.644361, time5.2\n",
      "Epoch: 1013 \tTraining Loss: 58.170722 \tacc: 88.154270\tValidation Loss: 146.920600, time5.3\n",
      "Epoch: 1014 \tTraining Loss: 57.814891 \tacc: 86.432507\tValidation Loss: 145.618753, time5.3\n",
      "Epoch: 1015 \tTraining Loss: 57.110882 \tacc: 86.897383\tValidation Loss: 151.058892, time5.3\n",
      "Epoch: 1016 \tTraining Loss: 57.189317 \tacc: 87.964876\tValidation Loss: 147.877593, time5.3\n",
      "Epoch: 1017 \tTraining Loss: 58.164983 \tacc: 86.392332\tValidation Loss: 150.707417, time5.3\n",
      "Epoch: 1018 \tTraining Loss: 57.084099 \tacc: 86.202938\tValidation Loss: 152.306569, time5.3\n",
      "Epoch: 1019 \tTraining Loss: 57.344276 \tacc: 87.247475\tValidation Loss: 147.526933, time5.2\n",
      "Epoch: 1020 \tTraining Loss: 57.520279 \tacc: 86.960514\tValidation Loss: 152.177267, time5.3\n",
      "Epoch: 1021 \tTraining Loss: 58.180288 \tacc: 86.897383\tValidation Loss: 149.151618, time5.3\n",
      "Epoch: 1022 \tTraining Loss: 57.493496 \tacc: 87.115473\tValidation Loss: 150.457583, time5.3\n",
      "Epoch: 1023 \tTraining Loss: 58.061677 \tacc: 86.753903\tValidation Loss: 148.991525, time5.2\n",
      "Epoch: 1024 \tTraining Loss: 57.807239 \tacc: 86.690771\tValidation Loss: 147.671039, time5.3\n",
      "Epoch: 1025 \tTraining Loss: 57.874197 \tacc: 86.472681\tValidation Loss: 145.601458, time5.3\n",
      "Epoch: 1026 \tTraining Loss: 58.031068 \tacc: 87.620523\tValidation Loss: 146.361402, time5.3\n",
      "Epoch: 1027 \tTraining Loss: 57.669498 \tacc: 86.363636\tValidation Loss: 150.727822, time5.2\n",
      "Epoch: 1028 \tTraining Loss: 57.338537 \tacc: 87.264692\tValidation Loss: 150.989715, time5.3\n",
      "Epoch: 1029 \tTraining Loss: 57.703933 \tacc: 87.454086\tValidation Loss: 150.008506, time5.3\n",
      "Epoch: 1030 \tTraining Loss: 58.021503 \tacc: 86.621901\tValidation Loss: 150.870469, time5.3\n",
      "Epoch: 1031 \tTraining Loss: 58.375421 \tacc: 87.293388\tValidation Loss: 149.275488, time5.3\n",
      "Epoch: 1032 \tTraining Loss: 57.384451 \tacc: 87.764004\tValidation Loss: 149.254129, time5.3\n",
      "Epoch: 1033 \tTraining Loss: 57.095577 \tacc: 87.218779\tValidation Loss: 153.763190, time5.3\n",
      "Epoch: 1034 \tTraining Loss: 57.784282 \tacc: 87.121212\tValidation Loss: 149.683156, time5.2\n",
      "Epoch: 1035 \tTraining Loss: 57.516452 \tacc: 86.862948\tValidation Loss: 152.400664, time5.3\n",
      "Epoch: 1036 \tTraining Loss: 57.323232 \tacc: 87.109734\tValidation Loss: 149.190240, time5.3\n",
      "Epoch: 1037 \tTraining Loss: 58.119069 \tacc: 87.333563\tValidation Loss: 149.624030, time5.3\n",
      "Epoch: 1038 \tTraining Loss: 57.520279 \tacc: 87.689394\tValidation Loss: 149.506335, time5.3\n",
      "Epoch: 1039 \tTraining Loss: 58.597337 \tacc: 87.052342\tValidation Loss: 147.396237, time5.2\n",
      "Epoch: 1040 \tTraining Loss: 58.260637 \tacc: 86.828512\tValidation Loss: 148.148421, time5.2\n",
      "Epoch: 1041 \tTraining Loss: 57.160621 \tacc: 86.248852\tValidation Loss: 148.021989, time5.3\n",
      "Epoch: 1042 \tTraining Loss: 58.021503 \tacc: 88.888889\tValidation Loss: 145.229106, time5.3\n",
      "Epoch: 1043 \tTraining Loss: 59.222911 \tacc: 87.436869\tValidation Loss: 147.828342, time5.3\n",
      "Epoch: 1044 \tTraining Loss: 58.463422 \tacc: 86.794077\tValidation Loss: 146.918200, time5.2\n",
      "Epoch: 1045 \tTraining Loss: 58.130548 \tacc: 86.461203\tValidation Loss: 149.206172, time5.3\n",
      "Epoch: 1046 \tTraining Loss: 57.267753 \tacc: 86.461203\tValidation Loss: 147.177710, time5.3\n",
      "Epoch: 1047 \tTraining Loss: 57.160621 \tacc: 87.293388\tValidation Loss: 148.094482, time5.3\n",
      "Epoch: 1048 \tTraining Loss: 58.111417 \tacc: 86.248852\tValidation Loss: 150.893534, time5.3\n",
      "Epoch: 1049 \tTraining Loss: 57.640802 \tacc: 87.195822\tValidation Loss: 148.036164, time5.3\n",
      "Epoch: 1050 \tTraining Loss: 57.482017 \tacc: 86.725207\tValidation Loss: 148.833159, time5.3\n",
      "Epoch: 1051 \tTraining Loss: 56.869835 \tacc: 86.817034\tValidation Loss: 149.947323, time5.3\n",
      "Epoch: 1052 \tTraining Loss: 57.747934 \tacc: 86.403811\tValidation Loss: 149.570554, time5.3\n",
      "Epoch: 1053 \tTraining Loss: 57.036272 \tacc: 86.885904\tValidation Loss: 148.486975, time5.3\n",
      "Epoch: 1054 \tTraining Loss: 57.786195 \tacc: 86.788338\tValidation Loss: 146.218956, time5.3\n",
      "Epoch: 1055 \tTraining Loss: 57.824457 \tacc: 86.455464\tValidation Loss: 149.103508, time5.2\n",
      "Epoch: 1056 \tTraining Loss: 56.829660 \tacc: 86.673554\tValidation Loss: 151.160477, time5.3\n",
      "Epoch: 1057 \tTraining Loss: 57.495409 \tacc: 86.771120\tValidation Loss: 149.139300, time5.2\n",
      "Epoch: 1058 \tTraining Loss: 57.834022 \tacc: 86.874426\tValidation Loss: 148.272959, time5.3\n",
      "Epoch: 1059 \tTraining Loss: 57.807239 \tacc: 86.518595\tValidation Loss: 147.401145, time5.3\n",
      "Epoch: 1060 \tTraining Loss: 58.019590 \tacc: 86.587466\tValidation Loss: 149.394141, time5.3\n",
      "Epoch: 1061 \tTraining Loss: 56.401133 \tacc: 86.914601\tValidation Loss: 147.939250, time5.3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1062 \tTraining Loss: 57.296449 \tacc: 87.356520\tValidation Loss: 152.565049, time5.3\n",
      "Epoch: 1063 \tTraining Loss: 57.472452 \tacc: 86.587466\tValidation Loss: 152.241998, time5.3\n",
      "Epoch: 1064 \tTraining Loss: 57.214187 \tacc: 86.839991\tValidation Loss: 151.719262, time5.3\n",
      "Epoch: 1065 \tTraining Loss: 57.556627 \tacc: 86.409550\tValidation Loss: 150.112964, time5.3\n",
      "Epoch: 1066 \tTraining Loss: 57.244796 \tacc: 86.966253\tValidation Loss: 151.605624, time5.2\n",
      "Epoch: 1067 \tTraining Loss: 57.338537 \tacc: 86.570248\tValidation Loss: 150.214767, time5.3\n",
      "Epoch: 1068 \tTraining Loss: 57.466713 \tacc: 86.334940\tValidation Loss: 152.162881, time5.3\n",
      "Epoch: 1069 \tTraining Loss: 57.594888 \tacc: 86.558770\tValidation Loss: 151.269468, time5.3\n",
      "Epoch: 1070 \tTraining Loss: 57.939241 \tacc: 87.746786\tValidation Loss: 149.564549, time5.3\n",
      "Epoch: 1071 \tTraining Loss: 57.105142 \tacc: 86.352158\tValidation Loss: 152.667816, time5.3\n",
      "Epoch: 1072 \tTraining Loss: 57.614019 \tacc: 87.614784\tValidation Loss: 150.388525, time5.3\n",
      "Epoch: 1073 \tTraining Loss: 58.293159 \tacc: 87.281910\tValidation Loss: 148.327608, time5.3\n",
      "Epoch: 1074 \tTraining Loss: 57.755586 \tacc: 87.218779\tValidation Loss: 148.852180, time5.3\n",
      "Epoch: 1075 \tTraining Loss: 57.782369 \tacc: 86.834252\tValidation Loss: 149.335185, time5.3\n",
      "Epoch: 1076 \tTraining Loss: 57.757499 \tacc: 88.280533\tValidation Loss: 148.925610, time5.3\n",
      "Epoch: 1077 \tTraining Loss: 58.495944 \tacc: 87.155647\tValidation Loss: 147.092885, time5.2\n",
      "Epoch: 1078 \tTraining Loss: 57.491582 \tacc: 86.443985\tValidation Loss: 149.795162, time5.3\n",
      "Epoch: 1079 \tTraining Loss: 57.537496 \tacc: 86.633379\tValidation Loss: 151.888537, time5.3\n",
      "Epoch: 1080 \tTraining Loss: 57.214187 \tacc: 86.449725\tValidation Loss: 151.704952, time5.3\n",
      "Epoch: 1081 \tTraining Loss: 57.644628 \tacc: 86.093893\tValidation Loss: 152.055831, time5.3\n",
      "Validation loss decreased (86.185721 --> 86.093893).  Saving model ...\n",
      "Epoch: 1082 \tTraining Loss: 56.869835 \tacc: 86.644858\tValidation Loss: 152.422306, time5.3\n",
      "Epoch: 1083 \tTraining Loss: 56.988445 \tacc: 86.621901\tValidation Loss: 155.237153, time5.2\n",
      "Epoch: 1084 \tTraining Loss: 57.239057 \tacc: 86.484160\tValidation Loss: 150.428925, time5.2\n",
      "Epoch: 1085 \tTraining Loss: 57.539409 \tacc: 86.713728\tValidation Loss: 148.992433, time5.3\n",
      "Epoch: 1086 \tTraining Loss: 57.721151 \tacc: 88.188705\tValidation Loss: 150.478848, time5.2\n",
      "Epoch: 1087 \tTraining Loss: 57.348102 \tacc: 87.832874\tValidation Loss: 150.813513, time5.3\n",
      "Epoch: 1088 \tTraining Loss: 57.116621 \tacc: 86.828512\tValidation Loss: 153.370353, time5.2\n",
      "Epoch: 1089 \tTraining Loss: 57.174013 \tacc: 87.012167\tValidation Loss: 150.984337, time5.3\n",
      "Epoch: 1090 \tTraining Loss: 57.671411 \tacc: 87.281910\tValidation Loss: 151.392701, time5.2\n",
      "Epoch: 1091 \tTraining Loss: 56.967401 \tacc: 86.834252\tValidation Loss: 152.911977, time5.3\n",
      "Epoch: 1092 \tTraining Loss: 56.787573 \tacc: 86.484160\tValidation Loss: 153.867833, time5.2\n",
      "Epoch: 1093 \tTraining Loss: 57.376798 \tacc: 87.040863\tValidation Loss: 150.732192, time5.3\n",
      "Epoch: 1094 \tTraining Loss: 57.108968 \tacc: 87.396694\tValidation Loss: 152.689659, time5.2\n",
      "Epoch: 1095 \tTraining Loss: 56.466177 \tacc: 87.000689\tValidation Loss: 152.339062, time5.3\n",
      "Epoch: 1096 \tTraining Loss: 57.120447 \tacc: 86.725207\tValidation Loss: 151.266625, time5.2\n",
      "Epoch: 1097 \tTraining Loss: 56.994184 \tacc: 86.707989\tValidation Loss: 156.237892, time5.2\n",
      "Epoch: 1098 \tTraining Loss: 56.657484 \tacc: 87.385216\tValidation Loss: 156.696531, time5.3\n",
      "Epoch: 1099 \tTraining Loss: 56.722528 \tacc: 86.484160\tValidation Loss: 153.865172, time5.2\n",
      "Epoch: 1100 \tTraining Loss: 57.749847 \tacc: 87.276171\tValidation Loss: 151.110985, time5.3\n",
      "Epoch: 1101 \tTraining Loss: 57.416973 \tacc: 86.966253\tValidation Loss: 155.325942, time5.2\n",
      "Epoch: 1102 \tTraining Loss: 56.686180 \tacc: 86.851469\tValidation Loss: 154.045697, time5.3\n",
      "Epoch: 1103 \tTraining Loss: 56.774181 \tacc: 86.151286\tValidation Loss: 153.339888, time5.2\n",
      "Epoch: 1104 \tTraining Loss: 57.133838 \tacc: 87.838613\tValidation Loss: 147.784139, time5.3\n",
      "Epoch: 1105 \tTraining Loss: 57.851240 \tacc: 86.386593\tValidation Loss: 153.024982, time5.2\n",
      "Epoch: 1106 \tTraining Loss: 57.204622 \tacc: 86.834252\tValidation Loss: 148.300374, time5.3\n",
      "Epoch: 1107 \tTraining Loss: 57.403581 \tacc: 86.874426\tValidation Loss: 153.243255, time5.2\n",
      "Epoch: 1108 \tTraining Loss: 56.743572 \tacc: 87.614784\tValidation Loss: 149.461048, time5.2\n",
      "Epoch: 1109 \tTraining Loss: 57.252449 \tacc: 87.712351\tValidation Loss: 150.697017, time5.2\n",
      "Epoch: 1110 \tTraining Loss: 57.547062 \tacc: 88.085399\tValidation Loss: 152.090998, time5.2\n",
      "Epoch: 1111 \tTraining Loss: 57.399755 \tacc: 86.472681\tValidation Loss: 152.631367, time5.3\n",
      "Epoch: 1112 \tTraining Loss: 57.072620 \tacc: 86.977732\tValidation Loss: 151.287671, time5.2\n",
      "Epoch: 1113 \tTraining Loss: 57.185491 \tacc: 86.426768\tValidation Loss: 150.312117, time5.3\n",
      "Epoch: 1114 \tTraining Loss: 57.152969 \tacc: 87.632002\tValidation Loss: 150.903634, time5.2\n",
      "Epoch: 1115 \tTraining Loss: 57.311754 \tacc: 86.926079\tValidation Loss: 150.995039, time5.3\n",
      "Epoch: 1116 \tTraining Loss: 56.615396 \tacc: 86.197199\tValidation Loss: 156.723404, time5.2\n",
      "Epoch: 1117 \tTraining Loss: 57.130012 \tacc: 86.570248\tValidation Loss: 154.309046, time5.3\n",
      "Epoch: 1118 \tTraining Loss: 57.472452 \tacc: 86.329201\tValidation Loss: 153.869957, time5.2\n",
      "Epoch: 1119 \tTraining Loss: 57.330885 \tacc: 86.908861\tValidation Loss: 152.367428, time5.3\n",
      "Epoch: 1120 \tTraining Loss: 57.439930 \tacc: 86.541552\tValidation Loss: 154.394022, time5.2\n",
      "Epoch: 1121 \tTraining Loss: 57.154882 \tacc: 86.633379\tValidation Loss: 155.053013, time5.3\n",
      "Epoch: 1122 \tTraining Loss: 57.422712 \tacc: 88.406795\tValidation Loss: 153.233150, time5.2\n",
      "Epoch: 1123 \tTraining Loss: 57.587236 \tacc: 87.167126\tValidation Loss: 152.283586, time5.2\n",
      "Epoch: 1124 \tTraining Loss: 57.084099 \tacc: 86.202938\tValidation Loss: 156.615143, time5.3\n",
      "Epoch: 1125 \tTraining Loss: 56.929140 \tacc: 86.771120\tValidation Loss: 153.141460, time5.2\n",
      "Epoch: 1126 \tTraining Loss: 57.254362 \tacc: 86.960514\tValidation Loss: 155.134803, time5.3\n",
      "Epoch: 1127 \tTraining Loss: 57.679063 \tacc: 86.346419\tValidation Loss: 154.496414, time5.2\n",
      "Epoch: 1128 \tTraining Loss: 57.198883 \tacc: 86.707989\tValidation Loss: 153.587507, time5.3\n",
      "Epoch: 1129 \tTraining Loss: 56.726354 \tacc: 87.379477\tValidation Loss: 154.689949, time5.2\n",
      "Epoch: 1130 \tTraining Loss: 57.118534 \tacc: 88.039486\tValidation Loss: 154.404279, time5.3\n",
      "Epoch: 1131 \tTraining Loss: 57.472452 \tacc: 87.023646\tValidation Loss: 155.719719, time5.2\n",
      "Epoch: 1132 \tTraining Loss: 57.384451 \tacc: 86.891644\tValidation Loss: 155.204742, time5.3\n",
      "Epoch: 1133 \tTraining Loss: 57.485843 \tacc: 86.966253\tValidation Loss: 151.433266, time5.2\n",
      "Epoch: 1134 \tTraining Loss: 56.978880 \tacc: 86.627640\tValidation Loss: 153.034757, time5.3\n",
      "Epoch: 1135 \tTraining Loss: 57.893327 \tacc: 88.079660\tValidation Loss: 153.363000, time5.2\n",
      "Epoch: 1136 \tTraining Loss: 57.399755 \tacc: 86.604683\tValidation Loss: 156.502816, time5.2\n",
      "Epoch: 1137 \tTraining Loss: 57.449495 \tacc: 86.466942\tValidation Loss: 153.360200, time5.3\n",
      "Epoch: 1138 \tTraining Loss: 57.170187 \tacc: 86.524334\tValidation Loss: 151.563236, time5.2\n",
      "Epoch: 1139 \tTraining Loss: 56.479568 \tacc: 86.346419\tValidation Loss: 153.326010, time5.3\n",
      "Epoch: 1140 \tTraining Loss: 57.422712 \tacc: 87.167126\tValidation Loss: 154.977611, time5.2\n",
      "Epoch: 1141 \tTraining Loss: 57.359581 \tacc: 86.558770\tValidation Loss: 152.969499, time5.3\n",
      "Epoch: 1142 \tTraining Loss: 57.415060 \tacc: 86.392332\tValidation Loss: 152.472965, time5.2\n",
      "Epoch: 1143 \tTraining Loss: 56.674702 \tacc: 86.897383\tValidation Loss: 152.339893, time5.3\n",
      "Epoch: 1144 \tTraining Loss: 57.151056 \tacc: 88.929063\tValidation Loss: 154.074378, time5.2\n",
      "Epoch: 1145 \tTraining Loss: 58.268289 \tacc: 87.700872\tValidation Loss: 153.594829, time5.3\n",
      "Epoch: 1146 \tTraining Loss: 57.330885 \tacc: 85.927456\tValidation Loss: 156.440405, time5.2\n",
      "Validation loss decreased (86.093893 --> 85.927456).  Saving model ...\n",
      "Epoch: 1147 \tTraining Loss: 56.950184 \tacc: 86.662075\tValidation Loss: 155.577751, time5.3\n",
      "Epoch: 1148 \tTraining Loss: 57.055403 \tacc: 86.185721\tValidation Loss: 157.431336, time5.3\n",
      "Epoch: 1149 \tTraining Loss: 56.236609 \tacc: 86.771120\tValidation Loss: 155.458323, time5.2\n",
      "Epoch: 1150 \tTraining Loss: 56.456612 \tacc: 88.068182\tValidation Loss: 154.580095, time5.3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1151 \tTraining Loss: 57.350015 \tacc: 87.138430\tValidation Loss: 155.764807, time5.3\n",
      "Epoch: 1152 \tTraining Loss: 56.544613 \tacc: 86.690771\tValidation Loss: 160.276766, time5.3\n",
      "Epoch: 1153 \tTraining Loss: 56.950184 \tacc: 86.323462\tValidation Loss: 153.295353, time5.2\n",
      "Epoch: 1154 \tTraining Loss: 56.443220 \tacc: 86.828512\tValidation Loss: 156.863742, time5.3\n",
      "Epoch: 1155 \tTraining Loss: 56.866009 \tacc: 87.310606\tValidation Loss: 154.084572, time5.3\n",
      "Epoch: 1156 \tTraining Loss: 57.193144 \tacc: 86.294766\tValidation Loss: 159.019675, time5.3\n",
      "Epoch: 1157 \tTraining Loss: 57.288797 \tacc: 86.231635\tValidation Loss: 159.431516, time5.3\n",
      "Epoch: 1158 \tTraining Loss: 56.362871 \tacc: 86.076676\tValidation Loss: 156.853519, time5.3\n",
      "Epoch: 1159 \tTraining Loss: 56.996097 \tacc: 86.478421\tValidation Loss: 157.433266, time5.3\n",
      "Epoch: 1160 \tTraining Loss: 55.997475 \tacc: 86.507117\tValidation Loss: 157.562053, time5.3\n",
      "Epoch: 1161 \tTraining Loss: 56.852617 \tacc: 86.231635\tValidation Loss: 158.807065, time5.3\n",
      "Epoch: 1162 \tTraining Loss: 56.383915 \tacc: 88.538797\tValidation Loss: 155.660337, time5.3\n",
      "Epoch: 1163 \tTraining Loss: 57.560453 \tacc: 87.035124\tValidation Loss: 157.765230, time5.2\n",
      "Epoch: 1164 \tTraining Loss: 56.856443 \tacc: 86.587466\tValidation Loss: 152.308443, time5.2\n",
      "Epoch: 1165 \tTraining Loss: 56.858356 \tacc: 87.465565\tValidation Loss: 154.356526, time5.2\n",
      "Epoch: 1166 \tTraining Loss: 57.051576 \tacc: 87.993572\tValidation Loss: 157.238110, time5.3\n",
      "Epoch: 1167 \tTraining Loss: 56.992271 \tacc: 86.426768\tValidation Loss: 154.484393, time5.2\n",
      "Epoch: 1168 \tTraining Loss: 56.471916 \tacc: 87.402433\tValidation Loss: 155.550741, time5.3\n",
      "Epoch: 1169 \tTraining Loss: 56.399219 \tacc: 86.610422\tValidation Loss: 159.465299, time5.3\n",
      "Epoch: 1170 \tTraining Loss: 56.579048 \tacc: 86.610422\tValidation Loss: 161.594312, time5.3\n",
      "Epoch: 1171 \tTraining Loss: 56.839226 \tacc: 86.570248\tValidation Loss: 157.193292, time5.3\n",
      "Epoch: 1172 \tTraining Loss: 56.554178 \tacc: 86.931818\tValidation Loss: 155.798086, time5.3\n",
      "Epoch: 1173 \tTraining Loss: 56.261478 \tacc: 86.839991\tValidation Loss: 157.163887, time5.3\n",
      "Epoch: 1174 \tTraining Loss: 57.086012 \tacc: 86.989210\tValidation Loss: 154.159749, time5.2\n",
      "Epoch: 1175 \tTraining Loss: 57.426538 \tacc: 87.109734\tValidation Loss: 153.073586, time5.3\n",
      "Epoch: 1176 \tTraining Loss: 57.103229 \tacc: 87.075298\tValidation Loss: 158.702316, time5.3\n",
      "Epoch: 1177 \tTraining Loss: 57.084099 \tacc: 87.436869\tValidation Loss: 157.387873, time5.3\n",
      "Epoch: 1178 \tTraining Loss: 56.820095 \tacc: 87.012167\tValidation Loss: 159.300215, time5.3\n",
      "Epoch: 1179 \tTraining Loss: 56.756964 \tacc: 88.045225\tValidation Loss: 155.747733, time5.3\n",
      "Epoch: 1180 \tTraining Loss: 56.297827 \tacc: 86.145546\tValidation Loss: 160.528060, time5.3\n",
      "Epoch: 1181 \tTraining Loss: 56.383915 \tacc: 86.817034\tValidation Loss: 159.927913, time5.3\n",
      "Epoch: 1182 \tTraining Loss: 56.213652 \tacc: 86.300505\tValidation Loss: 158.512197, time5.3\n",
      "Epoch: 1183 \tTraining Loss: 57.087925 \tacc: 88.056703\tValidation Loss: 155.605782, time5.3\n",
      "Epoch: 1184 \tTraining Loss: 57.814891 \tacc: 86.662075\tValidation Loss: 157.056800, time5.3\n",
      "Epoch: 1185 \tTraining Loss: 56.615396 \tacc: 87.029385\tValidation Loss: 157.048503, time5.2\n",
      "Epoch: 1186 \tTraining Loss: 56.458525 \tacc: 86.507117\tValidation Loss: 156.312343, time5.3\n",
      "Epoch: 1187 \tTraining Loss: 56.202173 \tacc: 87.511478\tValidation Loss: 159.445722, time5.3\n",
      "Epoch: 1188 \tTraining Loss: 57.045837 \tacc: 86.914601\tValidation Loss: 156.266396, time5.3\n",
      "Epoch: 1189 \tTraining Loss: 56.326523 \tacc: 86.685032\tValidation Loss: 162.118601, time5.3\n",
      "Epoch: 1190 \tTraining Loss: 56.504438 \tacc: 87.522957\tValidation Loss: 159.556073, time5.3\n",
      "Epoch: 1191 \tTraining Loss: 57.114708 \tacc: 87.459826\tValidation Loss: 156.978324, time5.3\n",
      "Epoch: 1192 \tTraining Loss: 57.120447 \tacc: 86.598944\tValidation Loss: 155.763814, time5.3\n",
      "Epoch: 1193 \tTraining Loss: 56.957836 \tacc: 86.725207\tValidation Loss: 157.432788, time5.3\n",
      "Epoch: 1194 \tTraining Loss: 56.351393 \tacc: 86.748163\tValidation Loss: 158.962294, time5.3\n",
      "Epoch: 1195 \tTraining Loss: 56.492960 \tacc: 86.903122\tValidation Loss: 158.538349, time5.3\n",
      "Epoch: 1196 \tTraining Loss: 57.328972 \tacc: 87.040863\tValidation Loss: 155.592623, time5.2\n",
      "Epoch: 1197 \tTraining Loss: 56.678528 \tacc: 87.063820\tValidation Loss: 155.282973, time5.3\n",
      "Epoch: 1198 \tTraining Loss: 56.232782 \tacc: 86.713728\tValidation Loss: 158.509344, time5.3\n",
      "Epoch: 1199 \tTraining Loss: 56.047215 \tacc: 86.719467\tValidation Loss: 159.120206, time5.3\n",
      "Epoch: 1200 \tTraining Loss: 57.392103 \tacc: 86.839991\tValidation Loss: 155.492867, time5.3\n",
      "Epoch: 1201 \tTraining Loss: 56.315044 \tacc: 89.342287\tValidation Loss: 158.438291, time5.3\n",
      "Epoch: 1202 \tTraining Loss: 57.839761 \tacc: 86.478421\tValidation Loss: 157.875278, time5.3\n",
      "Epoch: 1203 \tTraining Loss: 56.552265 \tacc: 86.776860\tValidation Loss: 161.008314, time5.3\n",
      "Epoch: 1204 \tTraining Loss: 56.617309 \tacc: 86.323462\tValidation Loss: 162.366672, time5.3\n",
      "Epoch: 1205 \tTraining Loss: 55.911387 \tacc: 86.627640\tValidation Loss: 159.045887, time5.3\n",
      "Epoch: 1206 \tTraining Loss: 56.360958 \tacc: 86.621901\tValidation Loss: 158.522732, time5.2\n",
      "Epoch: 1207 \tTraining Loss: 56.041475 \tacc: 86.489899\tValidation Loss: 162.627988, time5.3\n",
      "Epoch: 1208 \tTraining Loss: 56.110346 \tacc: 87.959137\tValidation Loss: 158.970622, time5.3\n",
      "Epoch: 1209 \tTraining Loss: 56.858356 \tacc: 88.814279\tValidation Loss: 158.486896, time5.3\n",
      "Epoch: 1210 \tTraining Loss: 59.152127 \tacc: 87.786961\tValidation Loss: 154.964451, time5.3\n",
      "Epoch: 1211 \tTraining Loss: 57.900979 \tacc: 86.782599\tValidation Loss: 157.589408, time5.3\n",
      "Epoch: 1212 \tTraining Loss: 58.086547 \tacc: 87.115473\tValidation Loss: 159.479122, time5.3\n",
      "Epoch: 1213 \tTraining Loss: 57.606367 \tacc: 87.345041\tValidation Loss: 155.691005, time5.3\n",
      "Epoch: 1214 \tTraining Loss: 57.292623 \tacc: 86.960514\tValidation Loss: 162.722281, time5.3\n",
      "Epoch: 1215 \tTraining Loss: 57.399755 \tacc: 87.855831\tValidation Loss: 160.533437, time5.3\n",
      "Epoch: 1216 \tTraining Loss: 56.427916 \tacc: 87.040863\tValidation Loss: 160.983117, time5.3\n",
      "Epoch: 1217 \tTraining Loss: 57.064968 \tacc: 87.190083\tValidation Loss: 157.667248, time5.2\n",
      "Epoch: 1218 \tTraining Loss: 56.820095 \tacc: 87.522957\tValidation Loss: 160.995728, time5.3\n",
      "Epoch: 1219 \tTraining Loss: 57.019054 \tacc: 88.050964\tValidation Loss: 159.637246, time5.3\n",
      "Epoch: 1220 \tTraining Loss: 57.233318 \tacc: 86.799816\tValidation Loss: 161.813121, time5.3\n",
      "Epoch: 1221 \tTraining Loss: 56.454699 \tacc: 87.276171\tValidation Loss: 160.869483, time5.3\n",
      "Epoch: 1222 \tTraining Loss: 56.778007 \tacc: 87.396694\tValidation Loss: 158.329287, time5.3\n",
      "Epoch: 1223 \tTraining Loss: 56.596266 \tacc: 88.590450\tValidation Loss: 159.519653, time5.3\n",
      "Epoch: 1224 \tTraining Loss: 56.971227 \tacc: 87.075298\tValidation Loss: 163.045866, time5.3\n",
      "Epoch: 1225 \tTraining Loss: 56.695745 \tacc: 87.287649\tValidation Loss: 159.179125, time5.3\n",
      "Epoch: 1226 \tTraining Loss: 57.679063 \tacc: 86.730946\tValidation Loss: 155.978536, time5.3\n",
      "Epoch: 1227 \tTraining Loss: 56.659397 \tacc: 86.730946\tValidation Loss: 160.142227, time5.3\n",
      "Epoch: 1228 \tTraining Loss: 56.468090 \tacc: 86.794077\tValidation Loss: 160.540645, time5.2\n",
      "Epoch: 1229 \tTraining Loss: 56.299740 \tacc: 86.621901\tValidation Loss: 161.712303, time5.3\n",
      "Epoch: 1230 \tTraining Loss: 56.586700 \tacc: 86.880165\tValidation Loss: 164.922411, time5.3\n",
      "Epoch: 1231 \tTraining Loss: 56.213652 \tacc: 86.926079\tValidation Loss: 161.257167, time5.3\n",
      "Epoch: 1232 \tTraining Loss: 56.913835 \tacc: 86.403811\tValidation Loss: 160.534245, time5.3\n",
      "Epoch: 1233 \tTraining Loss: 56.536961 \tacc: 87.230257\tValidation Loss: 161.910704, time5.3\n",
      "Epoch: 1234 \tTraining Loss: 56.408785 \tacc: 87.580349\tValidation Loss: 160.718487, time5.3\n",
      "Epoch: 1235 \tTraining Loss: 56.351393 \tacc: 87.103994\tValidation Loss: 158.261507, time5.3\n",
      "Epoch: 1236 \tTraining Loss: 57.072620 \tacc: 86.512856\tValidation Loss: 159.792900, time5.3\n",
      "Epoch: 1237 \tTraining Loss: 56.898531 \tacc: 86.713728\tValidation Loss: 159.654229, time5.3\n",
      "Epoch: 1238 \tTraining Loss: 56.225130 \tacc: 86.903122\tValidation Loss: 163.193555, time5.3\n",
      "Epoch: 1239 \tTraining Loss: 56.244261 \tacc: 86.593205\tValidation Loss: 162.671672, time5.2\n",
      "Epoch: 1240 \tTraining Loss: 56.370523 \tacc: 86.685032\tValidation Loss: 161.861719, time5.3\n",
      "Epoch: 1241 \tTraining Loss: 56.255739 \tacc: 87.379477\tValidation Loss: 157.667270, time5.2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1242 \tTraining Loss: 56.462351 \tacc: 86.174242\tValidation Loss: 161.260290, time5.2\n",
      "Epoch: 1243 \tTraining Loss: 56.638353 \tacc: 88.642103\tValidation Loss: 157.482138, time5.3\n",
      "Epoch: 1244 \tTraining Loss: 56.663223 \tacc: 86.151286\tValidation Loss: 162.578345, time5.3\n",
      "Epoch: 1245 \tTraining Loss: 56.119911 \tacc: 88.223140\tValidation Loss: 158.336805, time5.3\n",
      "Epoch: 1246 \tTraining Loss: 56.927227 \tacc: 87.729568\tValidation Loss: 157.913061, time5.3\n",
      "Epoch: 1247 \tTraining Loss: 56.898531 \tacc: 86.461203\tValidation Loss: 156.612662, time5.3\n",
      "Epoch: 1248 \tTraining Loss: 56.418350 \tacc: 87.161387\tValidation Loss: 162.671754, time5.3\n",
      "Epoch: 1249 \tTraining Loss: 56.223217 \tacc: 86.443985\tValidation Loss: 160.509049, time5.2\n",
      "Epoch: 1250 \tTraining Loss: 55.534512 \tacc: 87.017906\tValidation Loss: 163.780688, time5.3\n",
      "Epoch: 1251 \tTraining Loss: 56.374350 \tacc: 86.656336\tValidation Loss: 163.338635, time5.3\n",
      "Epoch: 1252 \tTraining Loss: 57.460973 \tacc: 86.690771\tValidation Loss: 161.553594, time5.3\n",
      "Epoch: 1253 \tTraining Loss: 56.600092 \tacc: 87.126951\tValidation Loss: 163.231651, time5.3\n",
      "Epoch: 1254 \tTraining Loss: 55.934343 \tacc: 87.258953\tValidation Loss: 163.461161, time5.3\n",
      "Epoch: 1255 \tTraining Loss: 56.580961 \tacc: 86.621901\tValidation Loss: 163.772961, time5.3\n",
      "Epoch: 1256 \tTraining Loss: 56.272957 \tacc: 86.575987\tValidation Loss: 163.702538, time5.3\n",
      "Epoch: 1257 \tTraining Loss: 55.882691 \tacc: 86.989210\tValidation Loss: 163.443973, time5.3\n",
      "Epoch: 1258 \tTraining Loss: 56.812443 \tacc: 87.304867\tValidation Loss: 158.190129, time5.3\n",
      "Epoch: 1259 \tTraining Loss: 56.624962 \tacc: 87.741047\tValidation Loss: 157.186150, time5.3\n",
      "Epoch: 1260 \tTraining Loss: 56.374350 \tacc: 86.564509\tValidation Loss: 162.520600, time5.2\n",
      "Epoch: 1261 \tTraining Loss: 56.479568 \tacc: 86.805556\tValidation Loss: 160.557760, time5.3\n",
      "Epoch: 1262 \tTraining Loss: 56.121824 \tacc: 87.115473\tValidation Loss: 161.602565, time5.3\n",
      "Epoch: 1263 \tTraining Loss: 56.347567 \tacc: 86.633379\tValidation Loss: 159.857553, time5.3\n",
      "Epoch: 1264 \tTraining Loss: 56.008953 \tacc: 87.878788\tValidation Loss: 161.955110, time5.3\n",
      "Epoch: 1265 \tTraining Loss: 57.099403 \tacc: 86.375115\tValidation Loss: 157.699399, time5.3\n",
      "Epoch: 1266 \tTraining Loss: 56.127564 \tacc: 87.517218\tValidation Loss: 161.623660, time5.3\n",
      "Epoch: 1267 \tTraining Loss: 56.709137 \tacc: 87.316345\tValidation Loss: 162.581088, time5.3\n",
      "Epoch: 1268 \tTraining Loss: 56.808616 \tacc: 86.134068\tValidation Loss: 161.827280, time5.3\n",
      "Epoch: 1269 \tTraining Loss: 55.452250 \tacc: 87.046602\tValidation Loss: 164.109435, time5.3\n",
      "Epoch: 1270 \tTraining Loss: 56.485308 \tacc: 86.053719\tValidation Loss: 164.768964, time5.3\n",
      "Epoch: 1271 \tTraining Loss: 55.779385 \tacc: 86.547291\tValidation Loss: 162.640395, time5.2\n",
      "Epoch: 1272 \tTraining Loss: 56.420263 \tacc: 87.654959\tValidation Loss: 165.648016, time5.2\n",
      "Epoch: 1273 \tTraining Loss: 57.089838 \tacc: 87.614784\tValidation Loss: 165.058520, time5.3\n",
      "Epoch: 1274 \tTraining Loss: 55.637818 \tacc: 86.340680\tValidation Loss: 163.441891, time5.2\n",
      "Epoch: 1275 \tTraining Loss: 56.246174 \tacc: 86.432507\tValidation Loss: 164.878027, time5.3\n",
      "Epoch: 1276 \tTraining Loss: 55.572773 \tacc: 86.937557\tValidation Loss: 166.500797, time5.3\n",
      "Epoch: 1277 \tTraining Loss: 55.827212 \tacc: 86.587466\tValidation Loss: 166.333876, time5.2\n",
      "Epoch: 1278 \tTraining Loss: 56.070171 \tacc: 87.488522\tValidation Loss: 163.378143, time5.2\n",
      "Epoch: 1279 \tTraining Loss: 56.110346 \tacc: 87.218779\tValidation Loss: 166.041319, time5.2\n",
      "Epoch: 1280 \tTraining Loss: 56.466177 \tacc: 87.207300\tValidation Loss: 160.609806, time5.2\n",
      "Epoch: 1281 \tTraining Loss: 56.774181 \tacc: 87.058081\tValidation Loss: 165.851536, time5.2\n",
      "Epoch: 1282 \tTraining Loss: 56.196434 \tacc: 86.725207\tValidation Loss: 162.277182, time5.3\n",
      "Epoch: 1283 \tTraining Loss: 55.618687 \tacc: 86.134068\tValidation Loss: 163.679800, time5.3\n",
      "Epoch: 1284 \tTraining Loss: 55.748776 \tacc: 86.317723\tValidation Loss: 162.668618, time5.3\n",
      "Epoch: 1285 \tTraining Loss: 56.357132 \tacc: 87.023646\tValidation Loss: 164.263120, time5.3\n",
      "Epoch: 1286 \tTraining Loss: 55.798515 \tacc: 86.461203\tValidation Loss: 165.854312, time5.2\n",
      "Epoch: 1287 \tTraining Loss: 55.429293 \tacc: 85.996327\tValidation Loss: 169.075209, time5.2\n",
      "Epoch: 1288 \tTraining Loss: 56.041475 \tacc: 87.069559\tValidation Loss: 162.667430, time5.3\n",
      "Epoch: 1289 \tTraining Loss: 56.112259 \tacc: 86.208678\tValidation Loss: 165.306446, time5.2\n",
      "Epoch: 1290 \tTraining Loss: 55.733471 \tacc: 86.157025\tValidation Loss: 167.377778, time5.2\n",
      "Epoch: 1291 \tTraining Loss: 55.919039 \tacc: 86.811295\tValidation Loss: 169.755477, time5.3\n",
      "Epoch: 1292 \tTraining Loss: 56.202173 \tacc: 86.616162\tValidation Loss: 166.459051, time5.2\n",
      "Epoch: 1293 \tTraining Loss: 56.062519 \tacc: 86.111111\tValidation Loss: 168.130544, time5.2\n",
      "Epoch: 1294 \tTraining Loss: 56.171564 \tacc: 87.138430\tValidation Loss: 163.414747, time5.2\n",
      "Epoch: 1295 \tTraining Loss: 56.087389 \tacc: 85.973370\tValidation Loss: 169.067611, time5.3\n",
      "Epoch: 1296 \tTraining Loss: 55.886517 \tacc: 86.696511\tValidation Loss: 164.758078, time5.2\n",
      "Epoch: 1297 \tTraining Loss: 55.984083 \tacc: 87.804178\tValidation Loss: 161.764842, time5.2\n",
      "Epoch: 1298 \tTraining Loss: 57.179752 \tacc: 87.425390\tValidation Loss: 162.770067, time5.3\n",
      "Epoch: 1299 \tTraining Loss: 56.638353 \tacc: 86.306244\tValidation Loss: 163.922555, time5.2\n",
      "Epoch: 1300 \tTraining Loss: 56.020432 \tacc: 86.868687\tValidation Loss: 163.108381, time5.2\n",
      "Epoch: 1301 \tTraining Loss: 55.869299 \tacc: 86.507117\tValidation Loss: 160.878845, time5.2\n",
      "Epoch: 1302 \tTraining Loss: 56.906183 \tacc: 86.920340\tValidation Loss: 163.436132, time5.3\n",
      "Epoch: 1303 \tTraining Loss: 55.480946 \tacc: 87.402433\tValidation Loss: 163.152834, time5.2\n",
      "Epoch: 1304 \tTraining Loss: 56.123737 \tacc: 86.507117\tValidation Loss: 168.082874, time5.3\n",
      "Epoch: 1305 \tTraining Loss: 56.366697 \tacc: 87.029385\tValidation Loss: 165.866132, time5.3\n",
      "Epoch: 1306 \tTraining Loss: 56.278696 \tacc: 87.012167\tValidation Loss: 165.222433, time5.3\n",
      "Epoch: 1307 \tTraining Loss: 56.093128 \tacc: 87.390955\tValidation Loss: 166.544588, time5.3\n",
      "Epoch: 1308 \tTraining Loss: 56.640266 \tacc: 86.455464\tValidation Loss: 166.373836, time5.3\n",
      "Epoch: 1309 \tTraining Loss: 56.104607 \tacc: 86.581726\tValidation Loss: 167.270550, time5.3\n",
      "Epoch: 1310 \tTraining Loss: 55.547903 \tacc: 86.478421\tValidation Loss: 168.920610, time5.2\n",
      "Epoch: 1311 \tTraining Loss: 55.589991 \tacc: 87.184343\tValidation Loss: 169.229499, time5.2\n",
      "Epoch: 1312 \tTraining Loss: 56.751224 \tacc: 86.501377\tValidation Loss: 162.994736, time5.2\n",
      "Epoch: 1313 \tTraining Loss: 56.571396 \tacc: 86.707989\tValidation Loss: 165.893229, time5.2\n",
      "Epoch: 1314 \tTraining Loss: 55.750689 \tacc: 86.667815\tValidation Loss: 166.522125, time5.2\n",
      "Epoch: 1315 \tTraining Loss: 56.093128 \tacc: 86.885904\tValidation Loss: 165.960309, time5.3\n",
      "Epoch: 1316 \tTraining Loss: 56.299740 \tacc: 86.025023\tValidation Loss: 166.776046, time5.2\n",
      "Epoch: 1317 \tTraining Loss: 56.257652 \tacc: 88.131313\tValidation Loss: 163.219141, time5.3\n",
      "Epoch: 1318 \tTraining Loss: 56.003214 \tacc: 87.132691\tValidation Loss: 164.795816, time5.2\n",
      "Epoch: 1319 \tTraining Loss: 55.555556 \tacc: 87.551653\tValidation Loss: 165.296141, time5.2\n",
      "Epoch: 1320 \tTraining Loss: 56.414524 \tacc: 86.277548\tValidation Loss: 167.573777, time5.3\n",
      "Epoch: 1321 \tTraining Loss: 55.427380 \tacc: 86.220156\tValidation Loss: 167.869470, time5.2\n",
      "Epoch: 1322 \tTraining Loss: 55.741123 \tacc: 86.685032\tValidation Loss: 166.833245, time5.3\n",
      "Epoch: 1323 \tTraining Loss: 56.139042 \tacc: 86.788338\tValidation Loss: 166.348835, time5.3\n",
      "Epoch: 1324 \tTraining Loss: 56.313131 \tacc: 86.725207\tValidation Loss: 163.588817, time5.2\n",
      "Epoch: 1325 \tTraining Loss: 55.368075 \tacc: 87.976354\tValidation Loss: 165.520890, time5.3\n",
      "Epoch: 1326 \tTraining Loss: 56.835399 \tacc: 85.824151\tValidation Loss: 167.418265, time5.3\n",
      "Validation loss decreased (85.927456 --> 85.824151).  Saving model ...\n",
      "Epoch: 1327 \tTraining Loss: 55.368075 \tacc: 86.260331\tValidation Loss: 167.343315, time5.3\n",
      "Epoch: 1328 \tTraining Loss: 56.661310 \tacc: 86.771120\tValidation Loss: 166.101764, time5.2\n",
      "Epoch: 1329 \tTraining Loss: 56.089302 \tacc: 86.604683\tValidation Loss: 166.242224, time5.2\n",
      "Epoch: 1330 \tTraining Loss: 56.280609 \tacc: 86.794077\tValidation Loss: 167.206167, time5.2\n",
      "Epoch: 1331 \tTraining Loss: 56.862182 \tacc: 86.329201\tValidation Loss: 167.384184, time5.3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1332 \tTraining Loss: 55.599556 \tacc: 86.949036\tValidation Loss: 166.939550, time5.2\n",
      "Epoch: 1333 \tTraining Loss: 55.947735 \tacc: 87.809917\tValidation Loss: 166.768269, time5.2\n",
      "Epoch: 1334 \tTraining Loss: 55.909474 \tacc: 86.415289\tValidation Loss: 168.472554, time5.2\n",
      "Epoch: 1335 \tTraining Loss: 55.484772 \tacc: 86.696511\tValidation Loss: 164.685116, time5.2\n",
      "Epoch: 1336 \tTraining Loss: 56.184956 \tacc: 86.575987\tValidation Loss: 168.124518, time5.3\n",
      "Epoch: 1337 \tTraining Loss: 56.238522 \tacc: 87.293388\tValidation Loss: 167.416503, time5.2\n",
      "Epoch: 1338 \tTraining Loss: 56.343740 \tacc: 86.719467\tValidation Loss: 169.995181, time5.3\n",
      "Epoch: 1339 \tTraining Loss: 55.647383 \tacc: 87.218779\tValidation Loss: 167.676674, time5.2\n",
      "Epoch: 1340 \tTraining Loss: 55.840603 \tacc: 87.769743\tValidation Loss: 167.455442, time5.3\n",
      "Epoch: 1341 \tTraining Loss: 56.171564 \tacc: 86.128329\tValidation Loss: 167.768421, time5.2\n",
      "Epoch: 1342 \tTraining Loss: 56.607744 \tacc: 87.970615\tValidation Loss: 164.702050, time5.3\n",
      "Epoch: 1343 \tTraining Loss: 57.399755 \tacc: 87.964876\tValidation Loss: 161.995928, time5.2\n",
      "Epoch: 1344 \tTraining Loss: 56.372436 \tacc: 86.817034\tValidation Loss: 168.904268, time5.2\n",
      "Epoch: 1345 \tTraining Loss: 55.896082 \tacc: 88.246097\tValidation Loss: 159.215491, time5.2\n",
      "Epoch: 1346 \tTraining Loss: 56.215565 \tacc: 86.587466\tValidation Loss: 169.625395, time5.2\n",
      "Epoch: 1347 \tTraining Loss: 55.171028 \tacc: 86.914601\tValidation Loss: 169.970729, time5.2\n",
      "Epoch: 1348 \tTraining Loss: 56.615396 \tacc: 87.632002\tValidation Loss: 165.820163, time5.2\n",
      "Epoch: 1349 \tTraining Loss: 56.791399 \tacc: 87.626263\tValidation Loss: 165.886688, time5.3\n",
      "Epoch: 1350 \tTraining Loss: 57.131925 \tacc: 87.017906\tValidation Loss: 163.320051, time5.2\n",
      "Epoch: 1351 \tTraining Loss: 56.848791 \tacc: 86.857208\tValidation Loss: 166.789711, time5.3\n",
      "Epoch: 1352 \tTraining Loss: 55.515381 \tacc: 87.069559\tValidation Loss: 164.480405, time5.2\n",
      "Epoch: 1353 \tTraining Loss: 56.290174 \tacc: 87.092516\tValidation Loss: 168.635109, time5.3\n",
      "Epoch: 1354 \tTraining Loss: 55.521120 \tacc: 86.748163\tValidation Loss: 169.116438, time5.2\n",
      "Epoch: 1355 \tTraining Loss: 55.857821 \tacc: 86.575987\tValidation Loss: 167.004797, time5.2\n",
      "Epoch: 1356 \tTraining Loss: 55.253290 \tacc: 87.069559\tValidation Loss: 169.759894, time5.2\n",
      "Epoch: 1357 \tTraining Loss: 56.315044 \tacc: 86.748163\tValidation Loss: 167.973817, time5.2\n",
      "Epoch: 1358 \tTraining Loss: 55.402510 \tacc: 86.759642\tValidation Loss: 169.796209, time5.2\n",
      "Epoch: 1359 \tTraining Loss: 55.553642 \tacc: 86.386593\tValidation Loss: 166.983402, time5.3\n",
      "Epoch: 1360 \tTraining Loss: 55.285813 \tacc: 86.208678\tValidation Loss: 171.216920, time5.2\n",
      "Epoch: 1361 \tTraining Loss: 55.907560 \tacc: 86.707989\tValidation Loss: 173.573538, time5.3\n",
      "Epoch: 1362 \tTraining Loss: 56.429829 \tacc: 86.461203\tValidation Loss: 168.028929, time5.2\n",
      "Epoch: 1363 \tTraining Loss: 55.764080 \tacc: 86.214417\tValidation Loss: 173.399753, time5.2\n",
      "Epoch: 1364 \tTraining Loss: 55.513468 \tacc: 86.294766\tValidation Loss: 170.089273, time5.2\n",
      "Epoch: 1365 \tTraining Loss: 56.251913 \tacc: 88.114096\tValidation Loss: 169.062314, time5.2\n",
      "Epoch: 1366 \tTraining Loss: 56.684267 \tacc: 86.455464\tValidation Loss: 169.374779, time5.2\n",
      "Epoch: 1367 \tTraining Loss: 55.712427 \tacc: 86.357897\tValidation Loss: 174.409485, time5.2\n",
      "Epoch: 1368 \tTraining Loss: 55.193985 \tacc: 87.098255\tValidation Loss: 172.500485, time5.2\n",
      "Epoch: 1369 \tTraining Loss: 55.733471 \tacc: 86.398072\tValidation Loss: 170.792224, time5.2\n",
      "Epoch: 1370 \tTraining Loss: 55.702862 \tacc: 86.271809\tValidation Loss: 171.377915, time5.2\n",
      "Epoch: 1371 \tTraining Loss: 55.999388 \tacc: 87.081038\tValidation Loss: 168.518140, time5.2\n",
      "Epoch: 1372 \tTraining Loss: 55.827212 \tacc: 86.960514\tValidation Loss: 169.689220, time5.3\n",
      "Epoch: 1373 \tTraining Loss: 56.028084 \tacc: 86.908861\tValidation Loss: 171.676213, time5.2\n",
      "Epoch: 1374 \tTraining Loss: 55.907560 \tacc: 86.185721\tValidation Loss: 173.538119, time5.2\n",
      "Epoch: 1375 \tTraining Loss: 55.689470 \tacc: 89.485767\tValidation Loss: 162.889697, time5.2\n",
      "Epoch: 1376 \tTraining Loss: 58.101852 \tacc: 86.128329\tValidation Loss: 165.497215, time5.2\n",
      "Epoch: 1377 \tTraining Loss: 55.809994 \tacc: 86.231635\tValidation Loss: 166.741250, time5.3\n",
      "Epoch: 1378 \tTraining Loss: 55.347031 \tacc: 86.380854\tValidation Loss: 166.266244, time5.2\n",
      "Epoch: 1379 \tTraining Loss: 56.355219 \tacc: 86.455464\tValidation Loss: 171.336492, time5.2\n",
      "Epoch: 1380 \tTraining Loss: 55.048592 \tacc: 87.075298\tValidation Loss: 168.371248, time5.2\n",
      "Epoch: 1381 \tTraining Loss: 56.033823 \tacc: 86.553030\tValidation Loss: 170.149667, time5.2\n",
      "Epoch: 1382 \tTraining Loss: 55.852081 \tacc: 86.403811\tValidation Loss: 169.094227, time5.2\n",
      "Epoch: 1383 \tTraining Loss: 55.978344 \tacc: 86.294766\tValidation Loss: 171.412191, time5.2\n",
      "Epoch: 1384 \tTraining Loss: 55.427380 \tacc: 86.702250\tValidation Loss: 170.264178, time5.2\n",
      "Epoch: 1385 \tTraining Loss: 55.649296 \tacc: 86.122590\tValidation Loss: 170.214063, time5.2\n",
      "Epoch: 1386 \tTraining Loss: 55.836777 \tacc: 86.375115\tValidation Loss: 173.242107, time5.2\n",
      "Epoch: 1387 \tTraining Loss: 55.775559 \tacc: 86.002066\tValidation Loss: 170.495971, time5.3\n",
      "Epoch: 1388 \tTraining Loss: 56.072084 \tacc: 86.713728\tValidation Loss: 173.813068, time5.2\n",
      "Epoch: 1389 \tTraining Loss: 55.071549 \tacc: 87.109734\tValidation Loss: 171.720941, time5.2\n",
      "Epoch: 1390 \tTraining Loss: 56.286348 \tacc: 86.707989\tValidation Loss: 163.884328, time5.2\n",
      "Epoch: 1391 \tTraining Loss: 56.070171 \tacc: 86.564509\tValidation Loss: 170.473180, time5.2\n",
      "Epoch: 1392 \tTraining Loss: 55.165289 \tacc: 86.621901\tValidation Loss: 170.571035, time5.3\n",
      "Epoch: 1393 \tTraining Loss: 55.865473 \tacc: 86.989210\tValidation Loss: 171.096691, time5.2\n",
      "Epoch: 1394 \tTraining Loss: 55.903734 \tacc: 86.690771\tValidation Loss: 172.087101, time5.2\n",
      "Epoch: 1395 \tTraining Loss: 55.718167 \tacc: 86.426768\tValidation Loss: 167.586543, time5.2\n",
      "Epoch: 1396 \tTraining Loss: 55.524946 \tacc: 86.455464\tValidation Loss: 172.719868, time5.2\n",
      "Epoch: 1397 \tTraining Loss: 55.203551 \tacc: 86.937557\tValidation Loss: 170.107537, time5.2\n",
      "Epoch: 1398 \tTraining Loss: 55.961126 \tacc: 86.960514\tValidation Loss: 171.470840, time5.2\n",
      "Epoch: 1399 \tTraining Loss: 55.777472 \tacc: 86.868687\tValidation Loss: 172.599672, time5.2\n",
      "Epoch: 1400 \tTraining Loss: 56.014692 \tacc: 86.254591\tValidation Loss: 173.257647, time5.3\n",
      "Epoch: 1401 \tTraining Loss: 55.708601 \tacc: 86.501377\tValidation Loss: 170.639807, time5.2\n",
      "Epoch: 1402 \tTraining Loss: 55.515381 \tacc: 87.792700\tValidation Loss: 170.902566, time5.2\n",
      "Epoch: 1403 \tTraining Loss: 56.142868 \tacc: 87.293388\tValidation Loss: 169.050323, time5.3\n",
      "Epoch: 1404 \tTraining Loss: 55.630165 \tacc: 86.323462\tValidation Loss: 171.463076, time5.2\n",
      "Epoch: 1405 \tTraining Loss: 55.915213 \tacc: 87.040863\tValidation Loss: 171.558488, time5.2\n",
      "Epoch: 1406 \tTraining Loss: 55.773646 \tacc: 86.231635\tValidation Loss: 173.915493, time5.2\n",
      "Epoch: 1407 \tTraining Loss: 56.406872 \tacc: 86.696511\tValidation Loss: 169.549343, time5.3\n",
      "Epoch: 1408 \tTraining Loss: 55.113636 \tacc: 86.438246\tValidation Loss: 174.137812, time5.2\n",
      "Epoch: 1409 \tTraining Loss: 55.496250 \tacc: 88.073921\tValidation Loss: 171.552856, time5.3\n",
      "Epoch: 1410 \tTraining Loss: 56.594353 \tacc: 86.748163\tValidation Loss: 170.991206, time5.2\n",
      "Epoch: 1411 \tTraining Loss: 54.993113 \tacc: 86.363636\tValidation Loss: 172.869581, time5.3\n",
      "Epoch: 1412 \tTraining Loss: 55.714340 \tacc: 86.070937\tValidation Loss: 172.544253, time5.2\n",
      "Epoch: 1413 \tTraining Loss: 55.947735 \tacc: 86.805556\tValidation Loss: 171.857870, time5.2\n",
      "Epoch: 1414 \tTraining Loss: 55.614861 \tacc: 87.098255\tValidation Loss: 170.355064, time5.2\n",
      "Epoch: 1415 \tTraining Loss: 55.844429 \tacc: 86.667815\tValidation Loss: 173.666465, time5.2\n",
      "Epoch: 1416 \tTraining Loss: 55.815733 \tacc: 88.343664\tValidation Loss: 170.058587, time5.3\n",
      "Epoch: 1417 \tTraining Loss: 56.538874 \tacc: 86.415289\tValidation Loss: 170.240317, time5.2\n",
      "Epoch: 1418 \tTraining Loss: 55.040940 \tacc: 86.839991\tValidation Loss: 173.628538, time5.2\n",
      "Epoch: 1419 \tTraining Loss: 55.249464 \tacc: 87.413912\tValidation Loss: 172.405629, time5.2\n",
      "Epoch: 1420 \tTraining Loss: 55.345118 \tacc: 86.489899\tValidation Loss: 175.444412, time5.2\n",
      "Epoch: 1421 \tTraining Loss: 55.128941 \tacc: 86.690771\tValidation Loss: 175.692959, time5.2\n",
      "Epoch: 1422 \tTraining Loss: 55.385292 \tacc: 86.415289\tValidation Loss: 172.546015, time5.2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1423 \tTraining Loss: 55.779385 \tacc: 87.339302\tValidation Loss: 172.959195, time5.2\n",
      "Epoch: 1424 \tTraining Loss: 56.049128 \tacc: 86.524334\tValidation Loss: 172.182490, time5.2\n",
      "Epoch: 1425 \tTraining Loss: 55.750689 \tacc: 86.880165\tValidation Loss: 175.431692, time5.2\n",
      "Epoch: 1426 \tTraining Loss: 55.167202 \tacc: 87.075298\tValidation Loss: 173.105352, time5.3\n",
      "Epoch: 1427 \tTraining Loss: 55.677992 \tacc: 86.128329\tValidation Loss: 175.136297, time5.2\n",
      "Epoch: 1428 \tTraining Loss: 55.389118 \tacc: 87.362259\tValidation Loss: 177.607130, time5.3\n",
      "Epoch: 1429 \tTraining Loss: 55.408249 \tacc: 86.547291\tValidation Loss: 172.103046, time5.2\n",
      "Epoch: 1430 \tTraining Loss: 54.874503 \tacc: 86.478421\tValidation Loss: 172.907753, time5.2\n",
      "Epoch: 1431 \tTraining Loss: 54.752066 \tacc: 86.782599\tValidation Loss: 177.768931, time5.3\n",
      "Epoch: 1432 \tTraining Loss: 55.419728 \tacc: 87.373737\tValidation Loss: 174.171561, time5.2\n",
      "Epoch: 1433 \tTraining Loss: 55.859734 \tacc: 86.776860\tValidation Loss: 171.119180, time5.3\n",
      "Epoch: 1434 \tTraining Loss: 55.574686 \tacc: 88.561754\tValidation Loss: 174.293139, time5.2\n",
      "Epoch: 1435 \tTraining Loss: 56.288261 \tacc: 86.036501\tValidation Loss: 176.605249, time5.3\n",
      "Epoch: 1436 \tTraining Loss: 55.169115 \tacc: 87.086777\tValidation Loss: 170.972057, time5.2\n",
      "Epoch: 1437 \tTraining Loss: 55.375727 \tacc: 86.673554\tValidation Loss: 171.569548, time5.3\n",
      "Epoch: 1438 \tTraining Loss: 55.982170 \tacc: 86.926079\tValidation Loss: 170.952510, time5.2\n",
      "Epoch: 1439 \tTraining Loss: 55.065810 \tacc: 86.897383\tValidation Loss: 175.346428, time5.3\n",
      "Epoch: 1440 \tTraining Loss: 55.083027 \tacc: 86.426768\tValidation Loss: 179.510452, time5.2\n",
      "Epoch: 1441 \tTraining Loss: 55.498163 \tacc: 86.512856\tValidation Loss: 172.753324, time5.3\n",
      "Epoch: 1442 \tTraining Loss: 55.762167 \tacc: 86.111111\tValidation Loss: 173.881926, time5.2\n",
      "Epoch: 1443 \tTraining Loss: 55.486685 \tacc: 86.954775\tValidation Loss: 173.393112, time5.2\n",
      "Epoch: 1444 \tTraining Loss: 56.144781 \tacc: 86.593205\tValidation Loss: 172.774168, time5.3\n",
      "Epoch: 1445 \tTraining Loss: 55.555556 \tacc: 87.098255\tValidation Loss: 178.225726, time5.2\n",
      "Epoch: 1446 \tTraining Loss: 55.339379 \tacc: 87.270432\tValidation Loss: 175.729997, time5.2\n",
      "Epoch: 1447 \tTraining Loss: 56.163912 \tacc: 86.208678\tValidation Loss: 174.337670, time5.2\n",
      "Epoch: 1448 \tTraining Loss: 55.385292 \tacc: 86.593205\tValidation Loss: 174.335341, time5.3\n",
      "Epoch: 1449 \tTraining Loss: 54.738675 \tacc: 86.535813\tValidation Loss: 176.930702, time5.2\n",
      "Epoch: 1450 \tTraining Loss: 55.628252 \tacc: 87.471304\tValidation Loss: 177.193760, time5.3\n",
      "Epoch: 1451 \tTraining Loss: 56.500612 \tacc: 87.213039\tValidation Loss: 174.591360, time5.2\n",
      "Epoch: 1452 \tTraining Loss: 56.228956 \tacc: 86.443985\tValidation Loss: 173.866843, time5.3\n",
      "Epoch: 1453 \tTraining Loss: 56.265305 \tacc: 87.339302\tValidation Loss: 167.989465, time5.2\n",
      "Epoch: 1454 \tTraining Loss: 56.458525 \tacc: 88.257576\tValidation Loss: 171.303139, time5.2\n",
      "Epoch: 1455 \tTraining Loss: 57.250536 \tacc: 86.443985\tValidation Loss: 167.009426, time5.2\n",
      "Epoch: 1456 \tTraining Loss: 57.258188 \tacc: 86.696511\tValidation Loss: 171.874781, time5.2\n",
      "Epoch: 1457 \tTraining Loss: 56.611570 \tacc: 86.311983\tValidation Loss: 168.005710, time5.3\n",
      "Epoch: 1458 \tTraining Loss: 55.930517 \tacc: 86.225895\tValidation Loss: 171.489437, time5.2\n",
      "Epoch: 1459 \tTraining Loss: 56.029997 \tacc: 86.920340\tValidation Loss: 171.924244, time5.3\n",
      "Epoch: 1460 \tTraining Loss: 56.590526 \tacc: 87.419651\tValidation Loss: 169.266579, time5.2\n",
      "Epoch: 1461 \tTraining Loss: 55.855908 \tacc: 87.161387\tValidation Loss: 170.233090, time5.3\n",
      "Epoch: 1462 \tTraining Loss: 56.600092 \tacc: 86.828512\tValidation Loss: 171.949448, time5.2\n",
      "Epoch: 1463 \tTraining Loss: 55.957300 \tacc: 86.851469\tValidation Loss: 171.914075, time5.3\n",
      "Epoch: 1464 \tTraining Loss: 55.907560 \tacc: 86.277548\tValidation Loss: 174.230507, time5.2\n",
      "Epoch: 1465 \tTraining Loss: 56.647919 \tacc: 86.243113\tValidation Loss: 172.715077, time5.3\n",
      "Epoch: 1466 \tTraining Loss: 56.768442 \tacc: 85.915978\tValidation Loss: 172.697638, time5.2\n",
      "Epoch: 1467 \tTraining Loss: 55.976431 \tacc: 86.885904\tValidation Loss: 168.622394, time5.3\n",
      "Epoch: 1468 \tTraining Loss: 55.609122 \tacc: 86.719467\tValidation Loss: 170.075154, time5.2\n",
      "Epoch: 1469 \tTraining Loss: 56.056780 \tacc: 87.213039\tValidation Loss: 169.807736, time5.2\n",
      "Epoch: 1470 \tTraining Loss: 56.680441 \tacc: 86.696511\tValidation Loss: 168.631811, time5.2\n",
      "Epoch: 1471 \tTraining Loss: 56.292088 \tacc: 86.214417\tValidation Loss: 167.682289, time5.2\n",
      "Epoch: 1472 \tTraining Loss: 56.517830 \tacc: 87.132691\tValidation Loss: 166.621885, time5.3\n",
      "Epoch: 1473 \tTraining Loss: 56.892792 \tacc: 86.822773\tValidation Loss: 167.783502, time5.2\n",
      "Epoch: 1474 \tTraining Loss: 56.697658 \tacc: 87.276171\tValidation Loss: 168.275261, time5.3\n",
      "Epoch: 1475 \tTraining Loss: 56.770355 \tacc: 86.231635\tValidation Loss: 169.685211, time5.2\n",
      "Epoch: 1476 \tTraining Loss: 56.749311 \tacc: 87.052342\tValidation Loss: 169.730158, time5.3\n",
      "Epoch: 1477 \tTraining Loss: 56.232782 \tacc: 86.926079\tValidation Loss: 169.038207, time5.2\n",
      "Epoch: 1478 \tTraining Loss: 56.800964 \tacc: 86.914601\tValidation Loss: 169.082768, time5.3\n",
      "Epoch: 1479 \tTraining Loss: 56.567570 \tacc: 86.489899\tValidation Loss: 170.055614, time5.2\n",
      "Epoch: 1480 \tTraining Loss: 56.119911 \tacc: 86.690771\tValidation Loss: 170.765807, time5.2\n",
      "Epoch: 1481 \tTraining Loss: 56.336088 \tacc: 86.788338\tValidation Loss: 170.772043, time5.2\n",
      "Epoch: 1482 \tTraining Loss: 57.072620 \tacc: 87.563131\tValidation Loss: 168.061983, time5.2\n",
      "Epoch: 1483 \tTraining Loss: 56.492960 \tacc: 86.443985\tValidation Loss: 169.044833, time5.2\n",
      "Epoch: 1484 \tTraining Loss: 56.316957 \tacc: 86.254591\tValidation Loss: 171.403743, time5.2\n",
      "Epoch: 1485 \tTraining Loss: 55.875038 \tacc: 86.725207\tValidation Loss: 169.470754, time5.2\n",
      "Epoch: 1486 \tTraining Loss: 56.829660 \tacc: 86.392332\tValidation Loss: 168.163922, time5.2\n",
      "Epoch: 1487 \tTraining Loss: 56.603918 \tacc: 86.719467\tValidation Loss: 166.422395, time5.3\n",
      "Epoch: 1488 \tTraining Loss: 56.546526 \tacc: 86.794077\tValidation Loss: 171.505235, time5.2\n",
      "Epoch: 1489 \tTraining Loss: 56.783747 \tacc: 86.237374\tValidation Loss: 173.058518, time5.2\n",
      "Epoch: 1490 \tTraining Loss: 56.091215 \tacc: 86.168503\tValidation Loss: 169.522749, time5.2\n",
      "Epoch: 1491 \tTraining Loss: 55.974518 \tacc: 87.626263\tValidation Loss: 169.194938, time5.2\n",
      "Epoch: 1492 \tTraining Loss: 56.778007 \tacc: 86.541552\tValidation Loss: 174.549346, time5.2\n",
      "Epoch: 1493 \tTraining Loss: 55.890343 \tacc: 86.512856\tValidation Loss: 171.685660, time5.2\n",
      "Epoch: 1494 \tTraining Loss: 55.492424 \tacc: 86.587466\tValidation Loss: 168.732713, time5.2\n",
      "Epoch: 1495 \tTraining Loss: 56.485308 \tacc: 86.363636\tValidation Loss: 173.169905, time5.2\n",
      "Epoch: 1496 \tTraining Loss: 56.626875 \tacc: 86.174242\tValidation Loss: 171.903901, time5.3\n",
      "Epoch: 1497 \tTraining Loss: 56.829660 \tacc: 85.801194\tValidation Loss: 172.540662, time5.2\n",
      "Validation loss decreased (85.824151 --> 85.801194).  Saving model ...\n",
      "Epoch: 1498 \tTraining Loss: 56.236609 \tacc: 87.356520\tValidation Loss: 168.165143, time5.2\n",
      "Epoch: 1499 \tTraining Loss: 56.016605 \tacc: 87.098255\tValidation Loss: 173.225259, time5.3\n",
      "Epoch: 1500 \tTraining Loss: 56.644092 \tacc: 86.926079\tValidation Loss: 171.689919, time5.3\n",
      "Epoch: 1501 \tTraining Loss: 55.681818 \tacc: 86.260331\tValidation Loss: 176.082154, time5.3\n",
      "Epoch: 1502 \tTraining Loss: 56.003214 \tacc: 86.438246\tValidation Loss: 172.911549, time5.2\n",
      "Epoch: 1503 \tTraining Loss: 56.154346 \tacc: 86.587466\tValidation Loss: 168.614247, time5.3\n",
      "Epoch: 1504 \tTraining Loss: 56.353306 \tacc: 86.047980\tValidation Loss: 172.825178, time5.3\n",
      "Epoch: 1505 \tTraining Loss: 56.404959 \tacc: 86.885904\tValidation Loss: 169.723751, time5.2\n",
      "Epoch: 1506 \tTraining Loss: 56.655571 \tacc: 86.294766\tValidation Loss: 172.502139, time5.2\n",
      "Epoch: 1507 \tTraining Loss: 55.932430 \tacc: 86.484160\tValidation Loss: 176.330229, time5.2\n",
      "Epoch: 1508 \tTraining Loss: 56.031910 \tacc: 86.398072\tValidation Loss: 174.809319, time5.3\n",
      "Epoch: 1509 \tTraining Loss: 55.943909 \tacc: 87.063820\tValidation Loss: 170.646705, time5.2\n",
      "Epoch: 1510 \tTraining Loss: 56.651745 \tacc: 88.165748\tValidation Loss: 173.292374, time5.3\n",
      "Epoch: 1511 \tTraining Loss: 57.975589 \tacc: 86.742424\tValidation Loss: 164.718993, time5.3\n",
      "Epoch: 1512 \tTraining Loss: 57.623584 \tacc: 86.598944\tValidation Loss: 169.520564, time5.2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1513 \tTraining Loss: 56.561830 \tacc: 86.575987\tValidation Loss: 172.165239, time5.2\n",
      "Epoch: 1514 \tTraining Loss: 55.633991 \tacc: 86.730946\tValidation Loss: 172.056840, time5.2\n",
      "Epoch: 1515 \tTraining Loss: 56.026171 \tacc: 86.300505\tValidation Loss: 173.196204, time5.2\n",
      "Epoch: 1516 \tTraining Loss: 56.506351 \tacc: 86.289027\tValidation Loss: 167.937978, time5.2\n",
      "Epoch: 1517 \tTraining Loss: 55.832951 \tacc: 86.719467\tValidation Loss: 174.581384, time5.2\n",
      "Epoch: 1518 \tTraining Loss: 56.238522 \tacc: 86.753903\tValidation Loss: 169.512771, time5.2\n",
      "Epoch: 1519 \tTraining Loss: 57.403581 \tacc: 87.327824\tValidation Loss: 170.495556, time5.2\n",
      "Epoch: 1520 \tTraining Loss: 55.876951 \tacc: 86.627640\tValidation Loss: 172.281153, time5.2\n",
      "Epoch: 1521 \tTraining Loss: 56.267218 \tacc: 86.610422\tValidation Loss: 171.352633, time5.3\n",
      "Epoch: 1522 \tTraining Loss: 55.624426 \tacc: 89.290634\tValidation Loss: 170.223865, time5.3\n",
      "Epoch: 1523 \tTraining Loss: 56.632614 \tacc: 86.581726\tValidation Loss: 168.509360, time5.3\n",
      "Epoch: 1524 \tTraining Loss: 56.965488 \tacc: 86.294766\tValidation Loss: 171.533050, time5.2\n",
      "Epoch: 1525 \tTraining Loss: 55.647383 \tacc: 86.053719\tValidation Loss: 171.835365, time5.3\n",
      "Epoch: 1526 \tTraining Loss: 55.748776 \tacc: 86.926079\tValidation Loss: 167.779909, time5.2\n",
      "Epoch: 1527 \tTraining Loss: 56.351393 \tacc: 86.553030\tValidation Loss: 174.484039, time5.2\n",
      "Epoch: 1528 \tTraining Loss: 55.836777 \tacc: 86.323462\tValidation Loss: 171.927306, time5.3\n",
      "Epoch: 1529 \tTraining Loss: 56.139042 \tacc: 86.598944\tValidation Loss: 172.483006, time5.2\n",
      "Epoch: 1530 \tTraining Loss: 55.997475 \tacc: 86.581726\tValidation Loss: 175.281567, time5.2\n",
      "Epoch: 1531 \tTraining Loss: 56.510178 \tacc: 86.116850\tValidation Loss: 174.562907, time5.2\n",
      "Epoch: 1532 \tTraining Loss: 56.303566 \tacc: 86.937557\tValidation Loss: 170.416887, time5.2\n",
      "Epoch: 1533 \tTraining Loss: 56.230869 \tacc: 86.685032\tValidation Loss: 171.390399, time5.3\n",
      "Epoch: 1534 \tTraining Loss: 55.735384 \tacc: 85.812672\tValidation Loss: 175.982075, time5.2\n",
      "Epoch: 1535 \tTraining Loss: 55.463728 \tacc: 86.742424\tValidation Loss: 170.347729, time5.2\n",
      "Epoch: 1536 \tTraining Loss: 56.225130 \tacc: 86.782599\tValidation Loss: 174.857575, time5.2\n",
      "Epoch: 1537 \tTraining Loss: 56.297827 \tacc: 88.079660\tValidation Loss: 169.157882, time5.2\n",
      "Epoch: 1538 \tTraining Loss: 56.447046 \tacc: 86.162764\tValidation Loss: 174.052579, time5.2\n",
      "Epoch: 1539 \tTraining Loss: 56.316957 \tacc: 86.237374\tValidation Loss: 176.719513, time5.3\n",
      "Epoch: 1540 \tTraining Loss: 55.691384 \tacc: 86.409550\tValidation Loss: 176.849304, time5.2\n",
      "Epoch: 1541 \tTraining Loss: 55.507729 \tacc: 88.217401\tValidation Loss: 178.134735, time5.2\n",
      "Epoch: 1542 \tTraining Loss: 56.456612 \tacc: 86.495638\tValidation Loss: 175.752314, time5.3\n",
      "Epoch: 1543 \tTraining Loss: 55.997475 \tacc: 86.122590\tValidation Loss: 174.635590, time5.2\n",
      "Epoch: 1544 \tTraining Loss: 55.716253 \tacc: 87.023646\tValidation Loss: 175.666594, time5.3\n",
      "Epoch: 1545 \tTraining Loss: 56.271044 \tacc: 86.598944\tValidation Loss: 175.691643, time5.2\n",
      "Epoch: 1546 \tTraining Loss: 56.112259 \tacc: 86.438246\tValidation Loss: 176.205176, time5.2\n",
      "Epoch: 1547 \tTraining Loss: 56.087389 \tacc: 87.052342\tValidation Loss: 172.731967, time5.3\n",
      "Epoch: 1548 \tTraining Loss: 55.555556 \tacc: 86.771120\tValidation Loss: 178.883754, time5.3\n",
      "Epoch: 1549 \tTraining Loss: 55.729645 \tacc: 86.742424\tValidation Loss: 179.480284, time5.2\n",
      "Epoch: 1550 \tTraining Loss: 56.073998 \tacc: 86.409550\tValidation Loss: 172.240820, time5.2\n",
      "Epoch: 1551 \tTraining Loss: 55.926691 \tacc: 88.286272\tValidation Loss: 175.897767, time5.3\n",
      "Epoch: 1552 \tTraining Loss: 57.022880 \tacc: 86.375115\tValidation Loss: 176.800170, time5.3\n",
      "Epoch: 1553 \tTraining Loss: 56.142868 \tacc: 87.270432\tValidation Loss: 176.073766, time5.3\n",
      "Epoch: 1554 \tTraining Loss: 56.169651 \tacc: 88.274793\tValidation Loss: 173.133409, time5.2\n",
      "Epoch: 1555 \tTraining Loss: 55.876951 \tacc: 87.419651\tValidation Loss: 177.400652, time5.2\n",
      "Epoch: 1556 \tTraining Loss: 55.953474 \tacc: 86.604683\tValidation Loss: 172.867318, time5.2\n",
      "Epoch: 1557 \tTraining Loss: 56.391567 \tacc: 88.332185\tValidation Loss: 170.913760, time5.2\n",
      "Epoch: 1558 \tTraining Loss: 57.066881 \tacc: 87.540174\tValidation Loss: 173.854018, time5.2\n",
      "Epoch: 1559 \tTraining Loss: 55.982170 \tacc: 86.753903\tValidation Loss: 176.504876, time5.2\n",
      "Epoch: 1560 \tTraining Loss: 56.253826 \tacc: 86.673554\tValidation Loss: 171.321306, time5.3\n",
      "Epoch: 1561 \tTraining Loss: 55.500077 \tacc: 87.568871\tValidation Loss: 176.928090, time5.2\n",
      "Epoch: 1562 \tTraining Loss: 56.225130 \tacc: 86.771120\tValidation Loss: 166.123432, time5.3\n",
      "Epoch: 1563 \tTraining Loss: 56.611570 \tacc: 88.022268\tValidation Loss: 175.114638, time5.2\n",
      "Epoch: 1564 \tTraining Loss: 56.887052 \tacc: 86.179982\tValidation Loss: 172.562026, time5.3\n",
      "Epoch: 1565 \tTraining Loss: 55.955387 \tacc: 87.586088\tValidation Loss: 174.862923, time5.3\n",
      "Epoch: 1566 \tTraining Loss: 55.743036 \tacc: 86.598944\tValidation Loss: 176.853045, time5.3\n",
      "Epoch: 1567 \tTraining Loss: 55.255204 \tacc: 86.421028\tValidation Loss: 178.702940, time5.2\n",
      "Epoch: 1568 \tTraining Loss: 56.374350 \tacc: 86.179982\tValidation Loss: 175.684260, time5.3\n",
      "Epoch: 1569 \tTraining Loss: 55.490511 \tacc: 86.363636\tValidation Loss: 181.947139, time5.3\n",
      "Epoch: 1570 \tTraining Loss: 56.160086 \tacc: 86.472681\tValidation Loss: 177.591726, time5.2\n",
      "Epoch: 1571 \tTraining Loss: 56.022345 \tacc: 86.306244\tValidation Loss: 176.057603, time5.2\n",
      "Epoch: 1572 \tTraining Loss: 55.536425 \tacc: 85.967631\tValidation Loss: 179.354585, time5.2\n",
      "Epoch: 1573 \tTraining Loss: 55.329813 \tacc: 86.530073\tValidation Loss: 176.227419, time5.3\n",
      "Epoch: 1574 \tTraining Loss: 55.764080 \tacc: 86.989210\tValidation Loss: 177.415689, time5.2\n",
      "Epoch: 1575 \tTraining Loss: 56.028084 \tacc: 86.369376\tValidation Loss: 177.194241, time5.3\n",
      "Epoch: 1576 \tTraining Loss: 56.269131 \tacc: 88.395317\tValidation Loss: 172.304436, time5.2\n",
      "Epoch: 1577 \tTraining Loss: 56.749311 \tacc: 86.839991\tValidation Loss: 174.090203, time5.2\n",
      "Epoch: 1578 \tTraining Loss: 56.573309 \tacc: 86.403811\tValidation Loss: 174.052443, time5.2\n",
      "Epoch: 1579 \tTraining Loss: 56.016605 \tacc: 86.570248\tValidation Loss: 177.401722, time5.3\n",
      "Epoch: 1580 \tTraining Loss: 55.597643 \tacc: 87.224518\tValidation Loss: 179.808241, time5.2\n",
      "Epoch: 1581 \tTraining Loss: 55.666514 \tacc: 86.667815\tValidation Loss: 177.200778, time5.2\n",
      "Epoch: 1582 \tTraining Loss: 56.073998 \tacc: 86.478421\tValidation Loss: 172.664700, time5.2\n",
      "Epoch: 1583 \tTraining Loss: 56.404959 \tacc: 87.568871\tValidation Loss: 171.270118, time5.2\n",
      "Epoch: 1584 \tTraining Loss: 55.838690 \tacc: 86.966253\tValidation Loss: 173.690635, time5.2\n",
      "Epoch: 1585 \tTraining Loss: 55.838690 \tacc: 87.603306\tValidation Loss: 178.363920, time5.3\n",
      "Epoch: 1586 \tTraining Loss: 57.141491 \tacc: 86.518595\tValidation Loss: 170.116358, time5.2\n",
      "Epoch: 1587 \tTraining Loss: 55.645470 \tacc: 86.409550\tValidation Loss: 177.396188, time5.3\n",
      "Epoch: 1588 \tTraining Loss: 54.882155 \tacc: 87.040863\tValidation Loss: 179.079758, time5.3\n",
      "Epoch: 1589 \tTraining Loss: 55.978344 \tacc: 86.828512\tValidation Loss: 173.588728, time5.3\n",
      "Epoch: 1590 \tTraining Loss: 55.984083 \tacc: 86.340680\tValidation Loss: 174.984131, time5.3\n",
      "Epoch: 1591 \tTraining Loss: 56.309305 \tacc: 86.662075\tValidation Loss: 175.701089, time5.2\n",
      "Epoch: 1592 \tTraining Loss: 55.796602 \tacc: 87.023646\tValidation Loss: 178.043601, time5.3\n",
      "Epoch: 1593 \tTraining Loss: 55.702862 \tacc: 87.402433\tValidation Loss: 175.132062, time5.2\n",
      "Epoch: 1594 \tTraining Loss: 56.001301 \tacc: 86.782599\tValidation Loss: 171.519939, time5.3\n",
      "Epoch: 1595 \tTraining Loss: 56.066345 \tacc: 86.587466\tValidation Loss: 174.951081, time5.2\n",
      "Epoch: 1596 \tTraining Loss: 55.505816 \tacc: 86.484160\tValidation Loss: 177.391861, time5.2\n",
      "Epoch: 1597 \tTraining Loss: 55.723906 \tacc: 87.333563\tValidation Loss: 176.782988, time5.2\n",
      "Epoch: 1598 \tTraining Loss: 55.647383 \tacc: 87.815657\tValidation Loss: 175.485636, time5.2\n",
      "Epoch: 1599 \tTraining Loss: 56.372436 \tacc: 86.403811\tValidation Loss: 174.501221, time5.2\n",
      "Epoch: 1600 \tTraining Loss: 55.475207 \tacc: 86.495638\tValidation Loss: 179.021205, time5.2\n",
      "Epoch: 1601 \tTraining Loss: 55.517294 \tacc: 86.604683\tValidation Loss: 178.018069, time5.2\n",
      "Epoch: 1602 \tTraining Loss: 56.255739 \tacc: 86.512856\tValidation Loss: 179.942330, time5.2\n",
      "Epoch: 1603 \tTraining Loss: 55.712427 \tacc: 87.230257\tValidation Loss: 181.849697, time5.2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1604 \tTraining Loss: 55.920952 \tacc: 86.742424\tValidation Loss: 181.417995, time5.2\n",
      "Epoch: 1605 \tTraining Loss: 55.997475 \tacc: 86.621901\tValidation Loss: 175.377583, time5.2\n",
      "Epoch: 1606 \tTraining Loss: 55.785124 \tacc: 86.409550\tValidation Loss: 178.342391, time5.2\n",
      "Epoch: 1607 \tTraining Loss: 55.186333 \tacc: 86.851469\tValidation Loss: 175.442345, time5.3\n",
      "Epoch: 1608 \tTraining Loss: 56.676615 \tacc: 87.029385\tValidation Loss: 179.012532, time5.2\n",
      "Epoch: 1609 \tTraining Loss: 55.301117 \tacc: 87.454086\tValidation Loss: 181.437423, time5.2\n",
      "Epoch: 1610 \tTraining Loss: 56.508264 \tacc: 86.553030\tValidation Loss: 177.798170, time5.2\n",
      "Epoch: 1611 \tTraining Loss: 56.718702 \tacc: 86.426768\tValidation Loss: 175.811447, time5.3\n",
      "Epoch: 1612 \tTraining Loss: 55.465641 \tacc: 86.874426\tValidation Loss: 174.543477, time5.2\n",
      "Epoch: 1613 \tTraining Loss: 55.809994 \tacc: 87.798439\tValidation Loss: 176.612340, time5.3\n",
      "Epoch: 1614 \tTraining Loss: 55.369988 \tacc: 86.696511\tValidation Loss: 179.446646, time5.3\n",
      "Epoch: 1615 \tTraining Loss: 55.806168 \tacc: 86.914601\tValidation Loss: 176.373461, time5.2\n",
      "Epoch: 1616 \tTraining Loss: 55.741123 \tacc: 87.264692\tValidation Loss: 176.981770, time5.2\n",
      "Epoch: 1617 \tTraining Loss: 56.588613 \tacc: 86.845730\tValidation Loss: 170.935698, time5.2\n",
      "Epoch: 1618 \tTraining Loss: 56.079737 \tacc: 88.808540\tValidation Loss: 176.589020, time5.2\n",
      "Epoch: 1619 \tTraining Loss: 57.227579 \tacc: 86.357897\tValidation Loss: 174.760349, time5.3\n",
      "Epoch: 1620 \tTraining Loss: 55.540251 \tacc: 86.914601\tValidation Loss: 176.168752, time5.2\n",
      "Epoch: 1621 \tTraining Loss: 55.553642 \tacc: 86.099633\tValidation Loss: 177.217752, time5.2\n",
      "Epoch: 1622 \tTraining Loss: 55.767906 \tacc: 86.673554\tValidation Loss: 176.411273, time5.3\n",
      "Epoch: 1623 \tTraining Loss: 55.746863 \tacc: 86.047980\tValidation Loss: 181.176044, time5.3\n",
      "Epoch: 1624 \tTraining Loss: 54.983548 \tacc: 86.443985\tValidation Loss: 179.546577, time5.2\n",
      "Epoch: 1625 \tTraining Loss: 54.918503 \tacc: 86.334940\tValidation Loss: 180.357765, time5.2\n",
      "Epoch: 1626 \tTraining Loss: 55.289639 \tacc: 86.415289\tValidation Loss: 179.600131, time5.3\n",
      "Epoch: 1627 \tTraining Loss: 55.947735 \tacc: 86.111111\tValidation Loss: 178.728491, time5.3\n",
      "Epoch: 1628 \tTraining Loss: 55.368075 \tacc: 87.327824\tValidation Loss: 175.697363, time5.3\n",
      "Epoch: 1629 \tTraining Loss: 55.941996 \tacc: 87.821396\tValidation Loss: 177.534728, time5.3\n",
      "Epoch: 1630 \tTraining Loss: 56.615396 \tacc: 86.489899\tValidation Loss: 177.693788, time5.3\n",
      "Epoch: 1631 \tTraining Loss: 55.656948 \tacc: 86.931818\tValidation Loss: 179.601684, time5.2\n",
      "Epoch: 1632 \tTraining Loss: 55.289639 \tacc: 86.461203\tValidation Loss: 181.897733, time5.2\n",
      "Epoch: 1633 \tTraining Loss: 55.521120 \tacc: 86.455464\tValidation Loss: 177.544060, time5.2\n",
      "Epoch: 1634 \tTraining Loss: 55.697123 \tacc: 86.656336\tValidation Loss: 180.635801, time5.3\n",
      "Epoch: 1635 \tTraining Loss: 56.093128 \tacc: 86.398072\tValidation Loss: 181.384775, time5.3\n",
      "Epoch: 1636 \tTraining Loss: 55.163376 \tacc: 86.455464\tValidation Loss: 177.656870, time5.2\n",
      "Epoch: 1637 \tTraining Loss: 56.163912 \tacc: 86.300505\tValidation Loss: 180.165974, time5.3\n",
      "Epoch: 1638 \tTraining Loss: 55.299204 \tacc: 86.340680\tValidation Loss: 180.128226, time5.3\n",
      "Epoch: 1639 \tTraining Loss: 55.524946 \tacc: 86.065197\tValidation Loss: 181.559866, time5.3\n",
      "Epoch: 1640 \tTraining Loss: 55.471380 \tacc: 85.841368\tValidation Loss: 179.830004, time5.3\n",
      "Epoch: 1641 \tTraining Loss: 55.125115 \tacc: 86.811295\tValidation Loss: 177.460323, time5.3\n",
      "Epoch: 1642 \tTraining Loss: 55.771732 \tacc: 86.047980\tValidation Loss: 180.533456, time5.3\n",
      "Epoch: 1643 \tTraining Loss: 55.643557 \tacc: 86.277548\tValidation Loss: 181.751615, time5.3\n",
      "Epoch: 1644 \tTraining Loss: 55.477120 \tacc: 88.561754\tValidation Loss: 176.586422, time5.3\n",
      "Epoch: 1645 \tTraining Loss: 56.265305 \tacc: 86.702250\tValidation Loss: 180.859600, time5.2\n",
      "Epoch: 1646 \tTraining Loss: 55.959213 \tacc: 86.707989\tValidation Loss: 179.458667, time5.3\n",
      "Epoch: 1647 \tTraining Loss: 54.916590 \tacc: 87.144169\tValidation Loss: 177.516312, time5.2\n",
      "Epoch: 1648 \tTraining Loss: 55.276247 \tacc: 86.294766\tValidation Loss: 180.559019, time5.3\n",
      "Epoch: 1649 \tTraining Loss: 55.369988 \tacc: 86.788338\tValidation Loss: 179.316927, time5.2\n",
      "Epoch: 1650 \tTraining Loss: 56.292088 \tacc: 87.448347\tValidation Loss: 178.999207, time5.2\n",
      "Epoch: 1651 \tTraining Loss: 55.792776 \tacc: 87.098255\tValidation Loss: 172.480020, time5.2\n",
      "Epoch: 1652 \tTraining Loss: 56.427916 \tacc: 87.235996\tValidation Loss: 183.010704, time5.2\n",
      "Epoch: 1653 \tTraining Loss: 55.672253 \tacc: 87.522957\tValidation Loss: 177.699535, time5.2\n",
      "Epoch: 1654 \tTraining Loss: 56.316957 \tacc: 87.132691\tValidation Loss: 184.780264, time5.2\n",
      "Epoch: 1655 \tTraining Loss: 54.776936 \tacc: 87.281910\tValidation Loss: 179.763496, time5.2\n",
      "Epoch: 1656 \tTraining Loss: 55.136593 \tacc: 86.524334\tValidation Loss: 183.262366, time5.2\n",
      "Epoch: 1657 \tTraining Loss: 55.660774 \tacc: 86.782599\tValidation Loss: 180.690360, time5.2\n",
      "Epoch: 1658 \tTraining Loss: 55.662687 \tacc: 86.472681\tValidation Loss: 181.009835, time5.3\n",
      "Epoch: 1659 \tTraining Loss: 54.465106 \tacc: 86.874426\tValidation Loss: 179.211298, time5.2\n",
      "Epoch: 1660 \tTraining Loss: 55.025635 \tacc: 87.637741\tValidation Loss: 182.393472, time5.2\n",
      "Epoch: 1661 \tTraining Loss: 54.868763 \tacc: 86.960514\tValidation Loss: 182.242088, time5.2\n",
      "Epoch: 1662 \tTraining Loss: 55.589991 \tacc: 87.660698\tValidation Loss: 178.096301, time5.2\n",
      "Epoch: 1663 \tTraining Loss: 55.371901 \tacc: 86.541552\tValidation Loss: 182.451431, time5.2\n",
      "Epoch: 1664 \tTraining Loss: 55.626339 \tacc: 85.847107\tValidation Loss: 180.842999, time5.2\n",
      "Epoch: 1665 \tTraining Loss: 54.958678 \tacc: 87.677916\tValidation Loss: 180.721765, time5.3\n",
      "Epoch: 1666 \tTraining Loss: 56.133303 \tacc: 87.517218\tValidation Loss: 182.909559, time5.2\n",
      "Epoch: 1667 \tTraining Loss: 56.538874 \tacc: 86.742424\tValidation Loss: 179.701161, time5.2\n",
      "Epoch: 1668 \tTraining Loss: 56.112259 \tacc: 86.266070\tValidation Loss: 182.907084, time5.2\n",
      "Epoch: 1669 \tTraining Loss: 55.060070 \tacc: 86.943297\tValidation Loss: 182.489858, time5.3\n",
      "Epoch: 1670 \tTraining Loss: 55.588078 \tacc: 87.718090\tValidation Loss: 180.026597, time5.2\n",
      "Epoch: 1671 \tTraining Loss: 56.259565 \tacc: 87.012167\tValidation Loss: 183.323984, time5.2\n",
      "Epoch: 1672 \tTraining Loss: 55.503903 \tacc: 86.633379\tValidation Loss: 180.386709, time5.2\n",
      "Epoch: 1673 \tTraining Loss: 55.172942 \tacc: 87.459826\tValidation Loss: 177.801517, time5.2\n",
      "Epoch: 1674 \tTraining Loss: 55.492424 \tacc: 86.019284\tValidation Loss: 185.219442, time5.2\n",
      "Epoch: 1675 \tTraining Loss: 55.423554 \tacc: 86.507117\tValidation Loss: 182.464750, time5.2\n",
      "Epoch: 1676 \tTraining Loss: 55.190159 \tacc: 86.134068\tValidation Loss: 182.608270, time5.2\n",
      "Epoch: 1677 \tTraining Loss: 55.318335 \tacc: 86.593205\tValidation Loss: 178.789240, time5.2\n",
      "Epoch: 1678 \tTraining Loss: 55.574686 \tacc: 86.707989\tValidation Loss: 179.172936, time5.3\n",
      "Epoch: 1679 \tTraining Loss: 55.329813 \tacc: 86.386593\tValidation Loss: 182.910260, time5.2\n",
      "Epoch: 1680 \tTraining Loss: 56.129477 \tacc: 86.346419\tValidation Loss: 182.043329, time5.3\n",
      "Epoch: 1681 \tTraining Loss: 54.945286 \tacc: 89.233242\tValidation Loss: 182.886528, time5.2\n",
      "Epoch: 1682 \tTraining Loss: 56.556091 \tacc: 86.059458\tValidation Loss: 185.641996, time5.3\n",
      "Epoch: 1683 \tTraining Loss: 55.848255 \tacc: 86.857208\tValidation Loss: 178.947430, time5.2\n",
      "Epoch: 1684 \tTraining Loss: 54.962504 \tacc: 86.231635\tValidation Loss: 182.039687, time5.2\n",
      "Epoch: 1685 \tTraining Loss: 55.293465 \tacc: 87.258953\tValidation Loss: 179.241957, time5.3\n",
      "Epoch: 1686 \tTraining Loss: 56.198347 \tacc: 86.868687\tValidation Loss: 180.199297, time5.3\n",
      "Epoch: 1687 \tTraining Loss: 55.387205 \tacc: 86.134068\tValidation Loss: 181.435520, time5.3\n",
      "Epoch: 1688 \tTraining Loss: 55.586165 \tacc: 87.597567\tValidation Loss: 180.015491, time5.3\n",
      "Epoch: 1689 \tTraining Loss: 56.952097 \tacc: 86.897383\tValidation Loss: 181.502195, time5.2\n",
      "Epoch: 1690 \tTraining Loss: 55.276247 \tacc: 88.039486\tValidation Loss: 178.074379, time5.2\n",
      "Epoch: 1691 \tTraining Loss: 55.593817 \tacc: 87.075298\tValidation Loss: 177.931728, time5.3\n",
      "Epoch: 1692 \tTraining Loss: 55.035200 \tacc: 86.334940\tValidation Loss: 183.145132, time5.2\n",
      "Epoch: 1693 \tTraining Loss: 55.094506 \tacc: 86.725207\tValidation Loss: 181.198908, time5.2\n",
      "Epoch: 1694 \tTraining Loss: 54.983548 \tacc: 87.035124\tValidation Loss: 182.343293, time5.2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1695 \tTraining Loss: 56.186869 \tacc: 86.375115\tValidation Loss: 182.638878, time5.3\n",
      "Epoch: 1696 \tTraining Loss: 55.155724 \tacc: 87.287649\tValidation Loss: 178.403360, time5.3\n",
      "Epoch: 1697 \tTraining Loss: 55.350857 \tacc: 86.524334\tValidation Loss: 179.814874, time5.2\n",
      "Epoch: 1698 \tTraining Loss: 55.169115 \tacc: 86.220156\tValidation Loss: 180.116716, time5.2\n",
      "Epoch: 1699 \tTraining Loss: 55.345118 \tacc: 86.633379\tValidation Loss: 185.116695, time5.3\n",
      "Epoch: 1700 \tTraining Loss: 55.236073 \tacc: 86.685032\tValidation Loss: 180.754786, time5.2\n",
      "Epoch: 1701 \tTraining Loss: 54.828589 \tacc: 87.700872\tValidation Loss: 182.518570, time5.2\n",
      "Epoch: 1702 \tTraining Loss: 55.838690 \tacc: 87.000689\tValidation Loss: 182.348246, time5.2\n",
      "Epoch: 1703 \tTraining Loss: 54.776936 \tacc: 85.956152\tValidation Loss: 183.075662, time5.2\n",
      "Epoch: 1704 \tTraining Loss: 55.853994 \tacc: 86.495638\tValidation Loss: 181.014048, time5.3\n",
      "Epoch: 1705 \tTraining Loss: 54.782675 \tacc: 86.845730\tValidation Loss: 179.066676, time5.3\n",
      "Epoch: 1706 \tTraining Loss: 55.534512 \tacc: 86.983471\tValidation Loss: 182.225683, time5.3\n",
      "Epoch: 1707 \tTraining Loss: 55.530686 \tacc: 86.817034\tValidation Loss: 177.176119, time5.3\n",
      "Epoch: 1708 \tTraining Loss: 54.421105 \tacc: 86.489899\tValidation Loss: 186.042046, time5.3\n",
      "Epoch: 1709 \tTraining Loss: 55.645470 \tacc: 86.277548\tValidation Loss: 179.744292, time5.3\n",
      "Epoch: 1710 \tTraining Loss: 55.421641 \tacc: 86.300505\tValidation Loss: 178.452900, time5.3\n",
      "Epoch: 1711 \tTraining Loss: 54.993113 \tacc: 86.168503\tValidation Loss: 183.453608, time5.2\n",
      "Epoch: 1712 \tTraining Loss: 55.287726 \tacc: 86.042241\tValidation Loss: 182.513590, time5.3\n",
      "Epoch: 1713 \tTraining Loss: 55.549816 \tacc: 86.369376\tValidation Loss: 180.821577, time5.3\n",
      "Epoch: 1714 \tTraining Loss: 55.394858 \tacc: 86.507117\tValidation Loss: 188.286181, time5.2\n",
      "Epoch: 1715 \tTraining Loss: 55.182507 \tacc: 86.633379\tValidation Loss: 189.013518, time5.2\n",
      "Epoch: 1716 \tTraining Loss: 55.360422 \tacc: 86.656336\tValidation Loss: 180.119541, time5.3\n",
      "Epoch: 1717 \tTraining Loss: 56.533134 \tacc: 86.834252\tValidation Loss: 178.551077, time5.3\n",
      "Epoch: 1718 \tTraining Loss: 55.588078 \tacc: 87.304867\tValidation Loss: 184.002922, time5.2\n",
      "Epoch: 1719 \tTraining Loss: 56.077824 \tacc: 86.788338\tValidation Loss: 178.491893, time5.2\n",
      "Epoch: 1720 \tTraining Loss: 54.899373 \tacc: 86.558770\tValidation Loss: 185.449370, time5.2\n",
      "Epoch: 1721 \tTraining Loss: 55.222681 \tacc: 86.019284\tValidation Loss: 182.959330, time5.2\n",
      "Epoch: 1722 \tTraining Loss: 55.379553 \tacc: 86.088154\tValidation Loss: 180.876698, time5.2\n",
      "Epoch: 1723 \tTraining Loss: 55.262856 \tacc: 86.484160\tValidation Loss: 181.894455, time5.3\n",
      "Epoch: 1724 \tTraining Loss: 55.419728 \tacc: 87.167126\tValidation Loss: 183.720823, time5.3\n",
      "Epoch: 1725 \tTraining Loss: 54.606673 \tacc: 86.719467\tValidation Loss: 185.691599, time5.2\n",
      "Epoch: 1726 \tTraining Loss: 55.138506 \tacc: 86.478421\tValidation Loss: 185.399090, time5.3\n",
      "Epoch: 1727 \tTraining Loss: 54.899373 \tacc: 86.197199\tValidation Loss: 185.929263, time5.2\n",
      "Epoch: 1728 \tTraining Loss: 56.073998 \tacc: 87.798439\tValidation Loss: 180.356500, time5.2\n",
      "Epoch: 1729 \tTraining Loss: 56.054867 \tacc: 86.759642\tValidation Loss: 183.756357, time5.2\n",
      "Epoch: 1730 \tTraining Loss: 55.276247 \tacc: 86.455464\tValidation Loss: 185.401805, time5.3\n",
      "Epoch: 1731 \tTraining Loss: 55.037114 \tacc: 86.570248\tValidation Loss: 183.362902, time5.3\n",
      "Epoch: 1732 \tTraining Loss: 55.079201 \tacc: 87.006428\tValidation Loss: 182.263293, time5.2\n",
      "Epoch: 1733 \tTraining Loss: 55.023722 \tacc: 86.799816\tValidation Loss: 179.507510, time5.2\n",
      "Epoch: 1734 \tTraining Loss: 54.734848 \tacc: 86.415289\tValidation Loss: 181.463322, time5.3\n",
      "Epoch: 1735 \tTraining Loss: 55.653122 \tacc: 86.949036\tValidation Loss: 182.115399, time5.3\n",
      "Epoch: 1736 \tTraining Loss: 56.802877 \tacc: 86.025023\tValidation Loss: 179.765728, time5.3\n",
      "Epoch: 1737 \tTraining Loss: 55.775559 \tacc: 87.190083\tValidation Loss: 178.494498, time5.3\n",
      "Epoch: 1738 \tTraining Loss: 55.609122 \tacc: 86.874426\tValidation Loss: 178.975564, time5.3\n",
      "Epoch: 1739 \tTraining Loss: 55.482859 \tacc: 86.403811\tValidation Loss: 184.727379, time5.3\n",
      "Epoch: 1740 \tTraining Loss: 55.243725 \tacc: 86.736685\tValidation Loss: 177.385654, time5.3\n",
      "Epoch: 1741 \tTraining Loss: 55.241812 \tacc: 87.115473\tValidation Loss: 184.414160, time5.3\n",
      "Epoch: 1742 \tTraining Loss: 55.603382 \tacc: 88.119835\tValidation Loss: 178.547775, time5.2\n",
      "Epoch: 1743 \tTraining Loss: 56.140955 \tacc: 87.115473\tValidation Loss: 184.103993, time5.2\n",
      "Epoch: 1744 \tTraining Loss: 55.188246 \tacc: 86.449725\tValidation Loss: 188.694620, time5.2\n",
      "Epoch: 1745 \tTraining Loss: 55.396771 \tacc: 87.126951\tValidation Loss: 185.975941, time5.3\n",
      "Epoch: 1746 \tTraining Loss: 55.262856 \tacc: 86.524334\tValidation Loss: 183.919142, time5.3\n",
      "Epoch: 1747 \tTraining Loss: 55.314509 \tacc: 86.317723\tValidation Loss: 190.061790, time5.3\n",
      "Epoch: 1748 \tTraining Loss: 55.769819 \tacc: 86.243113\tValidation Loss: 184.092581, time5.3\n",
      "Epoch: 1749 \tTraining Loss: 55.748776 \tacc: 86.541552\tValidation Loss: 186.344453, time5.3\n",
      "Epoch: 1750 \tTraining Loss: 55.366162 \tacc: 86.558770\tValidation Loss: 185.683361, time5.3\n",
      "Epoch: 1751 \tTraining Loss: 55.875038 \tacc: 86.604683\tValidation Loss: 183.919275, time5.3\n",
      "Epoch: 1752 \tTraining Loss: 54.455540 \tacc: 86.662075\tValidation Loss: 183.115510, time5.2\n",
      "Epoch: 1753 \tTraining Loss: 56.267218 \tacc: 86.971993\tValidation Loss: 178.501514, time5.3\n",
      "Epoch: 1754 \tTraining Loss: 55.907560 \tacc: 86.713728\tValidation Loss: 184.317819, time5.2\n",
      "Epoch: 1755 \tTraining Loss: 55.582339 \tacc: 86.489899\tValidation Loss: 182.003350, time5.2\n",
      "Epoch: 1756 \tTraining Loss: 55.448424 \tacc: 85.887282\tValidation Loss: 187.503647, time5.2\n",
      "Epoch: 1757 \tTraining Loss: 54.711892 \tacc: 86.495638\tValidation Loss: 185.901006, time5.3\n",
      "Epoch: 1758 \tTraining Loss: 55.060070 \tacc: 87.304867\tValidation Loss: 185.671798, time5.3\n",
      "Epoch: 1759 \tTraining Loss: 56.173477 \tacc: 86.202938\tValidation Loss: 187.237945, time5.3\n",
      "Epoch: 1760 \tTraining Loss: 55.534512 \tacc: 86.088154\tValidation Loss: 181.375896, time5.2\n",
      "Epoch: 1761 \tTraining Loss: 54.300582 \tacc: 86.851469\tValidation Loss: 185.630267, time5.2\n",
      "Epoch: 1762 \tTraining Loss: 54.895546 \tacc: 86.679293\tValidation Loss: 180.879861, time5.3\n",
      "Epoch: 1763 \tTraining Loss: 55.752602 \tacc: 86.323462\tValidation Loss: 176.457629, time5.3\n",
      "Epoch: 1764 \tTraining Loss: 56.177303 \tacc: 86.644858\tValidation Loss: 182.556425, time5.2\n",
      "Epoch: 1765 \tTraining Loss: 55.622513 \tacc: 86.260331\tValidation Loss: 188.992785, time5.2\n",
      "Epoch: 1766 \tTraining Loss: 55.163376 \tacc: 86.621901\tValidation Loss: 184.493440, time5.2\n",
      "Epoch: 1767 \tTraining Loss: 55.410162 \tacc: 86.558770\tValidation Loss: 184.991343, time5.2\n",
      "Epoch: 1768 \tTraining Loss: 55.190159 \tacc: 86.983471\tValidation Loss: 185.199654, time5.2\n",
      "Epoch: 1769 \tTraining Loss: 56.119911 \tacc: 86.771120\tValidation Loss: 187.387570, time5.2\n",
      "Epoch: 1770 \tTraining Loss: 55.624426 \tacc: 86.644858\tValidation Loss: 183.868646, time5.3\n",
      "Epoch: 1771 \tTraining Loss: 56.106520 \tacc: 86.880165\tValidation Loss: 180.099454, time5.3\n",
      "Epoch: 1772 \tTraining Loss: 55.410162 \tacc: 86.581726\tValidation Loss: 186.823674, time5.3\n",
      "Epoch: 1773 \tTraining Loss: 55.308770 \tacc: 86.076676\tValidation Loss: 182.682144, time5.3\n",
      "Epoch: 1774 \tTraining Loss: 55.165289 \tacc: 86.983471\tValidation Loss: 184.777354, time5.2\n",
      "Epoch: 1775 \tTraining Loss: 55.283900 \tacc: 86.225895\tValidation Loss: 182.515515, time5.3\n",
      "Epoch: 1776 \tTraining Loss: 56.359045 \tacc: 86.794077\tValidation Loss: 179.043927, time5.3\n",
      "Epoch: 1777 \tTraining Loss: 55.496250 \tacc: 86.667815\tValidation Loss: 181.764782, time5.3\n",
      "Epoch: 1778 \tTraining Loss: 56.410698 \tacc: 86.914601\tValidation Loss: 178.181087, time5.3\n",
      "Epoch: 1779 \tTraining Loss: 56.095041 \tacc: 86.277548\tValidation Loss: 181.215513, time5.2\n",
      "Epoch: 1780 \tTraining Loss: 55.117463 \tacc: 86.851469\tValidation Loss: 183.083497, time5.3\n",
      "Epoch: 1781 \tTraining Loss: 55.125115 \tacc: 87.614784\tValidation Loss: 179.795277, time5.3\n",
      "Epoch: 1782 \tTraining Loss: 56.005127 \tacc: 86.725207\tValidation Loss: 182.169286, time5.3\n",
      "Epoch: 1783 \tTraining Loss: 56.100781 \tacc: 86.868687\tValidation Loss: 182.892968, time5.3\n",
      "Epoch: 1784 \tTraining Loss: 55.245638 \tacc: 87.241736\tValidation Loss: 183.003296, time5.3\n",
      "Epoch: 1785 \tTraining Loss: 55.331726 \tacc: 86.197199\tValidation Loss: 184.010082, time5.3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1786 \tTraining Loss: 55.155724 \tacc: 86.512856\tValidation Loss: 179.523526, time5.2\n",
      "Epoch: 1787 \tTraining Loss: 55.494337 \tacc: 86.822773\tValidation Loss: 184.747131, time5.3\n",
      "Epoch: 1788 \tTraining Loss: 55.630165 \tacc: 87.327824\tValidation Loss: 183.339626, time5.3\n",
      "Epoch: 1789 \tTraining Loss: 56.213652 \tacc: 86.472681\tValidation Loss: 181.681366, time5.2\n",
      "Epoch: 1790 \tTraining Loss: 56.127564 \tacc: 86.558770\tValidation Loss: 184.823999, time5.2\n",
      "Epoch: 1791 \tTraining Loss: 55.982170 \tacc: 86.530073\tValidation Loss: 185.191201, time5.2\n",
      "Epoch: 1792 \tTraining Loss: 55.308770 \tacc: 86.564509\tValidation Loss: 183.387009, time5.2\n",
      "Epoch: 1793 \tTraining Loss: 55.033287 \tacc: 86.220156\tValidation Loss: 185.200674, time5.2\n",
      "Epoch: 1794 \tTraining Loss: 54.225972 \tacc: 86.432507\tValidation Loss: 185.756578, time5.3\n",
      "Epoch: 1795 \tTraining Loss: 55.501990 \tacc: 86.237374\tValidation Loss: 186.273100, time5.3\n",
      "Epoch: 1796 \tTraining Loss: 55.299204 \tacc: 85.996327\tValidation Loss: 185.136624, time5.3\n",
      "Epoch: 1797 \tTraining Loss: 55.111723 \tacc: 87.436869\tValidation Loss: 185.984713, time5.2\n",
      "Epoch: 1798 \tTraining Loss: 55.913300 \tacc: 88.825758\tValidation Loss: 179.817222, time5.3\n",
      "Epoch: 1799 \tTraining Loss: 57.944980 \tacc: 87.654959\tValidation Loss: 173.531945, time5.3\n",
      "Epoch: 1800 \tTraining Loss: 56.894705 \tacc: 86.891644\tValidation Loss: 174.970341, time5.3\n",
      "Epoch: 1801 \tTraining Loss: 56.272957 \tacc: 86.616162\tValidation Loss: 178.620730, time5.3\n",
      "Epoch: 1802 \tTraining Loss: 56.269131 \tacc: 86.656336\tValidation Loss: 178.793454, time5.3\n",
      "Epoch: 1803 \tTraining Loss: 55.534512 \tacc: 86.403811\tValidation Loss: 181.455669, time5.3\n",
      "Epoch: 1804 \tTraining Loss: 55.993649 \tacc: 86.070937\tValidation Loss: 182.168616, time5.3\n",
      "Epoch: 1805 \tTraining Loss: 57.223753 \tacc: 86.994949\tValidation Loss: 175.140956, time5.2\n",
      "Epoch: 1806 \tTraining Loss: 57.076446 \tacc: 87.046602\tValidation Loss: 179.209214, time5.3\n",
      "Epoch: 1807 \tTraining Loss: 56.877487 \tacc: 86.702250\tValidation Loss: 179.217056, time5.2\n",
      "Epoch: 1808 \tTraining Loss: 55.482859 \tacc: 86.495638\tValidation Loss: 177.437478, time5.2\n",
      "Epoch: 1809 \tTraining Loss: 55.689470 \tacc: 86.369376\tValidation Loss: 178.686806, time5.2\n",
      "Epoch: 1810 \tTraining Loss: 55.454163 \tacc: 86.874426\tValidation Loss: 185.501807, time5.3\n",
      "Epoch: 1811 \tTraining Loss: 55.936257 \tacc: 86.489899\tValidation Loss: 180.131375, time5.3\n",
      "Epoch: 1812 \tTraining Loss: 56.437481 \tacc: 86.248852\tValidation Loss: 184.092560, time5.3\n",
      "Epoch: 1813 \tTraining Loss: 55.467554 \tacc: 87.987833\tValidation Loss: 178.938674, time5.3\n",
      "Epoch: 1814 \tTraining Loss: 56.866009 \tacc: 86.495638\tValidation Loss: 178.784521, time5.3\n",
      "Epoch: 1815 \tTraining Loss: 55.838690 \tacc: 86.834252\tValidation Loss: 178.810395, time5.3\n",
      "Epoch: 1816 \tTraining Loss: 56.005127 \tacc: 86.179982\tValidation Loss: 178.833776, time5.3\n",
      "Epoch: 1817 \tTraining Loss: 55.536425 \tacc: 87.184343\tValidation Loss: 179.226266, time5.3\n",
      "Epoch: 1818 \tTraining Loss: 55.794689 \tacc: 87.471304\tValidation Loss: 180.517188, time5.2\n",
      "Epoch: 1819 \tTraining Loss: 55.735384 \tacc: 87.982094\tValidation Loss: 175.263813, time5.3\n",
      "Epoch: 1820 \tTraining Loss: 56.190695 \tacc: 87.023646\tValidation Loss: 180.704873, time5.2\n",
      "Epoch: 1821 \tTraining Loss: 56.139042 \tacc: 86.667815\tValidation Loss: 184.155917, time5.3\n",
      "Epoch: 1822 \tTraining Loss: 56.225130 \tacc: 87.287649\tValidation Loss: 179.587444, time5.2\n",
      "Epoch: 1823 \tTraining Loss: 55.691384 \tacc: 86.478421\tValidation Loss: 183.623635, time5.3\n",
      "Epoch: 1824 \tTraining Loss: 56.454699 \tacc: 87.012167\tValidation Loss: 180.009527, time5.2\n",
      "Epoch: 1825 \tTraining Loss: 56.617309 \tacc: 85.726584\tValidation Loss: 179.027252, time5.3\n",
      "Validation loss decreased (85.801194 --> 85.726584).  Saving model ...\n",
      "Epoch: 1826 \tTraining Loss: 55.205464 \tacc: 86.323462\tValidation Loss: 184.187180, time5.2\n",
      "Epoch: 1827 \tTraining Loss: 56.221304 \tacc: 87.505739\tValidation Loss: 179.139365, time5.2\n",
      "Epoch: 1828 \tTraining Loss: 55.808081 \tacc: 86.908861\tValidation Loss: 179.691881, time5.2\n",
      "Epoch: 1829 \tTraining Loss: 56.347567 \tacc: 86.598944\tValidation Loss: 179.530306, time5.2\n",
      "Epoch: 1830 \tTraining Loss: 55.974518 \tacc: 86.782599\tValidation Loss: 178.952842, time5.2\n",
      "Epoch: 1831 \tTraining Loss: 55.159550 \tacc: 87.230257\tValidation Loss: 185.648425, time5.2\n",
      "Epoch: 1832 \tTraining Loss: 55.521120 \tacc: 86.553030\tValidation Loss: 185.296450, time5.2\n",
      "Epoch: 1833 \tTraining Loss: 55.892256 \tacc: 86.507117\tValidation Loss: 186.669338, time5.2\n",
      "Epoch: 1834 \tTraining Loss: 55.280073 \tacc: 86.484160\tValidation Loss: 183.609914, time5.2\n",
      "Epoch: 1835 \tTraining Loss: 55.743036 \tacc: 87.683655\tValidation Loss: 185.805794, time5.2\n",
      "Epoch: 1836 \tTraining Loss: 55.697123 \tacc: 86.639118\tValidation Loss: 185.833280, time5.2\n",
      "Epoch: 1837 \tTraining Loss: 55.530686 \tacc: 87.976354\tValidation Loss: 178.658107, time5.2\n",
      "Epoch: 1838 \tTraining Loss: 57.476278 \tacc: 86.817034\tValidation Loss: 180.397200, time5.2\n",
      "Epoch: 1839 \tTraining Loss: 56.194521 \tacc: 86.931818\tValidation Loss: 183.972794, time5.2\n",
      "Epoch: 1840 \tTraining Loss: 56.204086 \tacc: 88.286272\tValidation Loss: 179.946930, time5.2\n",
      "Epoch: 1841 \tTraining Loss: 57.946893 \tacc: 86.157025\tValidation Loss: 182.027333, time5.2\n",
      "Epoch: 1842 \tTraining Loss: 55.547903 \tacc: 87.149908\tValidation Loss: 185.293526, time5.3\n",
      "Epoch: 1843 \tTraining Loss: 54.861111 \tacc: 87.545914\tValidation Loss: 185.722898, time5.2\n",
      "Epoch: 1844 \tTraining Loss: 56.471916 \tacc: 87.178604\tValidation Loss: 183.272732, time5.3\n",
      "Epoch: 1845 \tTraining Loss: 55.601469 \tacc: 86.271809\tValidation Loss: 184.672042, time5.3\n",
      "Epoch: 1846 \tTraining Loss: 55.798515 \tacc: 87.310606\tValidation Loss: 181.546494, time5.3\n",
      "Epoch: 1847 \tTraining Loss: 55.865473 \tacc: 86.558770\tValidation Loss: 185.995409, time5.2\n",
      "Epoch: 1848 \tTraining Loss: 55.609122 \tacc: 87.454086\tValidation Loss: 179.208115, time5.2\n",
      "Epoch: 1849 \tTraining Loss: 55.913300 \tacc: 86.570248\tValidation Loss: 183.265550, time5.2\n",
      "Epoch: 1850 \tTraining Loss: 55.056244 \tacc: 86.621901\tValidation Loss: 184.280557, time5.2\n",
      "Epoch: 1851 \tTraining Loss: 54.966330 \tacc: 86.202938\tValidation Loss: 187.832718, time5.2\n",
      "Epoch: 1852 \tTraining Loss: 55.316422 \tacc: 86.421028\tValidation Loss: 186.190861, time5.2\n",
      "Epoch: 1853 \tTraining Loss: 55.635904 \tacc: 87.092516\tValidation Loss: 183.087407, time5.2\n",
      "Epoch: 1854 \tTraining Loss: 54.558846 \tacc: 86.725207\tValidation Loss: 187.099383, time5.2\n",
      "Epoch: 1855 \tTraining Loss: 55.691384 \tacc: 86.690771\tValidation Loss: 186.800062, time5.2\n",
      "Epoch: 1856 \tTraining Loss: 54.742501 \tacc: 86.111111\tValidation Loss: 184.376958, time5.2\n",
      "Epoch: 1857 \tTraining Loss: 55.079201 \tacc: 87.551653\tValidation Loss: 184.635960, time5.3\n",
      "Epoch: 1858 \tTraining Loss: 55.130854 \tacc: 86.598944\tValidation Loss: 187.526542, time5.2\n",
      "Epoch: 1859 \tTraining Loss: 55.704775 \tacc: 86.530073\tValidation Loss: 187.111276, time5.3\n",
      "Epoch: 1860 \tTraining Loss: 56.167738 \tacc: 86.432507\tValidation Loss: 188.708363, time5.2\n",
      "Epoch: 1861 \tTraining Loss: 55.410162 \tacc: 87.459826\tValidation Loss: 188.805114, time5.2\n",
      "Epoch: 1862 \tTraining Loss: 56.052954 \tacc: 86.088154\tValidation Loss: 185.388388, time5.3\n",
      "Epoch: 1863 \tTraining Loss: 54.885981 \tacc: 86.610422\tValidation Loss: 189.326731, time5.2\n",
      "Epoch: 1864 \tTraining Loss: 55.193985 \tacc: 86.139807\tValidation Loss: 188.718965, time5.3\n",
      "Epoch: 1865 \tTraining Loss: 55.894169 \tacc: 86.472681\tValidation Loss: 186.056754, time5.2\n",
      "Epoch: 1866 \tTraining Loss: 55.475207 \tacc: 87.235996\tValidation Loss: 184.459381, time5.2\n",
      "Epoch: 1867 \tTraining Loss: 55.716253 \tacc: 86.369376\tValidation Loss: 184.239188, time5.2\n",
      "Epoch: 1868 \tTraining Loss: 55.645470 \tacc: 86.650597\tValidation Loss: 186.850269, time5.3\n",
      "Epoch: 1869 \tTraining Loss: 55.295378 \tacc: 86.357897\tValidation Loss: 187.731086, time5.2\n",
      "Epoch: 1870 \tTraining Loss: 55.092593 \tacc: 86.914601\tValidation Loss: 187.283886, time5.3\n",
      "Epoch: 1871 \tTraining Loss: 54.589455 \tacc: 86.702250\tValidation Loss: 190.173944, time5.2\n",
      "Epoch: 1872 \tTraining Loss: 55.589991 \tacc: 86.019284\tValidation Loss: 186.628134, time5.3\n",
      "Epoch: 1873 \tTraining Loss: 54.863024 \tacc: 87.247475\tValidation Loss: 188.375895, time5.2\n",
      "Epoch: 1874 \tTraining Loss: 55.031374 \tacc: 86.329201\tValidation Loss: 190.324601, time5.2\n",
      "Epoch: 1875 \tTraining Loss: 55.081114 \tacc: 86.231635\tValidation Loss: 186.179571, time5.2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1876 \tTraining Loss: 54.600934 \tacc: 87.195822\tValidation Loss: 186.796597, time5.2\n",
      "Epoch: 1877 \tTraining Loss: 55.473294 \tacc: 87.190083\tValidation Loss: 188.069883, time5.3\n",
      "Epoch: 1878 \tTraining Loss: 54.738675 \tacc: 86.306244\tValidation Loss: 188.177415, time5.2\n",
      "Epoch: 1879 \tTraining Loss: 54.384757 \tacc: 87.023646\tValidation Loss: 185.337979, time5.3\n",
      "Epoch: 1880 \tTraining Loss: 54.585629 \tacc: 85.927456\tValidation Loss: 195.287309, time5.2\n",
      "Epoch: 1881 \tTraining Loss: 54.092057 \tacc: 86.851469\tValidation Loss: 188.305171, time5.3\n",
      "Epoch: 1882 \tTraining Loss: 55.318335 \tacc: 86.283287\tValidation Loss: 181.977656, time5.2\n",
      "Epoch: 1883 \tTraining Loss: 55.249464 \tacc: 87.827135\tValidation Loss: 182.152467, time5.3\n",
      "Epoch: 1884 \tTraining Loss: 56.020432 \tacc: 86.931818\tValidation Loss: 190.854448, time5.2\n",
      "Epoch: 1885 \tTraining Loss: 54.776936 \tacc: 86.725207\tValidation Loss: 188.567558, time5.2\n",
      "Epoch: 1886 \tTraining Loss: 54.973982 \tacc: 86.484160\tValidation Loss: 189.228821, time5.2\n",
      "Epoch: 1887 \tTraining Loss: 54.738675 \tacc: 87.006428\tValidation Loss: 190.656580, time5.3\n",
      "Epoch: 1888 \tTraining Loss: 55.966866 \tacc: 86.776860\tValidation Loss: 183.770687, time5.2\n",
      "Epoch: 1889 \tTraining Loss: 55.318335 \tacc: 85.887282\tValidation Loss: 191.630154, time5.2\n",
      "Epoch: 1890 \tTraining Loss: 54.643021 \tacc: 86.214417\tValidation Loss: 191.596021, time5.2\n",
      "Epoch: 1891 \tTraining Loss: 54.488062 \tacc: 87.063820\tValidation Loss: 193.749937, time5.2\n",
      "Epoch: 1892 \tTraining Loss: 55.058157 \tacc: 86.759642\tValidation Loss: 190.254351, time5.3\n",
      "Epoch: 1893 \tTraining Loss: 54.296755 \tacc: 87.195822\tValidation Loss: 186.942591, time5.2\n",
      "Epoch: 1894 \tTraining Loss: 54.520585 \tacc: 86.128329\tValidation Loss: 191.114072, time5.2\n",
      "Epoch: 1895 \tTraining Loss: 55.293465 \tacc: 86.168503\tValidation Loss: 193.701263, time5.2\n",
      "Epoch: 1896 \tTraining Loss: 54.962504 \tacc: 87.006428\tValidation Loss: 185.602637, time5.3\n",
      "Epoch: 1897 \tTraining Loss: 54.065274 \tacc: 86.042241\tValidation Loss: 191.255573, time5.2\n",
      "Epoch: 1898 \tTraining Loss: 55.092593 \tacc: 86.891644\tValidation Loss: 187.840790, time5.3\n",
      "Epoch: 1899 \tTraining Loss: 55.743036 \tacc: 86.696511\tValidation Loss: 186.585540, time5.3\n",
      "Epoch: 1900 \tTraining Loss: 55.896082 \tacc: 87.448347\tValidation Loss: 188.281629, time5.2\n",
      "Epoch: 1901 \tTraining Loss: 54.910851 \tacc: 86.346419\tValidation Loss: 187.649353, time5.2\n",
      "Epoch: 1902 \tTraining Loss: 55.186333 \tacc: 87.299128\tValidation Loss: 189.586351, time5.3\n",
      "Epoch: 1903 \tTraining Loss: 54.706152 \tacc: 87.126951\tValidation Loss: 189.970891, time5.3\n",
      "Epoch: 1904 \tTraining Loss: 54.027013 \tacc: 86.007805\tValidation Loss: 191.054197, time5.2\n",
      "Epoch: 1905 \tTraining Loss: 55.260943 \tacc: 86.093893\tValidation Loss: 194.448124, time5.3\n",
      "Epoch: 1906 \tTraining Loss: 55.603382 \tacc: 86.421028\tValidation Loss: 184.248905, time5.2\n",
      "Epoch: 1907 \tTraining Loss: 54.543541 \tacc: 86.306244\tValidation Loss: 186.964518, time5.3\n",
      "Epoch: 1908 \tTraining Loss: 55.612948 \tacc: 86.759642\tValidation Loss: 186.005680, time5.2\n",
      "Epoch: 1909 \tTraining Loss: 55.316422 \tacc: 86.604683\tValidation Loss: 189.532237, time5.3\n",
      "Epoch: 1910 \tTraining Loss: 54.977808 \tacc: 86.168503\tValidation Loss: 191.229940, time5.2\n",
      "Epoch: 1911 \tTraining Loss: 54.082492 \tacc: 86.398072\tValidation Loss: 194.805507, time5.3\n",
      "Epoch: 1912 \tTraining Loss: 54.968243 \tacc: 86.438246\tValidation Loss: 196.451701, time5.2\n",
      "Epoch: 1913 \tTraining Loss: 54.537802 \tacc: 86.495638\tValidation Loss: 195.218897, time5.3\n",
      "Epoch: 1914 \tTraining Loss: 55.276247 \tacc: 87.126951\tValidation Loss: 189.713134, time5.2\n",
      "Epoch: 1915 \tTraining Loss: 55.052418 \tacc: 86.484160\tValidation Loss: 191.100556, time5.2\n",
      "Epoch: 1916 \tTraining Loss: 54.602847 \tacc: 86.748163\tValidation Loss: 193.960814, time5.3\n",
      "Epoch: 1917 \tTraining Loss: 54.641108 \tacc: 86.105372\tValidation Loss: 192.286690, time5.2\n",
      "Epoch: 1918 \tTraining Loss: 55.172942 \tacc: 86.564509\tValidation Loss: 184.480969, time5.2\n",
      "Epoch: 1919 \tTraining Loss: 55.127028 \tacc: 86.461203\tValidation Loss: 189.039172, time5.2\n",
      "Epoch: 1920 \tTraining Loss: 54.824763 \tacc: 86.002066\tValidation Loss: 188.900191, time5.3\n",
      "Epoch: 1921 \tTraining Loss: 54.262320 \tacc: 86.570248\tValidation Loss: 189.053322, time5.2\n",
      "Epoch: 1922 \tTraining Loss: 55.823385 \tacc: 86.644858\tValidation Loss: 187.107787, time5.2\n",
      "Epoch: 1923 \tTraining Loss: 56.271044 \tacc: 86.266070\tValidation Loss: 186.740387, time5.2\n",
      "Epoch: 1924 \tTraining Loss: 54.750153 \tacc: 85.944674\tValidation Loss: 193.276827, time5.3\n",
      "Epoch: 1925 \tTraining Loss: 55.172942 \tacc: 89.807163\tValidation Loss: 190.586475, time5.2\n",
      "Epoch: 1926 \tTraining Loss: 58.312290 \tacc: 86.966253\tValidation Loss: 185.848324, time5.3\n",
      "Epoch: 1927 \tTraining Loss: 55.383379 \tacc: 86.593205\tValidation Loss: 185.717379, time5.2\n",
      "Epoch: 1928 \tTraining Loss: 56.246174 \tacc: 87.913223\tValidation Loss: 185.069867, time5.2\n",
      "Epoch: 1929 \tTraining Loss: 56.420263 \tacc: 86.266070\tValidation Loss: 191.468601, time5.3\n",
      "Epoch: 1930 \tTraining Loss: 54.564585 \tacc: 86.788338\tValidation Loss: 190.811179, time5.2\n",
      "Epoch: 1931 \tTraining Loss: 54.587542 \tacc: 86.880165\tValidation Loss: 190.933873, time5.3\n",
      "Epoch: 1932 \tTraining Loss: 55.444597 \tacc: 86.323462\tValidation Loss: 192.644585, time5.2\n",
      "Epoch: 1933 \tTraining Loss: 54.623890 \tacc: 86.512856\tValidation Loss: 189.540637, time5.3\n",
      "Epoch: 1934 \tTraining Loss: 54.637282 \tacc: 86.093893\tValidation Loss: 190.767188, time5.2\n",
      "Epoch: 1935 \tTraining Loss: 54.597107 \tacc: 86.748163\tValidation Loss: 190.400974, time5.3\n",
      "Epoch: 1936 \tTraining Loss: 55.572773 \tacc: 86.903122\tValidation Loss: 190.384606, time5.2\n",
      "Epoch: 1937 \tTraining Loss: 55.270508 \tacc: 86.518595\tValidation Loss: 190.288069, time5.3\n",
      "Epoch: 1938 \tTraining Loss: 55.211203 \tacc: 87.436869\tValidation Loss: 187.557701, time5.2\n",
      "Epoch: 1939 \tTraining Loss: 55.557469 \tacc: 86.581726\tValidation Loss: 190.203441, time5.3\n",
      "Epoch: 1940 \tTraining Loss: 55.293465 \tacc: 87.373737\tValidation Loss: 192.609507, time5.2\n",
      "Epoch: 1941 \tTraining Loss: 54.966330 \tacc: 87.023646\tValidation Loss: 192.280805, time5.2\n",
      "Epoch: 1942 \tTraining Loss: 55.645470 \tacc: 86.357897\tValidation Loss: 193.456428, time5.3\n",
      "Epoch: 1943 \tTraining Loss: 55.897995 \tacc: 88.223140\tValidation Loss: 183.835382, time5.2\n",
      "Epoch: 1944 \tTraining Loss: 55.333639 \tacc: 86.627640\tValidation Loss: 194.089317, time5.3\n",
      "Epoch: 1945 \tTraining Loss: 54.736762 \tacc: 87.425390\tValidation Loss: 196.031236, time5.2\n",
      "Epoch: 1946 \tTraining Loss: 55.501990 \tacc: 86.960514\tValidation Loss: 193.639952, time5.3\n",
      "Epoch: 1947 \tTraining Loss: 54.792241 \tacc: 87.063820\tValidation Loss: 194.115940, time5.2\n",
      "Epoch: 1948 \tTraining Loss: 54.748240 \tacc: 86.403811\tValidation Loss: 192.929454, time5.3\n",
      "Epoch: 1949 \tTraining Loss: 54.757805 \tacc: 88.360882\tValidation Loss: 188.837658, time5.2\n",
      "Epoch: 1950 \tTraining Loss: 56.433655 \tacc: 86.977732\tValidation Loss: 195.863946, time5.3\n",
      "Epoch: 1951 \tTraining Loss: 55.517294 \tacc: 86.811295\tValidation Loss: 192.119847, time5.2\n",
      "Epoch: 1952 \tTraining Loss: 54.782675 \tacc: 86.966253\tValidation Loss: 194.842531, time5.3\n",
      "Epoch: 1953 \tTraining Loss: 55.077288 \tacc: 86.541552\tValidation Loss: 195.147973, time5.2\n",
      "Epoch: 1954 \tTraining Loss: 54.964417 \tacc: 87.373737\tValidation Loss: 190.224686, time5.2\n",
      "Epoch: 1955 \tTraining Loss: 56.278696 \tacc: 89.468549\tValidation Loss: 186.079878, time5.3\n",
      "Epoch: 1956 \tTraining Loss: 57.223753 \tacc: 86.478421\tValidation Loss: 185.788352, time5.2\n",
      "Epoch: 1957 \tTraining Loss: 55.350857 \tacc: 85.979109\tValidation Loss: 188.321186, time5.3\n",
      "Epoch: 1958 \tTraining Loss: 54.822850 \tacc: 87.029385\tValidation Loss: 184.643073, time5.3\n",
      "Epoch: 1959 \tTraining Loss: 55.756428 \tacc: 86.564509\tValidation Loss: 188.078890, time5.3\n",
      "Epoch: 1960 \tTraining Loss: 55.630165 \tacc: 86.742424\tValidation Loss: 189.761552, time5.2\n",
      "Epoch: 1961 \tTraining Loss: 54.652586 \tacc: 86.960514\tValidation Loss: 189.410029, time5.3\n",
      "Epoch: 1962 \tTraining Loss: 54.981635 \tacc: 86.736685\tValidation Loss: 189.811240, time5.2\n",
      "Epoch: 1963 \tTraining Loss: 56.056780 \tacc: 87.040863\tValidation Loss: 186.719278, time5.3\n",
      "Epoch: 1964 \tTraining Loss: 55.316422 \tacc: 86.765381\tValidation Loss: 194.683290, time5.2\n",
      "Epoch: 1965 \tTraining Loss: 55.561295 \tacc: 86.868687\tValidation Loss: 191.389087, time5.3\n",
      "Epoch: 1966 \tTraining Loss: 55.224594 \tacc: 86.346419\tValidation Loss: 192.792541, time5.2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1967 \tTraining Loss: 54.491889 \tacc: 86.386593\tValidation Loss: 195.168420, time5.3\n",
      "Epoch: 1968 \tTraining Loss: 54.444062 \tacc: 86.168503\tValidation Loss: 194.436045, time5.2\n",
      "Epoch: 1969 \tTraining Loss: 54.702326 \tacc: 86.530073\tValidation Loss: 197.938363, time5.2\n",
      "Epoch: 1970 \tTraining Loss: 54.658326 \tacc: 86.294766\tValidation Loss: 197.886585, time5.3\n",
      "Epoch: 1971 \tTraining Loss: 56.202173 \tacc: 86.415289\tValidation Loss: 193.632284, time5.2\n",
      "Epoch: 1972 \tTraining Loss: 55.159550 \tacc: 87.436869\tValidation Loss: 195.295034, time5.3\n",
      "Epoch: 1973 \tTraining Loss: 55.620600 \tacc: 86.656336\tValidation Loss: 194.582924, time5.2\n",
      "Epoch: 1974 \tTraining Loss: 56.022345 \tacc: 86.283287\tValidation Loss: 189.843677, time5.3\n",
      "Epoch: 1975 \tTraining Loss: 55.318335 \tacc: 85.996327\tValidation Loss: 187.414870, time5.2\n",
      "Epoch: 1976 \tTraining Loss: 55.157637 \tacc: 86.685032\tValidation Loss: 187.711711, time5.3\n",
      "Epoch: 1977 \tTraining Loss: 54.897459 \tacc: 86.421028\tValidation Loss: 194.418257, time5.2\n",
      "Epoch: 1978 \tTraining Loss: 55.012244 \tacc: 86.558770\tValidation Loss: 194.334317, time5.3\n",
      "Epoch: 1979 \tTraining Loss: 55.897995 \tacc: 87.821396\tValidation Loss: 190.784299, time5.2\n",
      "Epoch: 1980 \tTraining Loss: 56.194521 \tacc: 86.541552\tValidation Loss: 192.304058, time5.3\n",
      "Epoch: 1981 \tTraining Loss: 55.624426 \tacc: 87.230257\tValidation Loss: 189.192713, time5.2\n",
      "Epoch: 1982 \tTraining Loss: 55.984083 \tacc: 86.799816\tValidation Loss: 191.313176, time5.2\n",
      "Epoch: 1983 \tTraining Loss: 54.985461 \tacc: 86.438246\tValidation Loss: 186.349324, time5.3\n",
      "Epoch: 1984 \tTraining Loss: 55.844429 \tacc: 86.960514\tValidation Loss: 188.766869, time5.2\n",
      "Epoch: 1985 \tTraining Loss: 56.546526 \tacc: 86.421028\tValidation Loss: 185.086796, time5.3\n",
      "Epoch: 1986 \tTraining Loss: 55.373814 \tacc: 86.019284\tValidation Loss: 193.786418, time5.2\n",
      "Epoch: 1987 \tTraining Loss: 55.314509 \tacc: 86.702250\tValidation Loss: 195.773498, time5.3\n",
      "Epoch: 1988 \tTraining Loss: 55.039027 \tacc: 87.207300\tValidation Loss: 191.103340, time5.2\n",
      "Epoch: 1989 \tTraining Loss: 55.433119 \tacc: 87.580349\tValidation Loss: 191.392844, time5.3\n",
      "Epoch: 1990 \tTraining Loss: 56.236609 \tacc: 86.570248\tValidation Loss: 188.217282, time5.2\n",
      "Epoch: 1991 \tTraining Loss: 54.855372 \tacc: 86.128329\tValidation Loss: 188.296747, time5.3\n",
      "Epoch: 1992 \tTraining Loss: 54.732935 \tacc: 86.598944\tValidation Loss: 190.576475, time5.2\n",
      "Epoch: 1993 \tTraining Loss: 55.017983 \tacc: 86.157025\tValidation Loss: 195.319764, time5.3\n",
      "Epoch: 1994 \tTraining Loss: 55.098332 \tacc: 87.603306\tValidation Loss: 183.148381, time5.2\n",
      "Epoch: 1995 \tTraining Loss: 58.308463 \tacc: 86.794077\tValidation Loss: 192.541550, time5.2\n",
      "Epoch: 1996 \tTraining Loss: 55.148072 \tacc: 86.759642\tValidation Loss: 189.158821, time5.3\n",
      "Epoch: 1997 \tTraining Loss: 55.391032 \tacc: 85.692149\tValidation Loss: 196.950701, time5.3\n",
      "Validation loss decreased (85.726584 --> 85.692149).  Saving model ...\n",
      "Epoch: 1998 \tTraining Loss: 54.620064 \tacc: 86.662075\tValidation Loss: 191.921243, time5.3\n",
      "Epoch: 1999 \tTraining Loss: 54.945286 \tacc: 86.914601\tValidation Loss: 192.686577, time5.3\n",
      "Epoch: 2000 \tTraining Loss: 55.283900 \tacc: 86.174242\tValidation Loss: 197.002396, time5.2\n",
      "Epoch: 2001 \tTraining Loss: 54.914677 \tacc: 87.012167\tValidation Loss: 189.931854, time5.3\n",
      "Epoch: 2002 \tTraining Loss: 55.568947 \tacc: 86.151286\tValidation Loss: 191.328386, time5.2\n",
      "Epoch: 2003 \tTraining Loss: 54.641108 \tacc: 87.161387\tValidation Loss: 186.481293, time5.3\n",
      "Epoch: 2004 \tTraining Loss: 54.796067 \tacc: 86.231635\tValidation Loss: 194.857412, time5.3\n",
      "Epoch: 2005 \tTraining Loss: 53.979186 \tacc: 86.679293\tValidation Loss: 190.944838, time5.3\n",
      "Epoch: 2006 \tTraining Loss: 56.070171 \tacc: 86.449725\tValidation Loss: 187.209932, time5.3\n",
      "Epoch: 2007 \tTraining Loss: 55.322161 \tacc: 86.851469\tValidation Loss: 187.757114, time5.2\n",
      "Epoch: 2008 \tTraining Loss: 56.031910 \tacc: 87.000689\tValidation Loss: 187.316755, time5.2\n",
      "Epoch: 2009 \tTraining Loss: 56.558004 \tacc: 86.484160\tValidation Loss: 186.479820, time5.2\n",
      "Epoch: 2010 \tTraining Loss: 55.664601 \tacc: 85.961892\tValidation Loss: 182.710297, time5.2\n",
      "Epoch: 2011 \tTraining Loss: 55.077288 \tacc: 86.598944\tValidation Loss: 188.203942, time5.3\n",
      "Epoch: 2012 \tTraining Loss: 55.553642 \tacc: 87.293388\tValidation Loss: 187.771000, time5.3\n",
      "Epoch: 2013 \tTraining Loss: 56.020432 \tacc: 86.220156\tValidation Loss: 188.316431, time5.3\n",
      "Epoch: 2014 \tTraining Loss: 55.312596 \tacc: 86.013545\tValidation Loss: 186.955968, time5.3\n",
      "Epoch: 2015 \tTraining Loss: 56.007040 \tacc: 86.759642\tValidation Loss: 184.701304, time5.3\n",
      "Epoch: 2016 \tTraining Loss: 55.664601 \tacc: 85.910239\tValidation Loss: 190.503243, time5.3\n",
      "Epoch: 2017 \tTraining Loss: 55.618687 \tacc: 86.415289\tValidation Loss: 183.547943, time5.2\n",
      "Epoch: 2018 \tTraining Loss: 55.706688 \tacc: 86.380854\tValidation Loss: 186.459520, time5.2\n",
      "Epoch: 2019 \tTraining Loss: 55.744949 \tacc: 86.541552\tValidation Loss: 188.737496, time5.3\n",
      "Epoch: 2020 \tTraining Loss: 56.810530 \tacc: 87.367998\tValidation Loss: 181.386007, time5.3\n",
      "Epoch: 2021 \tTraining Loss: 55.523033 \tacc: 86.857208\tValidation Loss: 188.295659, time5.2\n",
      "Epoch: 2022 \tTraining Loss: 56.316957 \tacc: 86.398072\tValidation Loss: 181.589838, time5.2\n",
      "Epoch: 2023 \tTraining Loss: 56.005127 \tacc: 86.610422\tValidation Loss: 185.742844, time5.2\n",
      "Epoch: 2024 \tTraining Loss: 55.144245 \tacc: 86.662075\tValidation Loss: 184.876106, time5.3\n",
      "Epoch: 2025 \tTraining Loss: 54.926155 \tacc: 86.650597\tValidation Loss: 188.217845, time5.3\n",
      "Epoch: 2026 \tTraining Loss: 55.042853 \tacc: 86.369376\tValidation Loss: 189.414391, time5.2\n",
      "Epoch: 2027 \tTraining Loss: 55.226507 \tacc: 86.202938\tValidation Loss: 194.127842, time5.2\n",
      "Epoch: 2028 \tTraining Loss: 55.260943 \tacc: 86.111111\tValidation Loss: 192.162547, time5.3\n",
      "Epoch: 2029 \tTraining Loss: 55.272421 \tacc: 86.306244\tValidation Loss: 191.770786, time5.2\n",
      "Epoch: 2030 \tTraining Loss: 55.853994 \tacc: 86.960514\tValidation Loss: 191.174538, time5.2\n",
      "Epoch: 2031 \tTraining Loss: 56.045301 \tacc: 86.340680\tValidation Loss: 188.272806, time5.3\n",
      "Epoch: 2032 \tTraining Loss: 55.002678 \tacc: 86.231635\tValidation Loss: 187.518332, time5.3\n",
      "Epoch: 2033 \tTraining Loss: 55.614861 \tacc: 86.323462\tValidation Loss: 189.333083, time5.3\n",
      "Epoch: 2034 \tTraining Loss: 55.903734 \tacc: 87.482782\tValidation Loss: 185.219037, time5.3\n",
      "Epoch: 2035 \tTraining Loss: 55.601469 \tacc: 87.431129\tValidation Loss: 188.233630, time5.3\n",
      "Epoch: 2036 \tTraining Loss: 57.307928 \tacc: 86.811295\tValidation Loss: 184.831780, time5.3\n",
      "Epoch: 2037 \tTraining Loss: 56.045301 \tacc: 86.530073\tValidation Loss: 185.523820, time5.3\n",
      "Epoch: 2038 \tTraining Loss: 57.053489 \tacc: 87.304867\tValidation Loss: 184.324729, time5.3\n",
      "Epoch: 2039 \tTraining Loss: 56.165825 \tacc: 86.564509\tValidation Loss: 182.318846, time5.2\n",
      "Epoch: 2040 \tTraining Loss: 55.704775 \tacc: 86.570248\tValidation Loss: 184.979055, time5.3\n",
      "Epoch: 2041 \tTraining Loss: 55.976431 \tacc: 86.713728\tValidation Loss: 181.245154, time5.2\n",
      "Epoch: 2042 \tTraining Loss: 55.955387 \tacc: 86.868687\tValidation Loss: 188.413748, time5.3\n",
      "Epoch: 2043 \tTraining Loss: 55.056244 \tacc: 87.035124\tValidation Loss: 190.074207, time5.3\n",
      "Epoch: 2044 \tTraining Loss: 54.918503 \tacc: 86.535813\tValidation Loss: 193.043479, time5.3\n",
      "Epoch: 2045 \tTraining Loss: 55.127028 \tacc: 87.075298\tValidation Loss: 184.403475, time5.3\n",
      "Epoch: 2046 \tTraining Loss: 57.009489 \tacc: 87.471304\tValidation Loss: 187.755832, time5.3\n",
      "Epoch: 2047 \tTraining Loss: 57.577671 \tacc: 86.673554\tValidation Loss: 185.375846, time5.3\n",
      "Epoch: 2048 \tTraining Loss: 57.131925 \tacc: 86.271809\tValidation Loss: 183.592320, time5.3\n",
      "Epoch: 2049 \tTraining Loss: 57.307928 \tacc: 87.327824\tValidation Loss: 182.963372, time5.3\n",
      "Epoch: 2050 \tTraining Loss: 56.280609 \tacc: 86.369376\tValidation Loss: 189.459903, time5.2\n",
      "Epoch: 2051 \tTraining Loss: 55.785124 \tacc: 85.887282\tValidation Loss: 189.865641, time5.3\n",
      "Epoch: 2052 \tTraining Loss: 55.465641 \tacc: 86.627640\tValidation Loss: 188.483204, time5.3\n",
      "Epoch: 2053 \tTraining Loss: 56.202173 \tacc: 86.386593\tValidation Loss: 186.192997, time5.3\n",
      "Epoch: 2054 \tTraining Loss: 56.018519 \tacc: 86.369376\tValidation Loss: 186.061918, time5.2\n",
      "Epoch: 2055 \tTraining Loss: 56.724441 \tacc: 87.448347\tValidation Loss: 183.404096, time5.3\n",
      "Epoch: 2056 \tTraining Loss: 56.326523 \tacc: 86.392332\tValidation Loss: 185.493974, time5.3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2057 \tTraining Loss: 55.314509 \tacc: 86.794077\tValidation Loss: 190.341196, time5.3\n",
      "Epoch: 2058 \tTraining Loss: 56.514004 \tacc: 86.530073\tValidation Loss: 185.734734, time5.3\n",
      "Epoch: 2059 \tTraining Loss: 56.519743 \tacc: 86.231635\tValidation Loss: 191.697367, time5.3\n",
      "Epoch: 2060 \tTraining Loss: 56.422176 \tacc: 85.990588\tValidation Loss: 189.405817, time5.3\n",
      "Epoch: 2061 \tTraining Loss: 55.620600 \tacc: 87.017906\tValidation Loss: 187.485743, time5.2\n",
      "Epoch: 2062 \tTraining Loss: 56.644092 \tacc: 86.685032\tValidation Loss: 190.870589, time5.3\n",
      "Epoch: 2063 \tTraining Loss: 56.031910 \tacc: 87.109734\tValidation Loss: 189.454302, time5.3\n",
      "Epoch: 2064 \tTraining Loss: 56.276783 \tacc: 86.168503\tValidation Loss: 185.879413, time5.3\n",
      "Epoch: 2065 \tTraining Loss: 56.357132 \tacc: 87.867309\tValidation Loss: 183.648701, time5.3\n",
      "Epoch: 2066 \tTraining Loss: 57.392103 \tacc: 86.329201\tValidation Loss: 186.490261, time5.3\n",
      "Epoch: 2067 \tTraining Loss: 56.091215 \tacc: 86.334940\tValidation Loss: 184.982809, time5.3\n",
      "Epoch: 2068 \tTraining Loss: 56.259565 \tacc: 86.507117\tValidation Loss: 184.869480, time5.3\n",
      "Epoch: 2069 \tTraining Loss: 55.963039 \tacc: 86.593205\tValidation Loss: 188.648224, time5.3\n",
      "Epoch: 2070 \tTraining Loss: 56.100781 \tacc: 88.125574\tValidation Loss: 182.552614, time5.3\n",
      "Epoch: 2071 \tTraining Loss: 56.948271 \tacc: 86.289027\tValidation Loss: 185.159464, time5.2\n",
      "Epoch: 2072 \tTraining Loss: 55.230334 \tacc: 86.564509\tValidation Loss: 184.543962, time5.3\n",
      "Epoch: 2073 \tTraining Loss: 55.764080 \tacc: 86.748163\tValidation Loss: 183.977704, time5.3\n",
      "Epoch: 2074 \tTraining Loss: 55.915213 \tacc: 87.310606\tValidation Loss: 186.076216, time5.3\n",
      "Epoch: 2075 \tTraining Loss: 55.436945 \tacc: 87.477043\tValidation Loss: 188.245568, time5.3\n",
      "Epoch: 2076 \tTraining Loss: 56.295914 \tacc: 86.271809\tValidation Loss: 185.681869, time5.3\n",
      "Epoch: 2077 \tTraining Loss: 55.626339 \tacc: 86.311983\tValidation Loss: 190.785471, time5.3\n",
      "Epoch: 2078 \tTraining Loss: 55.700949 \tacc: 86.357897\tValidation Loss: 187.867779, time5.3\n",
      "Epoch: 2079 \tTraining Loss: 55.666514 \tacc: 86.375115\tValidation Loss: 187.793628, time5.3\n",
      "Epoch: 2080 \tTraining Loss: 54.960591 \tacc: 85.967631\tValidation Loss: 193.107975, time5.3\n",
      "Epoch: 2081 \tTraining Loss: 54.577977 \tacc: 87.367998\tValidation Loss: 187.097783, time5.3\n",
      "Epoch: 2082 \tTraining Loss: 55.771732 \tacc: 86.162764\tValidation Loss: 190.365533, time5.2\n",
      "Epoch: 2083 \tTraining Loss: 55.192072 \tacc: 86.782599\tValidation Loss: 186.798575, time5.3\n",
      "Epoch: 2084 \tTraining Loss: 55.827212 \tacc: 87.075298\tValidation Loss: 186.436421, time5.3\n",
      "Epoch: 2085 \tTraining Loss: 55.465641 \tacc: 87.517218\tValidation Loss: 188.365033, time5.3\n",
      "Epoch: 2086 \tTraining Loss: 56.238522 \tacc: 86.099633\tValidation Loss: 189.082818, time5.3\n",
      "Epoch: 2087 \tTraining Loss: 55.463728 \tacc: 86.294766\tValidation Loss: 183.053381, time5.3\n",
      "Epoch: 2088 \tTraining Loss: 56.219391 \tacc: 86.771120\tValidation Loss: 176.425774, time5.3\n",
      "Epoch: 2089 \tTraining Loss: 56.146694 \tacc: 87.844353\tValidation Loss: 184.419215, time5.3\n",
      "Epoch: 2090 \tTraining Loss: 56.294001 \tacc: 86.162764\tValidation Loss: 188.016495, time5.3\n",
      "Epoch: 2091 \tTraining Loss: 54.907025 \tacc: 86.621901\tValidation Loss: 191.519787, time5.3\n",
      "Epoch: 2092 \tTraining Loss: 55.622513 \tacc: 86.627640\tValidation Loss: 190.604342, time5.2\n",
      "Epoch: 2093 \tTraining Loss: 55.176768 \tacc: 86.570248\tValidation Loss: 192.226022, time5.2\n",
      "Epoch: 2094 \tTraining Loss: 55.211203 \tacc: 86.530073\tValidation Loss: 192.271152, time5.3\n",
      "Epoch: 2095 \tTraining Loss: 55.450337 \tacc: 86.277548\tValidation Loss: 191.697855, time5.3\n",
      "Epoch: 2096 \tTraining Loss: 55.576599 \tacc: 87.345041\tValidation Loss: 186.256784, time5.3\n",
      "Epoch: 2097 \tTraining Loss: 55.836777 \tacc: 86.822773\tValidation Loss: 193.994032, time5.3\n",
      "Epoch: 2098 \tTraining Loss: 54.532063 \tacc: 86.920340\tValidation Loss: 196.557052, time5.3\n",
      "Epoch: 2099 \tTraining Loss: 55.174855 \tacc: 87.718090\tValidation Loss: 192.880239, time5.3\n",
      "Epoch: 2100 \tTraining Loss: 55.589991 \tacc: 86.197199\tValidation Loss: 195.537400, time5.3\n",
      "Epoch: 2101 \tTraining Loss: 55.362335 \tacc: 86.553030\tValidation Loss: 187.416318, time5.3\n",
      "Epoch: 2102 \tTraining Loss: 55.588078 \tacc: 86.283287\tValidation Loss: 191.107303, time5.3\n",
      "Epoch: 2103 \tTraining Loss: 55.119376 \tacc: 86.093893\tValidation Loss: 197.287228, time5.3\n",
      "Epoch: 2104 \tTraining Loss: 56.246174 \tacc: 86.788338\tValidation Loss: 187.918403, time5.2\n",
      "Epoch: 2105 \tTraining Loss: 55.823385 \tacc: 86.421028\tValidation Loss: 195.740313, time5.3\n",
      "Epoch: 2106 \tTraining Loss: 55.188246 \tacc: 86.983471\tValidation Loss: 195.289732, time5.3\n",
      "Epoch: 2107 \tTraining Loss: 55.540251 \tacc: 86.616162\tValidation Loss: 194.267001, time5.3\n",
      "Epoch: 2108 \tTraining Loss: 54.956765 \tacc: 86.679293\tValidation Loss: 188.294615, time5.3\n",
      "Epoch: 2109 \tTraining Loss: 54.929982 \tacc: 86.530073\tValidation Loss: 188.009327, time5.2\n",
      "Epoch: 2110 \tTraining Loss: 54.975895 \tacc: 86.690771\tValidation Loss: 192.814846, time5.3\n",
      "Epoch: 2111 \tTraining Loss: 55.352770 \tacc: 87.155647\tValidation Loss: 193.205590, time5.3\n",
      "Epoch: 2112 \tTraining Loss: 55.186333 \tacc: 86.421028\tValidation Loss: 196.978934, time5.3\n",
      "Epoch: 2113 \tTraining Loss: 55.186333 \tacc: 85.967631\tValidation Loss: 196.070704, time5.3\n",
      "Epoch: 2114 \tTraining Loss: 55.683731 \tacc: 86.300505\tValidation Loss: 190.288382, time5.2\n",
      "Epoch: 2115 \tTraining Loss: 55.012244 \tacc: 87.224518\tValidation Loss: 192.183984, time5.3\n",
      "Epoch: 2116 \tTraining Loss: 55.589991 \tacc: 86.317723\tValidation Loss: 191.263036, time5.3\n",
      "Epoch: 2117 \tTraining Loss: 55.383379 \tacc: 85.990588\tValidation Loss: 187.497094, time5.3\n",
      "Epoch: 2118 \tTraining Loss: 55.044766 \tacc: 86.449725\tValidation Loss: 192.335615, time5.3\n",
      "Epoch: 2119 \tTraining Loss: 55.808081 \tacc: 86.202938\tValidation Loss: 189.675894, time5.3\n",
      "Epoch: 2120 \tTraining Loss: 56.332262 \tacc: 86.662075\tValidation Loss: 190.663951, time5.3\n",
      "Epoch: 2121 \tTraining Loss: 55.280073 \tacc: 86.151286\tValidation Loss: 198.902972, time5.3\n",
      "Epoch: 2122 \tTraining Loss: 54.589455 \tacc: 87.035124\tValidation Loss: 196.117778, time5.3\n",
      "Epoch: 2123 \tTraining Loss: 54.933808 \tacc: 86.013545\tValidation Loss: 195.526054, time5.3\n",
      "Epoch: 2124 \tTraining Loss: 54.526324 \tacc: 86.225895\tValidation Loss: 192.860620, time5.3\n",
      "Epoch: 2125 \tTraining Loss: 55.037114 \tacc: 86.753903\tValidation Loss: 192.516378, time5.2\n",
      "Epoch: 2126 \tTraining Loss: 54.828589 \tacc: 87.253214\tValidation Loss: 188.782699, time5.3\n",
      "Epoch: 2127 \tTraining Loss: 55.737297 \tacc: 85.835629\tValidation Loss: 196.452930, time5.3\n",
      "Epoch: 2128 \tTraining Loss: 54.692761 \tacc: 86.248852\tValidation Loss: 197.187622, time5.3\n",
      "Epoch: 2129 \tTraining Loss: 55.123202 \tacc: 87.356520\tValidation Loss: 191.037465, time5.3\n",
      "Epoch: 2130 \tTraining Loss: 55.086853 \tacc: 86.570248\tValidation Loss: 191.817223, time5.3\n",
      "Epoch: 2131 \tTraining Loss: 54.694674 \tacc: 86.053719\tValidation Loss: 199.295422, time5.3\n",
      "Epoch: 2132 \tTraining Loss: 54.849633 \tacc: 86.254591\tValidation Loss: 194.033651, time5.3\n",
      "Epoch: 2133 \tTraining Loss: 55.241812 \tacc: 86.851469\tValidation Loss: 189.312320, time5.3\n",
      "Epoch: 2134 \tTraining Loss: 56.512091 \tacc: 86.725207\tValidation Loss: 190.219018, time5.3\n",
      "Epoch: 2135 \tTraining Loss: 56.192608 \tacc: 87.781221\tValidation Loss: 188.204622, time5.3\n",
      "Epoch: 2136 \tTraining Loss: 56.334175 \tacc: 86.191460\tValidation Loss: 190.014767, time5.2\n",
      "Epoch: 2137 \tTraining Loss: 55.176768 \tacc: 86.311983\tValidation Loss: 192.958486, time5.3\n",
      "Epoch: 2138 \tTraining Loss: 55.176768 \tacc: 87.500000\tValidation Loss: 188.910953, time5.3\n",
      "Epoch: 2139 \tTraining Loss: 55.167202 \tacc: 88.567493\tValidation Loss: 191.164928, time5.3\n",
      "Epoch: 2140 \tTraining Loss: 56.165825 \tacc: 85.898760\tValidation Loss: 193.063380, time5.3\n",
      "Epoch: 2141 \tTraining Loss: 55.157637 \tacc: 87.121212\tValidation Loss: 194.382225, time5.3\n",
      "Epoch: 2142 \tTraining Loss: 55.368075 \tacc: 87.373737\tValidation Loss: 191.215030, time5.3\n",
      "Epoch: 2143 \tTraining Loss: 55.274334 \tacc: 87.741047\tValidation Loss: 192.110441, time5.3\n",
      "Epoch: 2144 \tTraining Loss: 54.650673 \tacc: 86.874426\tValidation Loss: 196.965220, time5.3\n",
      "Epoch: 2145 \tTraining Loss: 55.662687 \tacc: 86.518595\tValidation Loss: 190.156745, time5.3\n",
      "Epoch: 2146 \tTraining Loss: 55.924778 \tacc: 86.403811\tValidation Loss: 192.048458, time5.2\n",
      "Epoch: 2147 \tTraining Loss: 54.893633 \tacc: 86.891644\tValidation Loss: 199.549135, time5.3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2148 \tTraining Loss: 53.856749 \tacc: 86.799816\tValidation Loss: 196.138445, time5.3\n",
      "Epoch: 2149 \tTraining Loss: 54.505280 \tacc: 87.454086\tValidation Loss: 197.020511, time5.3\n",
      "Epoch: 2150 \tTraining Loss: 54.903199 \tacc: 86.231635\tValidation Loss: 199.231295, time5.3\n",
      "Epoch: 2151 \tTraining Loss: 55.704775 \tacc: 86.340680\tValidation Loss: 194.259635, time5.3\n",
      "Epoch: 2152 \tTraining Loss: 56.152433 \tacc: 86.633379\tValidation Loss: 193.382817, time5.3\n",
      "Epoch: 2153 \tTraining Loss: 54.796067 \tacc: 87.442608\tValidation Loss: 188.503101, time5.2\n",
      "Epoch: 2154 \tTraining Loss: 55.725819 \tacc: 86.329201\tValidation Loss: 196.461352, time5.3\n",
      "Epoch: 2155 \tTraining Loss: 54.706152 \tacc: 87.276171\tValidation Loss: 189.896560, time5.3\n",
      "Epoch: 2156 \tTraining Loss: 54.924242 \tacc: 87.597567\tValidation Loss: 195.508430, time5.3\n",
      "Epoch: 2157 \tTraining Loss: 56.424089 \tacc: 86.380854\tValidation Loss: 194.153377, time5.2\n",
      "Epoch: 2158 \tTraining Loss: 55.408249 \tacc: 86.828512\tValidation Loss: 190.404507, time5.2\n",
      "Epoch: 2159 \tTraining Loss: 55.785124 \tacc: 87.081038\tValidation Loss: 188.860525, time5.3\n",
      "Epoch: 2160 \tTraining Loss: 54.639195 \tacc: 86.277548\tValidation Loss: 198.716196, time5.3\n",
      "Epoch: 2161 \tTraining Loss: 54.813284 \tacc: 88.073921\tValidation Loss: 195.467189, time5.3\n",
      "Epoch: 2162 \tTraining Loss: 55.614861 \tacc: 86.891644\tValidation Loss: 195.583178, time5.3\n",
      "Epoch: 2163 \tTraining Loss: 54.962504 \tacc: 86.300505\tValidation Loss: 196.614875, time5.3\n",
      "Epoch: 2164 \tTraining Loss: 55.213116 \tacc: 86.604683\tValidation Loss: 193.533918, time5.3\n",
      "Epoch: 2165 \tTraining Loss: 55.289639 \tacc: 86.243113\tValidation Loss: 195.595679, time5.3\n",
      "Epoch: 2166 \tTraining Loss: 55.741123 \tacc: 85.990588\tValidation Loss: 188.792153, time5.3\n",
      "Epoch: 2167 \tTraining Loss: 54.954852 \tacc: 87.029385\tValidation Loss: 201.769545, time5.3\n",
      "Epoch: 2168 \tTraining Loss: 54.331191 \tacc: 86.822773\tValidation Loss: 195.494848, time5.2\n",
      "Epoch: 2169 \tTraining Loss: 55.611035 \tacc: 87.672176\tValidation Loss: 193.752974, time5.3\n",
      "Epoch: 2170 \tTraining Loss: 57.790021 \tacc: 86.053719\tValidation Loss: 190.416562, time5.3\n",
      "Epoch: 2171 \tTraining Loss: 54.597107 \tacc: 87.781221\tValidation Loss: 193.013407, time5.3\n",
      "Epoch: 2172 \tTraining Loss: 55.276247 \tacc: 87.121212\tValidation Loss: 194.444351, time5.3\n",
      "Epoch: 2173 \tTraining Loss: 55.017983 \tacc: 86.489899\tValidation Loss: 200.350239, time5.3\n",
      "Epoch: 2174 \tTraining Loss: 54.369452 \tacc: 86.489899\tValidation Loss: 196.900749, time5.3\n",
      "Epoch: 2175 \tTraining Loss: 54.755892 \tacc: 87.471304\tValidation Loss: 197.928408, time5.3\n",
      "Epoch: 2176 \tTraining Loss: 55.293465 \tacc: 87.632002\tValidation Loss: 194.392896, time5.3\n",
      "Epoch: 2177 \tTraining Loss: 56.039562 \tacc: 86.208678\tValidation Loss: 195.909641, time5.3\n",
      "Epoch: 2178 \tTraining Loss: 54.851546 \tacc: 86.994949\tValidation Loss: 190.587868, time5.3\n",
      "Epoch: 2179 \tTraining Loss: 55.570860 \tacc: 86.151286\tValidation Loss: 188.672802, time5.2\n",
      "Epoch: 2180 \tTraining Loss: 55.017983 \tacc: 86.868687\tValidation Loss: 190.947323, time5.3\n",
      "Epoch: 2181 \tTraining Loss: 56.297827 \tacc: 86.231635\tValidation Loss: 188.423399, time5.3\n",
      "Epoch: 2182 \tTraining Loss: 55.276247 \tacc: 86.231635\tValidation Loss: 187.737849, time5.3\n",
      "Epoch: 2183 \tTraining Loss: 54.937634 \tacc: 86.323462\tValidation Loss: 195.164314, time5.3\n",
      "Epoch: 2184 \tTraining Loss: 55.197811 \tacc: 86.822773\tValidation Loss: 197.042187, time5.3\n",
      "Epoch: 2185 \tTraining Loss: 55.039027 \tacc: 86.805556\tValidation Loss: 193.635244, time5.3\n",
      "Epoch: 2186 \tTraining Loss: 56.450872 \tacc: 86.570248\tValidation Loss: 194.297100, time5.3\n",
      "Epoch: 2187 \tTraining Loss: 55.817646 \tacc: 86.294766\tValidation Loss: 187.926646, time5.3\n",
      "Epoch: 2188 \tTraining Loss: 55.295378 \tacc: 86.885904\tValidation Loss: 193.844613, time5.3\n",
      "Epoch: 2189 \tTraining Loss: 55.274334 \tacc: 86.730946\tValidation Loss: 187.938208, time5.2\n",
      "Epoch: 2190 \tTraining Loss: 54.729109 \tacc: 87.058081\tValidation Loss: 193.344267, time5.3\n",
      "Epoch: 2191 \tTraining Loss: 55.394858 \tacc: 87.017906\tValidation Loss: 186.444142, time5.3\n",
      "Epoch: 2192 \tTraining Loss: 55.324074 \tacc: 85.818411\tValidation Loss: 189.933349, time5.3\n",
      "Epoch: 2193 \tTraining Loss: 55.216942 \tacc: 86.558770\tValidation Loss: 191.652450, time5.3\n",
      "Epoch: 2194 \tTraining Loss: 55.666514 \tacc: 86.759642\tValidation Loss: 187.818141, time5.3\n",
      "Epoch: 2195 \tTraining Loss: 55.104071 \tacc: 86.300505\tValidation Loss: 195.994263, time5.3\n",
      "Epoch: 2196 \tTraining Loss: 54.331191 \tacc: 86.449725\tValidation Loss: 200.366077, time5.3\n",
      "Epoch: 2197 \tTraining Loss: 55.146159 \tacc: 86.673554\tValidation Loss: 202.197296, time5.3\n",
      "Epoch: 2198 \tTraining Loss: 55.107897 \tacc: 86.197199\tValidation Loss: 199.276580, time5.3\n",
      "Epoch: 2199 \tTraining Loss: 55.276247 \tacc: 86.730946\tValidation Loss: 199.695956, time5.3\n",
      "Epoch: 2200 \tTraining Loss: 55.371901 \tacc: 88.779844\tValidation Loss: 195.628991, time5.2\n",
      "Epoch: 2201 \tTraining Loss: 55.415901 \tacc: 86.954775\tValidation Loss: 198.895337, time5.3\n",
      "Epoch: 2202 \tTraining Loss: 55.104071 \tacc: 87.654959\tValidation Loss: 197.902879, time5.3\n",
      "Epoch: 2203 \tTraining Loss: 57.172100 \tacc: 87.982094\tValidation Loss: 188.792659, time5.3\n",
      "Epoch: 2204 \tTraining Loss: 56.378176 \tacc: 86.449725\tValidation Loss: 194.009075, time5.3\n",
      "Epoch: 2205 \tTraining Loss: 56.318871 \tacc: 86.897383\tValidation Loss: 192.889161, time5.3\n",
      "Epoch: 2206 \tTraining Loss: 56.609657 \tacc: 86.443985\tValidation Loss: 190.218989, time5.3\n",
      "Epoch: 2207 \tTraining Loss: 55.677992 \tacc: 86.283287\tValidation Loss: 197.457123, time5.3\n",
      "Epoch: 2208 \tTraining Loss: 55.174855 \tacc: 86.581726\tValidation Loss: 193.297877, time5.3\n",
      "Epoch: 2209 \tTraining Loss: 55.899908 \tacc: 86.426768\tValidation Loss: 193.780210, time5.3\n",
      "Epoch: 2210 \tTraining Loss: 55.765993 \tacc: 87.178604\tValidation Loss: 196.250515, time5.3\n",
      "Epoch: 2211 \tTraining Loss: 56.014692 \tacc: 87.098255\tValidation Loss: 191.081066, time5.2\n",
      "Epoch: 2212 \tTraining Loss: 55.651209 \tacc: 86.897383\tValidation Loss: 190.091816, time5.3\n",
      "Epoch: 2213 \tTraining Loss: 55.765993 \tacc: 86.036501\tValidation Loss: 192.250199, time5.3\n",
      "Epoch: 2214 \tTraining Loss: 56.481481 \tacc: 85.973370\tValidation Loss: 193.770436, time5.3\n",
      "Epoch: 2215 \tTraining Loss: 56.068258 \tacc: 87.006428\tValidation Loss: 188.620986, time5.3\n",
      "Epoch: 2216 \tTraining Loss: 56.018519 \tacc: 87.436869\tValidation Loss: 189.777944, time5.3\n",
      "Epoch: 2217 \tTraining Loss: 54.840067 \tacc: 86.088154\tValidation Loss: 200.109358, time5.3\n",
      "Epoch: 2218 \tTraining Loss: 54.564585 \tacc: 86.082415\tValidation Loss: 205.850920, time5.3\n",
      "Epoch: 2219 \tTraining Loss: 54.585629 \tacc: 86.191460\tValidation Loss: 201.641111, time5.3\n",
      "Epoch: 2220 \tTraining Loss: 54.893633 \tacc: 86.667815\tValidation Loss: 207.474744, time5.3\n",
      "Epoch: 2221 \tTraining Loss: 55.276247 \tacc: 85.904500\tValidation Loss: 203.959316, time5.2\n",
      "Epoch: 2222 \tTraining Loss: 55.825298 \tacc: 86.753903\tValidation Loss: 194.218334, time5.3\n",
      "Epoch: 2223 \tTraining Loss: 55.693297 \tacc: 86.989210\tValidation Loss: 188.176567, time5.3\n",
      "Epoch: 2224 \tTraining Loss: 55.331726 \tacc: 86.254591\tValidation Loss: 199.610710, time5.3\n",
      "Epoch: 2225 \tTraining Loss: 55.086853 \tacc: 86.742424\tValidation Loss: 196.737491, time5.3\n",
      "Epoch: 2226 \tTraining Loss: 54.369452 \tacc: 86.065197\tValidation Loss: 202.965423, time5.3\n",
      "Epoch: 2227 \tTraining Loss: 54.840067 \tacc: 86.484160\tValidation Loss: 196.142618, time5.3\n",
      "Epoch: 2228 \tTraining Loss: 56.003214 \tacc: 87.677916\tValidation Loss: 190.885225, time5.3\n",
      "Epoch: 2229 \tTraining Loss: 55.509642 \tacc: 86.455464\tValidation Loss: 198.976705, time5.3\n",
      "Epoch: 2230 \tTraining Loss: 55.475207 \tacc: 86.805556\tValidation Loss: 191.938229, time5.3\n",
      "Epoch: 2231 \tTraining Loss: 55.039027 \tacc: 86.512856\tValidation Loss: 192.723086, time5.3\n",
      "Epoch: 2232 \tTraining Loss: 56.330349 \tacc: 87.121212\tValidation Loss: 191.221586, time5.2\n",
      "Epoch: 2233 \tTraining Loss: 55.486685 \tacc: 87.597567\tValidation Loss: 185.606192, time5.3\n",
      "Epoch: 2234 \tTraining Loss: 56.263391 \tacc: 86.776860\tValidation Loss: 188.595378, time5.3\n",
      "Epoch: 2235 \tTraining Loss: 55.019896 \tacc: 86.294766\tValidation Loss: 195.980889, time5.3\n",
      "Epoch: 2236 \tTraining Loss: 55.371901 \tacc: 86.862948\tValidation Loss: 191.742858, time5.3\n",
      "Epoch: 2237 \tTraining Loss: 54.899373 \tacc: 86.788338\tValidation Loss: 194.537120, time5.3\n",
      "Epoch: 2238 \tTraining Loss: 55.375727 \tacc: 86.834252\tValidation Loss: 197.058160, time5.3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2239 \tTraining Loss: 56.437481 \tacc: 86.053719\tValidation Loss: 195.359250, time5.3\n",
      "Epoch: 2240 \tTraining Loss: 54.520585 \tacc: 86.254591\tValidation Loss: 198.517063, time5.3\n",
      "Epoch: 2241 \tTraining Loss: 54.323538 \tacc: 86.179982\tValidation Loss: 196.308321, time5.3\n",
      "Epoch: 2242 \tTraining Loss: 55.237986 \tacc: 86.409550\tValidation Loss: 194.068195, time5.3\n",
      "Epoch: 2243 \tTraining Loss: 54.746327 \tacc: 86.484160\tValidation Loss: 195.792344, time5.2\n",
      "Epoch: 2244 \tTraining Loss: 55.457989 \tacc: 86.392332\tValidation Loss: 191.498954, time5.3\n",
      "Epoch: 2245 \tTraining Loss: 55.149985 \tacc: 87.970615\tValidation Loss: 187.234983, time5.3\n",
      "Epoch: 2246 \tTraining Loss: 57.872283 \tacc: 87.471304\tValidation Loss: 188.472202, time5.3\n",
      "Epoch: 2247 \tTraining Loss: 56.250000 \tacc: 86.914601\tValidation Loss: 192.622364, time5.3\n",
      "Epoch: 2248 \tTraining Loss: 55.955387 \tacc: 86.099633\tValidation Loss: 191.596960, time5.3\n",
      "Epoch: 2249 \tTraining Loss: 54.664065 \tacc: 85.950413\tValidation Loss: 197.019396, time5.2\n",
      "Epoch: 2250 \tTraining Loss: 55.077288 \tacc: 86.759642\tValidation Loss: 189.418291, time5.3\n",
      "Epoch: 2251 \tTraining Loss: 55.829125 \tacc: 86.713728\tValidation Loss: 194.291493, time5.3\n",
      "Epoch: 2252 \tTraining Loss: 55.710514 \tacc: 87.838613\tValidation Loss: 192.703566, time5.3\n",
      "Epoch: 2253 \tTraining Loss: 55.831038 \tacc: 86.082415\tValidation Loss: 201.985367, time5.3\n",
      "Epoch: 2254 \tTraining Loss: 54.916590 \tacc: 85.961892\tValidation Loss: 200.687726, time5.2\n",
      "Epoch: 2255 \tTraining Loss: 54.995026 \tacc: 86.352158\tValidation Loss: 200.042598, time5.3\n",
      "Epoch: 2256 \tTraining Loss: 55.006504 \tacc: 86.421028\tValidation Loss: 195.791827, time5.3\n",
      "Epoch: 2257 \tTraining Loss: 55.042853 \tacc: 86.914601\tValidation Loss: 198.907101, time5.3\n",
      "Epoch: 2258 \tTraining Loss: 55.033287 \tacc: 87.069559\tValidation Loss: 203.220148, time5.2\n",
      "Epoch: 2259 \tTraining Loss: 56.093128 \tacc: 87.086777\tValidation Loss: 189.820850, time5.3\n",
      "Epoch: 2260 \tTraining Loss: 55.612948 \tacc: 86.036501\tValidation Loss: 195.078454, time5.2\n",
      "Epoch: 2261 \tTraining Loss: 55.513468 \tacc: 86.053719\tValidation Loss: 194.453505, time5.3\n",
      "Epoch: 2262 \tTraining Loss: 55.040940 \tacc: 86.891644\tValidation Loss: 199.877829, time5.3\n",
      "Epoch: 2263 \tTraining Loss: 55.199725 \tacc: 87.276171\tValidation Loss: 193.227206, time5.3\n",
      "Epoch: 2264 \tTraining Loss: 54.903199 \tacc: 86.289027\tValidation Loss: 196.859277, time5.2\n",
      "Epoch: 2265 \tTraining Loss: 54.530150 \tacc: 86.507117\tValidation Loss: 200.670715, time5.3\n",
      "Epoch: 2266 \tTraining Loss: 55.291552 \tacc: 87.029385\tValidation Loss: 190.483486, time5.3\n",
      "Epoch: 2267 \tTraining Loss: 54.972069 \tacc: 86.088154\tValidation Loss: 200.690629, time5.3\n",
      "Epoch: 2268 \tTraining Loss: 54.499541 \tacc: 87.000689\tValidation Loss: 196.865748, time5.3\n",
      "Epoch: 2269 \tTraining Loss: 54.878329 \tacc: 87.207300\tValidation Loss: 204.639239, time5.3\n",
      "Epoch: 2270 \tTraining Loss: 56.221304 \tacc: 87.258953\tValidation Loss: 188.994021, time5.3\n",
      "Epoch: 2271 \tTraining Loss: 56.303566 \tacc: 86.839991\tValidation Loss: 185.126242, time5.3\n",
      "Epoch: 2272 \tTraining Loss: 55.968779 \tacc: 86.461203\tValidation Loss: 198.365959, time5.3\n",
      "Epoch: 2273 \tTraining Loss: 54.731022 \tacc: 86.357897\tValidation Loss: 199.109512, time5.3\n",
      "Epoch: 2274 \tTraining Loss: 54.740588 \tacc: 86.162764\tValidation Loss: 199.601569, time5.3\n",
      "Epoch: 2275 \tTraining Loss: 55.084940 \tacc: 86.174242\tValidation Loss: 200.080084, time5.2\n",
      "Epoch: 2276 \tTraining Loss: 55.084940 \tacc: 86.294766\tValidation Loss: 196.962835, time5.3\n",
      "Epoch: 2277 \tTraining Loss: 54.897459 \tacc: 86.294766\tValidation Loss: 200.341088, time5.3\n",
      "Epoch: 2278 \tTraining Loss: 55.869299 \tacc: 86.570248\tValidation Loss: 194.360820, time5.3\n",
      "Epoch: 2279 \tTraining Loss: 55.666514 \tacc: 86.673554\tValidation Loss: 193.862793, time5.3\n",
      "Epoch: 2280 \tTraining Loss: 56.010866 \tacc: 86.283287\tValidation Loss: 189.651940, time5.3\n",
      "Epoch: 2281 \tTraining Loss: 55.764080 \tacc: 86.289027\tValidation Loss: 191.786156, time5.3\n",
      "Epoch: 2282 \tTraining Loss: 54.916590 \tacc: 86.076676\tValidation Loss: 198.515821, time5.3\n",
      "Epoch: 2283 \tTraining Loss: 54.279538 \tacc: 86.553030\tValidation Loss: 197.264557, time5.3\n",
      "Epoch: 2284 \tTraining Loss: 55.016070 \tacc: 89.434114\tValidation Loss: 196.302149, time5.3\n",
      "Epoch: 2285 \tTraining Loss: 56.607744 \tacc: 86.443985\tValidation Loss: 197.520332, time5.3\n",
      "Epoch: 2286 \tTraining Loss: 54.051882 \tacc: 87.482782\tValidation Loss: 206.421350, time5.2\n",
      "Epoch: 2287 \tTraining Loss: 54.331191 \tacc: 85.961892\tValidation Loss: 202.958152, time5.3\n",
      "Epoch: 2288 \tTraining Loss: 54.447888 \tacc: 86.851469\tValidation Loss: 197.490754, time5.3\n",
      "Epoch: 2289 \tTraining Loss: 55.021809 \tacc: 86.943297\tValidation Loss: 194.442445, time5.2\n",
      "Epoch: 2290 \tTraining Loss: 55.471380 \tacc: 87.545914\tValidation Loss: 199.741160, time5.3\n",
      "Epoch: 2291 \tTraining Loss: 54.734848 \tacc: 86.047980\tValidation Loss: 200.545624, time5.3\n",
      "Epoch: 2292 \tTraining Loss: 55.000765 \tacc: 86.340680\tValidation Loss: 196.268432, time5.3\n",
      "Epoch: 2293 \tTraining Loss: 55.109810 \tacc: 87.385216\tValidation Loss: 192.903211, time5.3\n",
      "Epoch: 2294 \tTraining Loss: 54.170493 \tacc: 85.961892\tValidation Loss: 203.205319, time5.3\n",
      "Epoch: 2295 \tTraining Loss: 54.692761 \tacc: 86.478421\tValidation Loss: 198.792883, time5.3\n",
      "Epoch: 2296 \tTraining Loss: 54.742501 \tacc: 87.614784\tValidation Loss: 202.647463, time5.2\n",
      "Epoch: 2297 \tTraining Loss: 54.648760 \tacc: 86.093893\tValidation Loss: 203.619572, time5.3\n",
      "Epoch: 2298 \tTraining Loss: 55.161463 \tacc: 87.322084\tValidation Loss: 193.532634, time5.3\n",
      "Epoch: 2299 \tTraining Loss: 54.635369 \tacc: 86.828512\tValidation Loss: 195.034910, time5.3\n",
      "Epoch: 2300 \tTraining Loss: 56.368610 \tacc: 85.961892\tValidation Loss: 200.188948, time5.3\n",
      "Epoch: 2301 \tTraining Loss: 55.714340 \tacc: 86.289027\tValidation Loss: 199.072877, time5.3\n",
      "Epoch: 2302 \tTraining Loss: 54.644934 \tacc: 86.535813\tValidation Loss: 195.152781, time5.3\n",
      "Epoch: 2303 \tTraining Loss: 56.613483 \tacc: 85.829890\tValidation Loss: 199.673303, time5.3\n",
      "Epoch: 2304 \tTraining Loss: 55.088766 \tacc: 86.598944\tValidation Loss: 197.546190, time5.3\n",
      "Epoch: 2305 \tTraining Loss: 54.532063 \tacc: 86.587466\tValidation Loss: 200.087138, time5.3\n",
      "Epoch: 2306 \tTraining Loss: 55.088766 \tacc: 86.093893\tValidation Loss: 199.735091, time5.3\n",
      "Epoch: 2307 \tTraining Loss: 55.153811 \tacc: 86.553030\tValidation Loss: 197.631327, time5.2\n",
      "Epoch: 2308 \tTraining Loss: 54.549281 \tacc: 86.736685\tValidation Loss: 203.233296, time5.3\n",
      "Epoch: 2309 \tTraining Loss: 54.576064 \tacc: 87.132691\tValidation Loss: 199.623952, time5.3\n",
      "Epoch: 2310 \tTraining Loss: 54.375191 \tacc: 86.151286\tValidation Loss: 202.708014, time5.3\n",
      "Epoch: 2311 \tTraining Loss: 54.530150 \tacc: 87.103994\tValidation Loss: 198.653981, time5.3\n",
      "Epoch: 2312 \tTraining Loss: 55.192072 \tacc: 86.771120\tValidation Loss: 201.596020, time5.3\n",
      "Epoch: 2313 \tTraining Loss: 54.468932 \tacc: 86.862948\tValidation Loss: 196.298945, time5.3\n",
      "Epoch: 2314 \tTraining Loss: 55.301117 \tacc: 86.438246\tValidation Loss: 194.487389, time5.3\n",
      "Epoch: 2315 \tTraining Loss: 54.583716 \tacc: 86.604683\tValidation Loss: 202.336002, time5.3\n",
      "Epoch: 2316 \tTraining Loss: 55.620600 \tacc: 87.201561\tValidation Loss: 194.561876, time5.3\n",
      "Epoch: 2317 \tTraining Loss: 54.891720 \tacc: 86.386593\tValidation Loss: 197.019964, time5.3\n",
      "Epoch: 2318 \tTraining Loss: 55.083027 \tacc: 86.340680\tValidation Loss: 195.963023, time5.3\n",
      "Epoch: 2319 \tTraining Loss: 54.233624 \tacc: 86.484160\tValidation Loss: 202.542964, time5.3\n",
      "Epoch: 2320 \tTraining Loss: 54.952938 \tacc: 86.639118\tValidation Loss: 194.500246, time5.3\n",
      "Epoch: 2321 \tTraining Loss: 54.641108 \tacc: 86.587466\tValidation Loss: 197.973006, time5.3\n",
      "Epoch: 2322 \tTraining Loss: 54.553107 \tacc: 89.348026\tValidation Loss: 197.343576, time5.3\n",
      "Epoch: 2323 \tTraining Loss: 57.068794 \tacc: 86.518595\tValidation Loss: 190.872133, time5.3\n",
      "Epoch: 2324 \tTraining Loss: 55.314509 \tacc: 86.512856\tValidation Loss: 200.785207, time5.3\n",
      "Epoch: 2325 \tTraining Loss: 54.604760 \tacc: 87.115473\tValidation Loss: 194.968236, time5.3\n",
      "Epoch: 2326 \tTraining Loss: 54.876416 \tacc: 86.690771\tValidation Loss: 199.705068, time5.3\n",
      "Epoch: 2327 \tTraining Loss: 54.512932 \tacc: 87.465565\tValidation Loss: 202.093950, time5.3\n",
      "Epoch: 2328 \tTraining Loss: 55.100245 \tacc: 86.966253\tValidation Loss: 202.037043, time5.3\n",
      "Epoch: 2329 \tTraining Loss: 54.960591 \tacc: 86.415289\tValidation Loss: 200.978294, time5.2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2330 \tTraining Loss: 55.612948 \tacc: 85.944674\tValidation Loss: 200.510253, time5.3\n",
      "Epoch: 2331 \tTraining Loss: 55.159550 \tacc: 87.081038\tValidation Loss: 195.724358, time5.3\n",
      "Epoch: 2332 \tTraining Loss: 55.105984 \tacc: 86.208678\tValidation Loss: 198.793629, time5.3\n",
      "Epoch: 2333 \tTraining Loss: 55.513468 \tacc: 86.799816\tValidation Loss: 195.245230, time5.3\n",
      "Epoch: 2334 \tTraining Loss: 54.889807 \tacc: 86.598944\tValidation Loss: 198.345017, time5.3\n",
      "Epoch: 2335 \tTraining Loss: 55.985996 \tacc: 86.289027\tValidation Loss: 203.008911, time5.3\n",
      "Epoch: 2336 \tTraining Loss: 55.107897 \tacc: 86.311983\tValidation Loss: 197.768446, time5.3\n",
      "Epoch: 2337 \tTraining Loss: 55.094506 \tacc: 86.719467\tValidation Loss: 193.771431, time5.3\n",
      "Epoch: 2338 \tTraining Loss: 55.875038 \tacc: 86.111111\tValidation Loss: 195.218256, time5.3\n",
      "Epoch: 2339 \tTraining Loss: 55.880777 \tacc: 86.805556\tValidation Loss: 196.567200, time5.2\n",
      "Epoch: 2340 \tTraining Loss: 56.104607 \tacc: 87.178604\tValidation Loss: 189.533652, time5.3\n",
      "Epoch: 2341 \tTraining Loss: 55.873125 \tacc: 85.990588\tValidation Loss: 197.104099, time5.3\n",
      "Epoch: 2342 \tTraining Loss: 56.054867 \tacc: 87.029385\tValidation Loss: 193.111613, time5.3\n",
      "Epoch: 2343 \tTraining Loss: 55.848255 \tacc: 86.243113\tValidation Loss: 200.910026, time5.3\n",
      "Epoch: 2344 \tTraining Loss: 54.826676 \tacc: 86.707989\tValidation Loss: 204.329379, time5.3\n",
      "Epoch: 2345 \tTraining Loss: 55.153811 \tacc: 87.161387\tValidation Loss: 200.119920, time5.3\n",
      "Epoch: 2346 \tTraining Loss: 55.333639 \tacc: 87.017906\tValidation Loss: 193.358177, time5.3\n",
      "Epoch: 2347 \tTraining Loss: 54.269972 \tacc: 86.455464\tValidation Loss: 199.705361, time5.3\n",
      "Epoch: 2348 \tTraining Loss: 55.079201 \tacc: 87.442608\tValidation Loss: 190.848895, time5.3\n",
      "Epoch: 2349 \tTraining Loss: 56.341827 \tacc: 86.776860\tValidation Loss: 196.376580, time5.3\n",
      "Epoch: 2350 \tTraining Loss: 56.045301 \tacc: 86.908861\tValidation Loss: 194.242529, time5.2\n",
      "Epoch: 2351 \tTraining Loss: 55.335552 \tacc: 86.667815\tValidation Loss: 200.707262, time5.3\n",
      "Epoch: 2352 \tTraining Loss: 54.233624 \tacc: 86.432507\tValidation Loss: 201.041137, time5.3\n",
      "Epoch: 2353 \tTraining Loss: 54.317799 \tacc: 86.742424\tValidation Loss: 200.394026, time5.3\n",
      "Epoch: 2354 \tTraining Loss: 55.046679 \tacc: 87.654959\tValidation Loss: 199.373776, time5.3\n",
      "Epoch: 2355 \tTraining Loss: 55.295378 \tacc: 85.990588\tValidation Loss: 204.413993, time5.3\n",
      "Epoch: 2356 \tTraining Loss: 54.203015 \tacc: 85.766758\tValidation Loss: 205.190561, time5.3\n",
      "Epoch: 2357 \tTraining Loss: 54.535889 \tacc: 86.593205\tValidation Loss: 200.030943, time5.3\n",
      "Epoch: 2358 \tTraining Loss: 54.794154 \tacc: 87.213039\tValidation Loss: 199.605957, time5.3\n",
      "Epoch: 2359 \tTraining Loss: 54.639195 \tacc: 86.776860\tValidation Loss: 202.109223, time5.3\n",
      "Epoch: 2360 \tTraining Loss: 54.264233 \tacc: 86.679293\tValidation Loss: 205.070856, time5.3\n",
      "Epoch: 2361 \tTraining Loss: 54.679369 \tacc: 86.380854\tValidation Loss: 202.468953, time5.2\n",
      "Epoch: 2362 \tTraining Loss: 54.421105 \tacc: 86.713728\tValidation Loss: 211.071586, time5.3\n",
      "Epoch: 2363 \tTraining Loss: 54.832415 \tacc: 86.438246\tValidation Loss: 201.560398, time5.3\n",
      "Epoch: 2364 \tTraining Loss: 55.586165 \tacc: 86.134068\tValidation Loss: 201.157089, time5.3\n",
      "Epoch: 2365 \tTraining Loss: 55.025635 \tacc: 86.271809\tValidation Loss: 197.489045, time5.3\n",
      "Epoch: 2366 \tTraining Loss: 54.979721 \tacc: 86.231635\tValidation Loss: 203.093000, time5.3\n",
      "Epoch: 2367 \tTraining Loss: 54.794154 \tacc: 86.300505\tValidation Loss: 206.461884, time5.3\n",
      "Epoch: 2368 \tTraining Loss: 54.931895 \tacc: 86.243113\tValidation Loss: 199.582081, time5.3\n",
      "Epoch: 2369 \tTraining Loss: 54.505280 \tacc: 86.237374\tValidation Loss: 200.703219, time5.3\n",
      "Epoch: 2370 \tTraining Loss: 55.299204 \tacc: 86.851469\tValidation Loss: 195.547861, time5.3\n",
      "Epoch: 2371 \tTraining Loss: 54.784588 \tacc: 87.103994\tValidation Loss: 198.013415, time5.2\n",
      "Epoch: 2372 \tTraining Loss: 54.805632 \tacc: 86.518595\tValidation Loss: 204.950198, time5.3\n",
      "Epoch: 2373 \tTraining Loss: 54.287190 \tacc: 87.448347\tValidation Loss: 202.888523, time5.2\n",
      "Epoch: 2374 \tTraining Loss: 55.172942 \tacc: 87.907484\tValidation Loss: 202.508511, time5.2\n",
      "Epoch: 2375 \tTraining Loss: 54.143710 \tacc: 86.748163\tValidation Loss: 205.505983, time5.3\n",
      "Epoch: 2376 \tTraining Loss: 55.341292 \tacc: 86.443985\tValidation Loss: 202.004641, time5.3\n",
      "Epoch: 2377 \tTraining Loss: 54.778849 \tacc: 86.765381\tValidation Loss: 202.495599, time5.3\n",
      "Epoch: 2378 \tTraining Loss: 55.096419 \tacc: 86.036501\tValidation Loss: 202.338585, time5.3\n",
      "Epoch: 2379 \tTraining Loss: 55.071549 \tacc: 88.154270\tValidation Loss: 201.595505, time5.3\n",
      "Epoch: 2380 \tTraining Loss: 55.174855 \tacc: 86.145546\tValidation Loss: 205.712910, time5.3\n",
      "Epoch: 2381 \tTraining Loss: 55.452250 \tacc: 86.191460\tValidation Loss: 208.156399, time5.3\n",
      "Epoch: 2382 \tTraining Loss: 54.926155 \tacc: 86.713728\tValidation Loss: 205.660963, time5.2\n",
      "Epoch: 2383 \tTraining Loss: 55.228421 \tacc: 86.535813\tValidation Loss: 204.233392, time5.3\n",
      "Epoch: 2384 \tTraining Loss: 54.832415 \tacc: 87.350781\tValidation Loss: 198.517698, time5.2\n",
      "Epoch: 2385 \tTraining Loss: 56.632614 \tacc: 86.736685\tValidation Loss: 196.855146, time5.3\n",
      "Epoch: 2386 \tTraining Loss: 55.542164 \tacc: 87.683655\tValidation Loss: 196.923007, time5.3\n",
      "Epoch: 2387 \tTraining Loss: 55.848255 \tacc: 87.310606\tValidation Loss: 199.259578, time5.3\n",
      "Epoch: 2388 \tTraining Loss: 55.511555 \tacc: 87.718090\tValidation Loss: 192.555698, time5.3\n",
      "Epoch: 2389 \tTraining Loss: 55.756428 \tacc: 86.553030\tValidation Loss: 203.496570, time5.3\n",
      "Epoch: 2390 \tTraining Loss: 55.260943 \tacc: 86.696511\tValidation Loss: 201.158842, time5.3\n",
      "Epoch: 2391 \tTraining Loss: 54.924242 \tacc: 86.099633\tValidation Loss: 206.007698, time5.3\n",
      "Epoch: 2392 \tTraining Loss: 54.621977 \tacc: 85.921717\tValidation Loss: 201.408131, time5.3\n",
      "Epoch: 2393 \tTraining Loss: 54.637282 \tacc: 87.276171\tValidation Loss: 201.405502, time5.2\n",
      "Epoch: 2394 \tTraining Loss: 54.644934 \tacc: 87.052342\tValidation Loss: 203.991115, time5.3\n",
      "Epoch: 2395 \tTraining Loss: 54.359887 \tacc: 87.230257\tValidation Loss: 201.196648, time5.3\n",
      "Epoch: 2396 \tTraining Loss: 54.972069 \tacc: 86.696511\tValidation Loss: 201.940805, time5.3\n",
      "Epoch: 2397 \tTraining Loss: 55.209290 \tacc: 86.415289\tValidation Loss: 201.758742, time5.3\n",
      "Epoch: 2398 \tTraining Loss: 55.042853 \tacc: 86.283287\tValidation Loss: 200.520653, time5.3\n",
      "Epoch: 2399 \tTraining Loss: 54.233624 \tacc: 86.575987\tValidation Loss: 198.764887, time5.2\n",
      "Epoch: 2400 \tTraining Loss: 55.019896 \tacc: 87.161387\tValidation Loss: 198.937603, time5.3\n",
      "Epoch: 2401 \tTraining Loss: 55.764080 \tacc: 86.145546\tValidation Loss: 205.516854, time5.3\n",
      "Epoch: 2402 \tTraining Loss: 55.201638 \tacc: 86.667815\tValidation Loss: 199.971150, time5.3\n",
      "Epoch: 2403 \tTraining Loss: 56.500612 \tacc: 86.317723\tValidation Loss: 199.625573, time5.3\n",
      "Epoch: 2404 \tTraining Loss: 55.413988 \tacc: 86.243113\tValidation Loss: 205.268973, time5.2\n",
      "Epoch: 2405 \tTraining Loss: 55.023722 \tacc: 86.679293\tValidation Loss: 199.034214, time5.3\n",
      "Epoch: 2406 \tTraining Loss: 55.645470 \tacc: 85.720845\tValidation Loss: 205.024662, time5.3\n",
      "Epoch: 2407 \tTraining Loss: 54.901286 \tacc: 86.174242\tValidation Loss: 206.373576, time5.3\n",
      "Epoch: 2408 \tTraining Loss: 54.778849 \tacc: 86.116850\tValidation Loss: 199.072780, time5.3\n",
      "Epoch: 2409 \tTraining Loss: 54.600934 \tacc: 85.973370\tValidation Loss: 202.364646, time5.3\n",
      "Epoch: 2410 \tTraining Loss: 55.117463 \tacc: 85.984848\tValidation Loss: 200.818128, time5.3\n",
      "Epoch: 2411 \tTraining Loss: 55.123202 \tacc: 85.996327\tValidation Loss: 199.820418, time5.3\n",
      "Epoch: 2412 \tTraining Loss: 54.763545 \tacc: 86.466942\tValidation Loss: 200.797610, time5.3\n",
      "Epoch: 2413 \tTraining Loss: 54.972069 \tacc: 86.828512\tValidation Loss: 199.360246, time5.3\n",
      "Epoch: 2414 \tTraining Loss: 54.407713 \tacc: 86.753903\tValidation Loss: 206.890291, time5.2\n",
      "Epoch: 2415 \tTraining Loss: 54.262320 \tacc: 86.070937\tValidation Loss: 206.613086, time5.3\n",
      "Epoch: 2416 \tTraining Loss: 55.040940 \tacc: 85.961892\tValidation Loss: 194.938635, time5.3\n",
      "Epoch: 2417 \tTraining Loss: 55.132767 \tacc: 86.357897\tValidation Loss: 195.571555, time5.3\n",
      "Epoch: 2418 \tTraining Loss: 54.556933 \tacc: 87.052342\tValidation Loss: 202.060558, time5.3\n",
      "Epoch: 2419 \tTraining Loss: 55.647383 \tacc: 87.356520\tValidation Loss: 198.125145, time5.3\n",
      "Epoch: 2420 \tTraining Loss: 55.245638 \tacc: 86.426768\tValidation Loss: 197.912103, time5.3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2421 \tTraining Loss: 55.614861 \tacc: 86.208678\tValidation Loss: 192.735066, time5.3\n",
      "Epoch: 2422 \tTraining Loss: 55.392945 \tacc: 86.398072\tValidation Loss: 198.715649, time5.3\n",
      "Epoch: 2423 \tTraining Loss: 55.362335 \tacc: 85.910239\tValidation Loss: 198.049875, time5.3\n",
      "Epoch: 2424 \tTraining Loss: 55.364249 \tacc: 86.139807\tValidation Loss: 199.933449, time5.3\n",
      "Epoch: 2425 \tTraining Loss: 55.597643 \tacc: 86.771120\tValidation Loss: 188.481557, time5.2\n",
      "Epoch: 2426 \tTraining Loss: 55.716253 \tacc: 86.237374\tValidation Loss: 203.779688, time5.3\n",
      "Epoch: 2427 \tTraining Loss: 55.375727 \tacc: 86.289027\tValidation Loss: 199.399483, time5.3\n",
      "Epoch: 2428 \tTraining Loss: 55.366162 \tacc: 86.564509\tValidation Loss: 198.897483, time5.3\n",
      "Epoch: 2429 \tTraining Loss: 55.957300 \tacc: 86.644858\tValidation Loss: 193.361323, time5.3\n",
      "Epoch: 2430 \tTraining Loss: 55.574686 \tacc: 86.598944\tValidation Loss: 198.467250, time5.3\n",
      "Epoch: 2431 \tTraining Loss: 54.866850 \tacc: 86.461203\tValidation Loss: 196.541919, time5.3\n",
      "Epoch: 2432 \tTraining Loss: 55.731558 \tacc: 87.614784\tValidation Loss: 200.126815, time5.3\n",
      "Epoch: 2433 \tTraining Loss: 55.551729 \tacc: 86.260331\tValidation Loss: 199.719550, time5.3\n",
      "Epoch: 2434 \tTraining Loss: 55.660774 \tacc: 87.155647\tValidation Loss: 197.220022, time5.3\n",
      "Epoch: 2435 \tTraining Loss: 55.230334 \tacc: 86.375115\tValidation Loss: 199.203965, time5.3\n",
      "Epoch: 2436 \tTraining Loss: 54.797980 \tacc: 85.933196\tValidation Loss: 204.851821, time5.2\n",
      "Epoch: 2437 \tTraining Loss: 54.845807 \tacc: 86.168503\tValidation Loss: 203.646323, time5.3\n",
      "Epoch: 2438 \tTraining Loss: 55.063897 \tacc: 86.323462\tValidation Loss: 202.773103, time5.3\n",
      "Epoch: 2439 \tTraining Loss: 55.071549 \tacc: 86.289027\tValidation Loss: 202.915211, time5.3\n",
      "Epoch: 2440 \tTraining Loss: 54.893633 \tacc: 86.128329\tValidation Loss: 195.707430, time5.3\n",
      "Epoch: 2441 \tTraining Loss: 55.004591 \tacc: 86.357897\tValidation Loss: 198.656252, time5.3\n",
      "Epoch: 2442 \tTraining Loss: 55.199725 \tacc: 86.346419\tValidation Loss: 205.581809, time5.3\n",
      "Epoch: 2443 \tTraining Loss: 54.746327 \tacc: 86.679293\tValidation Loss: 199.806437, time5.3\n",
      "Epoch: 2444 \tTraining Loss: 55.496250 \tacc: 86.317723\tValidation Loss: 196.176020, time5.3\n",
      "Epoch: 2445 \tTraining Loss: 55.632078 \tacc: 86.679293\tValidation Loss: 196.291243, time5.3\n",
      "Epoch: 2446 \tTraining Loss: 54.853459 \tacc: 86.484160\tValidation Loss: 205.300478, time5.2\n",
      "Epoch: 2447 \tTraining Loss: 55.153811 \tacc: 86.495638\tValidation Loss: 201.710793, time5.3\n",
      "Epoch: 2448 \tTraining Loss: 55.073462 \tacc: 86.283287\tValidation Loss: 204.022896, time5.3\n",
      "Epoch: 2449 \tTraining Loss: 54.692761 \tacc: 86.225895\tValidation Loss: 207.406318, time5.3\n",
      "Epoch: 2450 \tTraining Loss: 55.899908 \tacc: 87.855831\tValidation Loss: 199.181053, time5.3\n",
      "Epoch: 2451 \tTraining Loss: 57.145317 \tacc: 86.225895\tValidation Loss: 197.744211, time5.3\n",
      "Epoch: 2452 \tTraining Loss: 55.132767 \tacc: 86.598944\tValidation Loss: 208.586149, time5.3\n",
      "Epoch: 2453 \tTraining Loss: 54.935721 \tacc: 86.742424\tValidation Loss: 200.631772, time5.3\n",
      "Epoch: 2454 \tTraining Loss: 54.945286 \tacc: 86.202938\tValidation Loss: 199.772701, time5.3\n",
      "Epoch: 2455 \tTraining Loss: 54.954852 \tacc: 86.713728\tValidation Loss: 203.880603, time5.3\n",
      "Epoch: 2456 \tTraining Loss: 54.614325 \tacc: 86.587466\tValidation Loss: 202.646446, time5.3\n",
      "Epoch: 2457 \tTraining Loss: 56.041475 \tacc: 86.891644\tValidation Loss: 202.194857, time5.3\n",
      "Epoch: 2458 \tTraining Loss: 55.859734 \tacc: 86.231635\tValidation Loss: 201.347863, time5.3\n",
      "Epoch: 2459 \tTraining Loss: 54.606673 \tacc: 87.367998\tValidation Loss: 203.619734, time5.3\n",
      "Epoch: 2460 \tTraining Loss: 54.947199 \tacc: 86.093893\tValidation Loss: 202.923021, time5.3\n",
      "Epoch: 2461 \tTraining Loss: 54.665978 \tacc: 86.300505\tValidation Loss: 209.729304, time5.3\n",
      "Epoch: 2462 \tTraining Loss: 54.442149 \tacc: 86.564509\tValidation Loss: 203.108295, time5.3\n",
      "Epoch: 2463 \tTraining Loss: 54.478497 \tacc: 86.822773\tValidation Loss: 205.507107, time5.3\n",
      "Epoch: 2464 \tTraining Loss: 54.675543 \tacc: 86.426768\tValidation Loss: 207.528445, time5.3\n",
      "Epoch: 2465 \tTraining Loss: 54.863024 \tacc: 87.241736\tValidation Loss: 201.913754, time5.3\n",
      "Epoch: 2466 \tTraining Loss: 54.562672 \tacc: 86.225895\tValidation Loss: 206.754986, time5.3\n",
      "Epoch: 2467 \tTraining Loss: 54.845807 \tacc: 86.558770\tValidation Loss: 206.924284, time5.3\n",
      "Epoch: 2468 \tTraining Loss: 55.469467 \tacc: 86.311983\tValidation Loss: 203.313312, time5.2\n",
      "Epoch: 2469 \tTraining Loss: 54.740588 \tacc: 87.109734\tValidation Loss: 200.628652, time5.3\n",
      "Epoch: 2470 \tTraining Loss: 54.530150 \tacc: 86.748163\tValidation Loss: 199.542173, time5.3\n",
      "Epoch: 2471 \tTraining Loss: 55.163376 \tacc: 86.736685\tValidation Loss: 198.260340, time5.3\n",
      "Epoch: 2472 \tTraining Loss: 54.335017 \tacc: 86.300505\tValidation Loss: 209.416930, time5.3\n",
      "Epoch: 2473 \tTraining Loss: 54.474671 \tacc: 87.884527\tValidation Loss: 207.192095, time5.3\n",
      "Epoch: 2474 \tTraining Loss: 55.632078 \tacc: 86.139807\tValidation Loss: 211.389329, time5.3\n",
      "Epoch: 2475 \tTraining Loss: 55.098332 \tacc: 86.461203\tValidation Loss: 203.199250, time5.3\n",
      "Epoch: 2476 \tTraining Loss: 54.499541 \tacc: 86.472681\tValidation Loss: 206.777025, time5.3\n",
      "Epoch: 2477 \tTraining Loss: 54.243189 \tacc: 86.466942\tValidation Loss: 205.308293, time5.3\n",
      "Epoch: 2478 \tTraining Loss: 54.987374 \tacc: 86.231635\tValidation Loss: 206.287528, time5.3\n",
      "Epoch: 2479 \tTraining Loss: 55.899908 \tacc: 87.195822\tValidation Loss: 204.179310, time5.2\n",
      "Epoch: 2480 \tTraining Loss: 56.663223 \tacc: 86.116850\tValidation Loss: 203.317124, time5.3\n",
      "Epoch: 2481 \tTraining Loss: 54.843893 \tacc: 86.455464\tValidation Loss: 205.725127, time5.3\n",
      "Epoch: 2482 \tTraining Loss: 55.293465 \tacc: 86.185721\tValidation Loss: 204.820629, time5.3\n",
      "Epoch: 2483 \tTraining Loss: 56.531221 \tacc: 86.380854\tValidation Loss: 198.817050, time5.3\n",
      "Epoch: 2484 \tTraining Loss: 54.748240 \tacc: 87.683655\tValidation Loss: 204.617793, time5.3\n",
      "Epoch: 2485 \tTraining Loss: 54.679369 \tacc: 87.000689\tValidation Loss: 210.074159, time5.3\n",
      "Epoch: 2486 \tTraining Loss: 54.773110 \tacc: 86.885904\tValidation Loss: 203.097924, time5.3\n",
      "Epoch: 2487 \tTraining Loss: 56.678528 \tacc: 86.805556\tValidation Loss: 197.388517, time5.3\n",
      "Epoch: 2488 \tTraining Loss: 55.429293 \tacc: 86.111111\tValidation Loss: 199.752879, time5.3\n",
      "Epoch: 2489 \tTraining Loss: 54.941460 \tacc: 86.340680\tValidation Loss: 199.622812, time5.2\n",
      "Epoch: 2490 \tTraining Loss: 55.203551 \tacc: 86.323462\tValidation Loss: 205.141706, time5.3\n",
      "Epoch: 2491 \tTraining Loss: 54.514845 \tacc: 86.271809\tValidation Loss: 205.093042, time5.3\n",
      "Epoch: 2492 \tTraining Loss: 55.553642 \tacc: 86.650597\tValidation Loss: 205.115243, time5.3\n",
      "Epoch: 2493 \tTraining Loss: 55.932430 \tacc: 87.075298\tValidation Loss: 207.626281, time5.3\n",
      "Epoch: 2494 \tTraining Loss: 55.243725 \tacc: 87.081038\tValidation Loss: 201.964433, time5.3\n",
      "Epoch: 2495 \tTraining Loss: 55.096419 \tacc: 86.139807\tValidation Loss: 210.509423, time5.3\n",
      "Epoch: 2496 \tTraining Loss: 54.478497 \tacc: 86.581726\tValidation Loss: 202.389634, time5.3\n",
      "Epoch: 2497 \tTraining Loss: 54.413453 \tacc: 86.673554\tValidation Loss: 202.252975, time5.3\n",
      "Epoch: 2498 \tTraining Loss: 54.616238 \tacc: 86.811295\tValidation Loss: 203.810247, time5.3\n",
      "Epoch: 2499 \tTraining Loss: 54.516758 \tacc: 85.703627\tValidation Loss: 214.849173, time5.3\n",
      "Epoch: 2500 \tTraining Loss: 54.287190 \tacc: 86.111111\tValidation Loss: 209.492312, time5.2\n",
      "Epoch: 2501 \tTraining Loss: 54.778849 \tacc: 86.908861\tValidation Loss: 207.594662, time5.3\n",
      "Epoch: 2502 \tTraining Loss: 54.996939 \tacc: 86.294766\tValidation Loss: 204.476331, time5.3\n",
      "Epoch: 2503 \tTraining Loss: 54.977808 \tacc: 86.581726\tValidation Loss: 199.096195, time5.3\n",
      "Epoch: 2504 \tTraining Loss: 55.111723 \tacc: 88.688017\tValidation Loss: 198.413447, time5.3\n",
      "Epoch: 2505 \tTraining Loss: 55.467554 \tacc: 86.937557\tValidation Loss: 207.975224, time5.3\n",
      "Epoch: 2506 \tTraining Loss: 55.293465 \tacc: 86.030762\tValidation Loss: 212.233734, time5.3\n",
      "Epoch: 2507 \tTraining Loss: 53.828053 \tacc: 86.564509\tValidation Loss: 205.096492, time5.3\n",
      "Epoch: 2508 \tTraining Loss: 54.074839 \tacc: 86.248852\tValidation Loss: 206.749632, time5.3\n",
      "Epoch: 2509 \tTraining Loss: 55.159550 \tacc: 86.306244\tValidation Loss: 201.897355, time5.3\n",
      "Epoch: 2510 \tTraining Loss: 54.765458 \tacc: 86.116850\tValidation Loss: 201.297264, time5.3\n",
      "Epoch: 2511 \tTraining Loss: 54.463193 \tacc: 86.082415\tValidation Loss: 211.495039, time5.2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2512 \tTraining Loss: 54.176232 \tacc: 85.818411\tValidation Loss: 208.520374, time5.3\n",
      "Epoch: 2513 \tTraining Loss: 55.352770 \tacc: 86.352158\tValidation Loss: 201.685711, time5.3\n",
      "Epoch: 2514 \tTraining Loss: 56.433655 \tacc: 86.340680\tValidation Loss: 199.768848, time5.3\n",
      "Epoch: 2515 \tTraining Loss: 54.864937 \tacc: 86.466942\tValidation Loss: 206.188695, time5.3\n",
      "Epoch: 2516 \tTraining Loss: 53.942837 \tacc: 86.260331\tValidation Loss: 215.452692, time5.3\n",
      "Epoch: 2517 \tTraining Loss: 54.685109 \tacc: 86.277548\tValidation Loss: 205.160405, time5.3\n",
      "Epoch: 2518 \tTraining Loss: 54.681283 \tacc: 87.017906\tValidation Loss: 209.294229, time5.3\n",
      "Epoch: 2519 \tTraining Loss: 55.329813 \tacc: 86.438246\tValidation Loss: 203.120467, time5.3\n",
      "Epoch: 2520 \tTraining Loss: 54.895546 \tacc: 86.122590\tValidation Loss: 204.384011, time5.3\n",
      "Epoch: 2521 \tTraining Loss: 54.348408 \tacc: 87.482782\tValidation Loss: 204.220412, time5.2\n",
      "Epoch: 2522 \tTraining Loss: 55.083027 \tacc: 87.006428\tValidation Loss: 208.760322, time5.3\n",
      "Epoch: 2523 \tTraining Loss: 54.616238 \tacc: 85.875803\tValidation Loss: 205.025248, time5.3\n",
      "Epoch: 2524 \tTraining Loss: 54.547368 \tacc: 89.129936\tValidation Loss: 195.803318, time5.3\n",
      "Epoch: 2525 \tTraining Loss: 56.073998 \tacc: 86.753903\tValidation Loss: 206.550019, time5.3\n",
      "Epoch: 2526 \tTraining Loss: 55.880777 \tacc: 87.327824\tValidation Loss: 203.125894, time5.3\n",
      "Epoch: 2527 \tTraining Loss: 55.976431 \tacc: 85.979109\tValidation Loss: 203.260821, time5.3\n",
      "Epoch: 2528 \tTraining Loss: 54.759718 \tacc: 87.075298\tValidation Loss: 206.534802, time5.3\n",
      "Epoch: 2529 \tTraining Loss: 54.843893 \tacc: 86.036501\tValidation Loss: 210.652322, time5.3\n",
      "Epoch: 2530 \tTraining Loss: 56.085476 \tacc: 86.662075\tValidation Loss: 195.690868, time5.3\n",
      "Epoch: 2531 \tTraining Loss: 54.247016 \tacc: 86.598944\tValidation Loss: 204.044931, time5.3\n",
      "Epoch: 2532 \tTraining Loss: 54.097796 \tacc: 86.134068\tValidation Loss: 204.757667, time5.2\n",
      "Epoch: 2533 \tTraining Loss: 54.390496 \tacc: 86.702250\tValidation Loss: 213.631686, time5.3\n",
      "Epoch: 2534 \tTraining Loss: 54.977808 \tacc: 86.403811\tValidation Loss: 205.256862, time5.3\n",
      "Epoch: 2535 \tTraining Loss: 54.767371 \tacc: 86.639118\tValidation Loss: 206.300678, time5.3\n",
      "Epoch: 2536 \tTraining Loss: 54.729109 \tacc: 85.761019\tValidation Loss: 205.158064, time5.3\n",
      "Epoch: 2537 \tTraining Loss: 54.336930 \tacc: 85.984848\tValidation Loss: 204.588196, time5.3\n",
      "Epoch: 2538 \tTraining Loss: 53.810836 \tacc: 87.000689\tValidation Loss: 205.222339, time5.3\n",
      "Epoch: 2539 \tTraining Loss: 55.381466 \tacc: 86.191460\tValidation Loss: 206.495635, time5.3\n",
      "Epoch: 2540 \tTraining Loss: 54.428757 \tacc: 86.501377\tValidation Loss: 208.932425, time5.3\n",
      "Epoch: 2541 \tTraining Loss: 54.143710 \tacc: 86.438246\tValidation Loss: 208.298507, time5.3\n",
      "Epoch: 2542 \tTraining Loss: 55.454163 \tacc: 86.725207\tValidation Loss: 201.889038, time5.3\n",
      "Epoch: 2543 \tTraining Loss: 55.320248 \tacc: 86.093893\tValidation Loss: 210.706848, time5.3\n",
      "Epoch: 2544 \tTraining Loss: 54.296755 \tacc: 87.132691\tValidation Loss: 207.211626, time5.3\n",
      "Epoch: 2545 \tTraining Loss: 54.641108 \tacc: 87.000689\tValidation Loss: 211.480152, time5.3\n",
      "Epoch: 2546 \tTraining Loss: 54.734848 \tacc: 86.719467\tValidation Loss: 205.314490, time5.3\n",
      "Epoch: 2547 \tTraining Loss: 56.284435 \tacc: 86.019284\tValidation Loss: 198.964083, time5.3\n",
      "Epoch: 2548 \tTraining Loss: 55.549816 \tacc: 86.713728\tValidation Loss: 197.252203, time5.3\n",
      "Epoch: 2549 \tTraining Loss: 55.071549 \tacc: 86.225895\tValidation Loss: 202.835485, time5.3\n",
      "Epoch: 2550 \tTraining Loss: 55.364249 \tacc: 88.177227\tValidation Loss: 197.487165, time5.3\n",
      "Epoch: 2551 \tTraining Loss: 57.369146 \tacc: 86.300505\tValidation Loss: 197.934164, time5.3\n",
      "Epoch: 2552 \tTraining Loss: 55.765993 \tacc: 86.421028\tValidation Loss: 203.725302, time5.3\n",
      "Epoch: 2553 \tTraining Loss: 55.785124 \tacc: 86.857208\tValidation Loss: 199.028998, time5.3\n",
      "Epoch: 2554 \tTraining Loss: 55.297291 \tacc: 86.208678\tValidation Loss: 205.670299, time5.2\n",
      "Epoch: 2555 \tTraining Loss: 54.113101 \tacc: 86.466942\tValidation Loss: 206.238362, time5.3\n",
      "Epoch: 2556 \tTraining Loss: 54.499541 \tacc: 86.558770\tValidation Loss: 204.446609, time5.3\n",
      "Epoch: 2557 \tTraining Loss: 54.975895 \tacc: 86.283287\tValidation Loss: 197.015880, time5.3\n",
      "Epoch: 2558 \tTraining Loss: 54.170493 \tacc: 86.914601\tValidation Loss: 207.970891, time5.3\n",
      "Epoch: 2559 \tTraining Loss: 55.318335 \tacc: 86.352158\tValidation Loss: 204.037460, time5.3\n",
      "Epoch: 2560 \tTraining Loss: 53.956229 \tacc: 87.580349\tValidation Loss: 203.675283, time5.3\n",
      "Epoch: 2561 \tTraining Loss: 56.431742 \tacc: 86.403811\tValidation Loss: 205.564193, time5.3\n",
      "Epoch: 2562 \tTraining Loss: 54.411540 \tacc: 86.134068\tValidation Loss: 206.283243, time5.3\n",
      "Epoch: 2563 \tTraining Loss: 54.331191 \tacc: 85.801194\tValidation Loss: 210.136409, time5.3\n",
      "Epoch: 2564 \tTraining Loss: 54.637282 \tacc: 85.996327\tValidation Loss: 207.444939, time5.2\n",
      "Epoch: 2565 \tTraining Loss: 53.399526 \tacc: 85.967631\tValidation Loss: 207.085815, time5.3\n",
      "Epoch: 2566 \tTraining Loss: 54.644934 \tacc: 86.817034\tValidation Loss: 209.999946, time5.3\n",
      "Epoch: 2567 \tTraining Loss: 54.530150 \tacc: 87.425390\tValidation Loss: 204.542404, time5.3\n",
      "Epoch: 2568 \tTraining Loss: 54.951025 \tacc: 86.426768\tValidation Loss: 204.057122, time5.3\n",
      "Epoch: 2569 \tTraining Loss: 54.868763 \tacc: 86.329201\tValidation Loss: 201.769516, time5.3\n",
      "Epoch: 2570 \tTraining Loss: 55.431206 \tacc: 85.870064\tValidation Loss: 208.162138, time5.3\n",
      "Epoch: 2571 \tTraining Loss: 54.882155 \tacc: 85.847107\tValidation Loss: 206.573123, time5.3\n",
      "Epoch: 2572 \tTraining Loss: 54.564585 \tacc: 86.845730\tValidation Loss: 204.389737, time5.3\n",
      "Epoch: 2573 \tTraining Loss: 55.176768 \tacc: 86.392332\tValidation Loss: 210.643868, time5.3\n",
      "Epoch: 2574 \tTraining Loss: 54.547368 \tacc: 87.058081\tValidation Loss: 201.906585, time5.3\n",
      "Epoch: 2575 \tTraining Loss: 55.515381 \tacc: 86.099633\tValidation Loss: 210.662159, time5.2\n",
      "Epoch: 2576 \tTraining Loss: 54.080579 \tacc: 85.990588\tValidation Loss: 212.956629, time5.3\n",
      "Epoch: 2577 \tTraining Loss: 55.197811 \tacc: 86.461203\tValidation Loss: 205.640949, time5.3\n",
      "Epoch: 2578 \tTraining Loss: 55.515381 \tacc: 86.541552\tValidation Loss: 211.391853, time5.3\n",
      "Epoch: 2579 \tTraining Loss: 54.767371 \tacc: 86.604683\tValidation Loss: 213.387512, time5.3\n",
      "Epoch: 2580 \tTraining Loss: 54.572238 \tacc: 86.266070\tValidation Loss: 213.974757, time5.3\n",
      "Epoch: 2581 \tTraining Loss: 55.375727 \tacc: 86.811295\tValidation Loss: 209.232450, time5.3\n",
      "Epoch: 2582 \tTraining Loss: 54.170493 \tacc: 86.053719\tValidation Loss: 210.910190, time5.3\n",
      "Epoch: 2583 \tTraining Loss: 54.159014 \tacc: 86.530073\tValidation Loss: 206.107309, time5.3\n",
      "Epoch: 2584 \tTraining Loss: 54.556933 \tacc: 86.323462\tValidation Loss: 205.117404, time5.3\n",
      "Epoch: 2585 \tTraining Loss: 54.665978 \tacc: 86.231635\tValidation Loss: 208.189720, time5.3\n",
      "Epoch: 2586 \tTraining Loss: 55.544077 \tacc: 86.329201\tValidation Loss: 207.336272, time5.2\n",
      "Epoch: 2587 \tTraining Loss: 54.092057 \tacc: 86.116850\tValidation Loss: 211.735790, time5.3\n",
      "Epoch: 2588 \tTraining Loss: 54.941460 \tacc: 87.195822\tValidation Loss: 205.504071, time5.3\n",
      "Epoch: 2589 \tTraining Loss: 54.656413 \tacc: 87.982094\tValidation Loss: 201.454881, time5.3\n",
      "Epoch: 2590 \tTraining Loss: 55.314509 \tacc: 86.524334\tValidation Loss: 207.884355, time5.3\n",
      "Epoch: 2591 \tTraining Loss: 54.417279 \tacc: 86.134068\tValidation Loss: 214.051986, time5.3\n",
      "Epoch: 2592 \tTraining Loss: 53.891185 \tacc: 86.742424\tValidation Loss: 210.705952, time5.3\n",
      "Epoch: 2593 \tTraining Loss: 55.004591 \tacc: 86.885904\tValidation Loss: 215.088274, time5.3\n",
      "Epoch: 2594 \tTraining Loss: 55.144245 \tacc: 86.139807\tValidation Loss: 212.798521, time5.3\n",
      "Epoch: 2595 \tTraining Loss: 54.373278 \tacc: 86.799816\tValidation Loss: 199.428064, time5.3\n",
      "Epoch: 2596 \tTraining Loss: 56.877487 \tacc: 86.329201\tValidation Loss: 195.781381, time5.2\n",
      "Epoch: 2597 \tTraining Loss: 56.058693 \tacc: 86.621901\tValidation Loss: 202.548309, time5.3\n",
      "Epoch: 2598 \tTraining Loss: 56.563743 \tacc: 86.616162\tValidation Loss: 199.100953, time5.3\n",
      "Epoch: 2599 \tTraining Loss: 56.661310 \tacc: 85.967631\tValidation Loss: 203.434720, time5.3\n",
      "Epoch: 2600 \tTraining Loss: 56.517830 \tacc: 86.650597\tValidation Loss: 201.997942, time5.3\n",
      "Epoch: 2601 \tTraining Loss: 54.509106 \tacc: 87.121212\tValidation Loss: 213.385310, time5.3\n",
      "Epoch: 2602 \tTraining Loss: 54.855372 \tacc: 86.254591\tValidation Loss: 210.836112, time5.3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2603 \tTraining Loss: 54.574151 \tacc: 85.715106\tValidation Loss: 212.462720, time5.3\n",
      "Epoch: 2604 \tTraining Loss: 55.113636 \tacc: 88.452709\tValidation Loss: 203.030998, time5.3\n",
      "Epoch: 2605 \tTraining Loss: 55.230334 \tacc: 86.082415\tValidation Loss: 208.957007, time5.3\n",
      "Epoch: 2606 \tTraining Loss: 54.790328 \tacc: 87.029385\tValidation Loss: 209.895281, time5.3\n",
      "Epoch: 2607 \tTraining Loss: 55.616774 \tacc: 86.759642\tValidation Loss: 212.352333, time5.2\n",
      "Epoch: 2608 \tTraining Loss: 54.587542 \tacc: 86.943297\tValidation Loss: 211.505896, time5.3\n",
      "Epoch: 2609 \tTraining Loss: 55.105984 \tacc: 86.289027\tValidation Loss: 209.608305, time5.3\n",
      "Epoch: 2610 \tTraining Loss: 54.220233 \tacc: 86.380854\tValidation Loss: 205.473877, time5.3\n",
      "Epoch: 2611 \tTraining Loss: 54.884068 \tacc: 86.713728\tValidation Loss: 207.706511, time5.3\n",
      "Epoch: 2612 \tTraining Loss: 54.931895 \tacc: 86.093893\tValidation Loss: 212.414781, time5.3\n",
      "Epoch: 2613 \tTraining Loss: 54.430670 \tacc: 86.644858\tValidation Loss: 212.458937, time5.3\n",
      "Epoch: 2614 \tTraining Loss: 53.745791 \tacc: 86.822773\tValidation Loss: 211.730086, time5.3\n",
      "Epoch: 2615 \tTraining Loss: 54.679369 \tacc: 86.730946\tValidation Loss: 216.647198, time5.3\n",
      "Epoch: 2616 \tTraining Loss: 54.813284 \tacc: 86.484160\tValidation Loss: 212.286987, time5.3\n",
      "Epoch: 2617 \tTraining Loss: 54.819024 \tacc: 87.161387\tValidation Loss: 212.424779, time5.3\n",
      "Epoch: 2618 \tTraining Loss: 55.195898 \tacc: 86.346419\tValidation Loss: 207.301688, time5.2\n",
      "Epoch: 2619 \tTraining Loss: 54.874503 \tacc: 88.320707\tValidation Loss: 208.729269, time5.3\n",
      "Epoch: 2620 \tTraining Loss: 55.241812 \tacc: 86.168503\tValidation Loss: 211.367634, time5.3\n",
      "Epoch: 2621 \tTraining Loss: 55.006504 \tacc: 88.297750\tValidation Loss: 211.127364, time5.3\n",
      "Epoch: 2622 \tTraining Loss: 57.275406 \tacc: 86.403811\tValidation Loss: 201.463429, time5.3\n",
      "Epoch: 2623 \tTraining Loss: 54.935721 \tacc: 86.254591\tValidation Loss: 211.892072, time5.3\n",
      "Epoch: 2624 \tTraining Loss: 54.004056 \tacc: 86.455464\tValidation Loss: 213.745858, time5.3\n",
      "Epoch: 2625 \tTraining Loss: 54.996939 \tacc: 86.943297\tValidation Loss: 211.749855, time5.3\n",
      "Epoch: 2626 \tTraining Loss: 55.077288 \tacc: 85.938935\tValidation Loss: 216.879492, time5.3\n",
      "Epoch: 2627 \tTraining Loss: 54.606673 \tacc: 86.380854\tValidation Loss: 216.772481, time5.3\n",
      "Epoch: 2628 \tTraining Loss: 54.729109 \tacc: 86.828512\tValidation Loss: 209.186664, time5.3\n",
      "Epoch: 2629 \tTraining Loss: 54.325451 \tacc: 86.702250\tValidation Loss: 215.176667, time5.2\n",
      "Epoch: 2630 \tTraining Loss: 55.025635 \tacc: 86.667815\tValidation Loss: 211.217287, time5.3\n",
      "Epoch: 2631 \tTraining Loss: 54.168580 \tacc: 86.598944\tValidation Loss: 213.137106, time5.3\n",
      "Epoch: 2632 \tTraining Loss: 54.426844 \tacc: 86.271809\tValidation Loss: 215.714554, time5.3\n",
      "Epoch: 2633 \tTraining Loss: 54.725283 \tacc: 87.264692\tValidation Loss: 211.686383, time5.3\n",
      "Epoch: 2634 \tTraining Loss: 54.451714 \tacc: 87.459826\tValidation Loss: 218.233970, time5.3\n",
      "Epoch: 2635 \tTraining Loss: 55.479033 \tacc: 86.225895\tValidation Loss: 220.777341, time5.2\n",
      "Epoch: 2636 \tTraining Loss: 54.732935 \tacc: 86.891644\tValidation Loss: 214.296375, time5.3\n",
      "Epoch: 2637 \tTraining Loss: 54.725283 \tacc: 87.597567\tValidation Loss: 209.032522, time5.3\n",
      "Epoch: 2638 \tTraining Loss: 55.335552 \tacc: 86.409550\tValidation Loss: 211.410415, time5.3\n",
      "Epoch: 2639 \tTraining Loss: 54.790328 \tacc: 86.994949\tValidation Loss: 205.563014, time5.2\n",
      "Epoch: 2640 \tTraining Loss: 54.713805 \tacc: 86.380854\tValidation Loss: 216.990689, time5.3\n",
      "Epoch: 2641 \tTraining Loss: 54.644934 \tacc: 86.489899\tValidation Loss: 202.524216, time5.3\n",
      "Epoch: 2642 \tTraining Loss: 56.799051 \tacc: 85.961892\tValidation Loss: 209.870063, time5.3\n",
      "Epoch: 2643 \tTraining Loss: 55.391032 \tacc: 86.231635\tValidation Loss: 208.992721, time5.3\n",
      "Epoch: 2644 \tTraining Loss: 55.301117 \tacc: 86.857208\tValidation Loss: 195.635293, time5.3\n",
      "Epoch: 2645 \tTraining Loss: 55.748776 \tacc: 86.524334\tValidation Loss: 201.588967, time5.3\n",
      "Epoch: 2646 \tTraining Loss: 55.039027 \tacc: 88.079660\tValidation Loss: 192.181888, time5.3\n",
      "Epoch: 2647 \tTraining Loss: 58.696817 \tacc: 86.719467\tValidation Loss: 188.720873, time5.3\n",
      "Epoch: 2648 \tTraining Loss: 58.310376 \tacc: 86.070937\tValidation Loss: 188.905219, time5.3\n",
      "Epoch: 2649 \tTraining Loss: 57.715412 \tacc: 86.977732\tValidation Loss: 193.233726, time5.3\n",
      "Epoch: 2650 \tTraining Loss: 57.219927 \tacc: 86.914601\tValidation Loss: 194.572699, time5.2\n",
      "Epoch: 2651 \tTraining Loss: 56.864096 \tacc: 86.736685\tValidation Loss: 198.832051, time5.3\n",
      "Epoch: 2652 \tTraining Loss: 57.311754 \tacc: 87.890266\tValidation Loss: 191.287663, time5.3\n",
      "Epoch: 2653 \tTraining Loss: 57.216100 \tacc: 86.954775\tValidation Loss: 198.190519, time5.3\n",
      "Epoch: 2654 \tTraining Loss: 56.695745 \tacc: 87.746786\tValidation Loss: 197.589067, time5.3\n",
      "Epoch: 2655 \tTraining Loss: 56.018519 \tacc: 86.702250\tValidation Loss: 198.079359, time5.3\n",
      "Epoch: 2656 \tTraining Loss: 55.490511 \tacc: 86.398072\tValidation Loss: 203.776393, time5.3\n",
      "Epoch: 2657 \tTraining Loss: 57.640802 \tacc: 86.489899\tValidation Loss: 191.514882, time5.3\n",
      "Epoch: 2658 \tTraining Loss: 56.770355 \tacc: 87.012167\tValidation Loss: 196.162699, time5.3\n",
      "Epoch: 2659 \tTraining Loss: 56.431742 \tacc: 86.616162\tValidation Loss: 199.692308, time5.3\n",
      "Epoch: 2660 \tTraining Loss: 55.957300 \tacc: 86.707989\tValidation Loss: 200.409993, time5.3\n",
      "Epoch: 2661 \tTraining Loss: 55.574686 \tacc: 86.438246\tValidation Loss: 204.854074, time5.2\n",
      "Epoch: 2662 \tTraining Loss: 55.343205 \tacc: 86.558770\tValidation Loss: 202.722963, time5.3\n",
      "Epoch: 2663 \tTraining Loss: 55.658861 \tacc: 86.776860\tValidation Loss: 198.598367, time5.3\n",
      "Epoch: 2664 \tTraining Loss: 55.333639 \tacc: 86.541552\tValidation Loss: 200.363595, time5.3\n",
      "Epoch: 2665 \tTraining Loss: 55.494337 \tacc: 86.512856\tValidation Loss: 205.896440, time5.3\n",
      "Epoch: 2666 \tTraining Loss: 55.922865 \tacc: 86.346419\tValidation Loss: 198.185202, time5.3\n",
      "Epoch: 2667 \tTraining Loss: 56.284435 \tacc: 85.927456\tValidation Loss: 203.855772, time5.3\n",
      "Epoch: 2668 \tTraining Loss: 55.534512 \tacc: 86.662075\tValidation Loss: 212.655103, time5.3\n",
      "Epoch: 2669 \tTraining Loss: 55.576599 \tacc: 86.134068\tValidation Loss: 209.045452, time5.3\n",
      "Epoch: 2670 \tTraining Loss: 55.105984 \tacc: 86.489899\tValidation Loss: 199.307801, time5.3\n",
      "Epoch: 2671 \tTraining Loss: 55.928604 \tacc: 86.908861\tValidation Loss: 204.513959, time5.2\n",
      "Epoch: 2672 \tTraining Loss: 55.589991 \tacc: 86.088154\tValidation Loss: 207.743950, time5.3\n",
      "Epoch: 2673 \tTraining Loss: 56.098867 \tacc: 86.839991\tValidation Loss: 199.986743, time5.3\n",
      "Epoch: 2674 \tTraining Loss: 55.943909 \tacc: 86.208678\tValidation Loss: 202.285128, time5.3\n",
      "Epoch: 2675 \tTraining Loss: 55.519207 \tacc: 85.887282\tValidation Loss: 205.776575, time5.3\n",
      "Epoch: 2676 \tTraining Loss: 55.867386 \tacc: 87.138430\tValidation Loss: 204.551754, time5.3\n",
      "Epoch: 2677 \tTraining Loss: 56.598179 \tacc: 86.271809\tValidation Loss: 206.561348, time5.3\n",
      "Epoch: 2678 \tTraining Loss: 55.440771 \tacc: 86.294766\tValidation Loss: 203.438484, time5.3\n",
      "Epoch: 2679 \tTraining Loss: 56.209826 \tacc: 86.036501\tValidation Loss: 204.197913, time5.3\n",
      "Epoch: 2680 \tTraining Loss: 55.899908 \tacc: 86.616162\tValidation Loss: 207.808437, time5.3\n",
      "Epoch: 2681 \tTraining Loss: 55.746863 \tacc: 86.317723\tValidation Loss: 204.529907, time5.3\n",
      "Epoch: 2682 \tTraining Loss: 55.656948 \tacc: 86.392332\tValidation Loss: 201.562857, time5.2\n",
      "Epoch: 2683 \tTraining Loss: 55.167202 \tacc: 86.621901\tValidation Loss: 209.525697, time5.3\n",
      "Epoch: 2684 \tTraining Loss: 55.693297 \tacc: 85.927456\tValidation Loss: 209.040738, time5.3\n",
      "Epoch: 2685 \tTraining Loss: 54.648760 \tacc: 87.195822\tValidation Loss: 209.662502, time5.2\n",
      "Epoch: 2686 \tTraining Loss: 56.448959 \tacc: 86.340680\tValidation Loss: 210.505549, time5.2\n",
      "Epoch: 2687 \tTraining Loss: 54.887894 \tacc: 86.237374\tValidation Loss: 203.449703, time5.2\n",
      "Epoch: 2688 \tTraining Loss: 55.176768 \tacc: 86.954775\tValidation Loss: 204.888560, time5.3\n",
      "Epoch: 2689 \tTraining Loss: 55.067723 \tacc: 85.950413\tValidation Loss: 213.637453, time5.2\n",
      "Epoch: 2690 \tTraining Loss: 54.813284 \tacc: 86.352158\tValidation Loss: 201.075421, time5.3\n",
      "Epoch: 2691 \tTraining Loss: 55.197811 \tacc: 86.357897\tValidation Loss: 207.923616, time5.3\n",
      "Epoch: 2692 \tTraining Loss: 55.324074 \tacc: 86.530073\tValidation Loss: 203.911875, time5.3\n",
      "Epoch: 2693 \tTraining Loss: 55.842516 \tacc: 86.662075\tValidation Loss: 206.908667, time5.3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2694 \tTraining Loss: 55.060070 \tacc: 85.783976\tValidation Loss: 214.700440, time5.3\n",
      "Epoch: 2695 \tTraining Loss: 54.857285 \tacc: 85.841368\tValidation Loss: 209.389114, time5.3\n",
      "Epoch: 2696 \tTraining Loss: 55.241812 \tacc: 86.484160\tValidation Loss: 211.130274, time5.3\n",
      "Epoch: 2697 \tTraining Loss: 55.352770 \tacc: 86.857208\tValidation Loss: 202.085222, time5.3\n",
      "Epoch: 2698 \tTraining Loss: 56.334175 \tacc: 87.534435\tValidation Loss: 211.771097, time5.3\n",
      "Epoch: 2699 \tTraining Loss: 55.676079 \tacc: 85.893021\tValidation Loss: 209.071020, time5.3\n",
      "Epoch: 2700 \tTraining Loss: 54.973982 \tacc: 86.507117\tValidation Loss: 206.436521, time5.3\n",
      "Epoch: 2701 \tTraining Loss: 57.007576 \tacc: 85.950413\tValidation Loss: 208.061032, time5.3\n",
      "Epoch: 2702 \tTraining Loss: 55.563208 \tacc: 86.530073\tValidation Loss: 210.032462, time5.3\n",
      "Epoch: 2703 \tTraining Loss: 56.812443 \tacc: 86.690771\tValidation Loss: 199.302379, time5.3\n",
      "Epoch: 2704 \tTraining Loss: 56.896618 \tacc: 86.512856\tValidation Loss: 207.758857, time5.2\n",
      "Epoch: 2705 \tTraining Loss: 56.311218 \tacc: 86.289027\tValidation Loss: 208.367934, time5.3\n",
      "Epoch: 2706 \tTraining Loss: 55.115549 \tacc: 86.839991\tValidation Loss: 201.917819, time5.3\n",
      "Epoch: 2707 \tTraining Loss: 55.961126 \tacc: 86.277548\tValidation Loss: 214.839166, time5.3\n",
      "Epoch: 2708 \tTraining Loss: 54.254668 \tacc: 86.713728\tValidation Loss: 211.849348, time5.2\n",
      "Epoch: 2709 \tTraining Loss: 54.803719 \tacc: 86.748163\tValidation Loss: 209.187158, time5.3\n",
      "Epoch: 2710 \tTraining Loss: 55.459902 \tacc: 85.864325\tValidation Loss: 210.875145, time5.3\n",
      "Epoch: 2711 \tTraining Loss: 56.005127 \tacc: 87.161387\tValidation Loss: 208.038949, time5.3\n",
      "Epoch: 2712 \tTraining Loss: 55.257117 \tacc: 85.847107\tValidation Loss: 214.296837, time5.3\n",
      "Epoch: 2713 \tTraining Loss: 54.688935 \tacc: 86.357897\tValidation Loss: 209.383989, time5.3\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-39c61911b212>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     41\u001b[0m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_cn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_cn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel_cn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabel_cn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m         \u001b[0mtrain_meter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_cn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel_cn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m#train_meter.update(output_cn, loss_cn)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m     \u001b[0meval_meter_cn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMeter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/BRL/c0822_dynamicFinal/utils_mlp.py\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self, y_pred, y_true)\u001b[0m\n\u001b[1;32m     54\u001b[0m             \u001b[0mtruth\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mshape\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mB\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m         \"\"\"\n\u001b[0;32m---> 56\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m         \u001b[0;31m#self.mask.append(mask.detach().cpu())\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.001\n",
      "0.0001\n"
     ]
    }
   ],
   "source": [
    "# torch.save({'model_state_dict': model.state_dict(),\n",
    "#     'optimizer': optimizer.state_dict()}, 'model_saved/mlp/multitask/'+str(epoch)+'.pth')\n",
    "\n",
    "state=torch.load('model_saved/mlp/multitask/model.pth') \n",
    "model.load_state_dict(state['model_state_dict']) \n",
    "optimizer.load_state_dict(state['optimizer'])\n",
    "\n",
    "for param_group in optimizer.param_groups: print(param_group['lr'])\n",
    "for param_group in optimizer.param_groups: param_group['lr'] = param_group['lr']*0.1\n",
    "for param_group in optimizer.param_groups: print(param_group['lr'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 \tTraining Loss: 51.497934 \tacc: 85.588843\tValidation Loss: 201.576589, time5.2\n",
      "Validation loss decreased (85.692149 --> 85.588843).  Saving model ...\n",
      "Epoch: 2 \tTraining Loss: 51.084711 \tacc: 85.542929\tValidation Loss: 205.263144, time5.2\n",
      "Validation loss decreased (85.588843 --> 85.542929).  Saving model ...\n",
      "Epoch: 3 \tTraining Loss: 50.948883 \tacc: 85.548669\tValidation Loss: 207.247040, time5.2\n",
      "Epoch: 4 \tTraining Loss: 50.397919 \tacc: 85.594582\tValidation Loss: 208.872700, time5.2\n",
      "Epoch: 5 \tTraining Loss: 50.514616 \tacc: 85.606061\tValidation Loss: 211.116680, time5.3\n",
      "Epoch: 6 \tTraining Loss: 50.572008 \tacc: 85.548669\tValidation Loss: 212.486968, time5.2\n",
      "Epoch: 7 \tTraining Loss: 50.147306 \tacc: 85.548669\tValidation Loss: 213.634955, time5.3\n",
      "Epoch: 8 \tTraining Loss: 50.327135 \tacc: 85.542929\tValidation Loss: 214.268986, time5.3\n",
      "Validation loss decreased (85.542929 --> 85.542929).  Saving model ...\n",
      "Epoch: 9 \tTraining Loss: 50.229568 \tacc: 85.611800\tValidation Loss: 215.519638, time5.3\n",
      "Epoch: 10 \tTraining Loss: 49.722605 \tacc: 85.462580\tValidation Loss: 216.067652, time5.2\n",
      "Validation loss decreased (85.542929 --> 85.462580).  Saving model ...\n",
      "Epoch: 11 \tTraining Loss: 50.183655 \tacc: 85.606061\tValidation Loss: 216.365274, time5.3\n",
      "Epoch: 12 \tTraining Loss: 49.871824 \tacc: 85.497016\tValidation Loss: 217.436681, time5.3\n",
      "Epoch: 13 \tTraining Loss: 49.686257 \tacc: 85.508494\tValidation Loss: 218.135118, time5.3\n",
      "Epoch: 14 \tTraining Loss: 50.254438 \tacc: 85.497016\tValidation Loss: 218.740532, time5.3\n",
      "Epoch: 15 \tTraining Loss: 49.944521 \tacc: 85.629017\tValidation Loss: 220.085705, time5.3\n",
      "Epoch: 16 \tTraining Loss: 50.082262 \tacc: 85.525712\tValidation Loss: 219.699726, time5.3\n",
      "Epoch: 17 \tTraining Loss: 50.193220 \tacc: 85.617539\tValidation Loss: 220.788069, time5.3\n",
      "Epoch: 18 \tTraining Loss: 49.996174 \tacc: 85.531451\tValidation Loss: 221.027601, time5.3\n",
      "Epoch: 19 \tTraining Loss: 50.158785 \tacc: 85.565886\tValidation Loss: 221.470239, time5.3\n",
      "Epoch: 20 \tTraining Loss: 49.644169 \tacc: 85.542929\tValidation Loss: 221.204348, time5.3\n",
      "Epoch: 21 \tTraining Loss: 49.866085 \tacc: 85.720845\tValidation Loss: 222.488975, time5.3\n",
      "Epoch: 22 \tTraining Loss: 49.768519 \tacc: 85.726584\tValidation Loss: 222.879476, time5.3\n",
      "Epoch: 23 \tTraining Loss: 49.470080 \tacc: 85.497016\tValidation Loss: 223.750776, time5.3\n",
      "Epoch: 24 \tTraining Loss: 49.450949 \tacc: 85.497016\tValidation Loss: 223.906321, time5.3\n",
      "Epoch: 25 \tTraining Loss: 49.726431 \tacc: 85.657713\tValidation Loss: 224.480980, time5.3\n",
      "Epoch: 26 \tTraining Loss: 49.487297 \tacc: 85.565886\tValidation Loss: 226.026250, time5.3\n",
      "Epoch: 27 \tTraining Loss: 49.885216 \tacc: 85.468320\tValidation Loss: 226.521366, time5.3\n",
      "Epoch: 28 \tTraining Loss: 49.965565 \tacc: 85.697888\tValidation Loss: 225.329788, time5.2\n",
      "Epoch: 29 \tTraining Loss: 49.787649 \tacc: 85.749541\tValidation Loss: 225.094359, time5.3\n",
      "Epoch: 30 \tTraining Loss: 49.579125 \tacc: 85.519972\tValidation Loss: 225.656443, time5.3\n",
      "Epoch: 31 \tTraining Loss: 49.867998 \tacc: 85.519972\tValidation Loss: 227.356885, time5.3\n",
      "Epoch: 32 \tTraining Loss: 49.615473 \tacc: 85.560147\tValidation Loss: 228.349733, time5.3\n",
      "Epoch: 33 \tTraining Loss: 49.816345 \tacc: 85.514233\tValidation Loss: 227.854025, time5.3\n",
      "Epoch: 34 \tTraining Loss: 49.447123 \tacc: 85.485537\tValidation Loss: 228.027502, time5.3\n",
      "Epoch: 35 \tTraining Loss: 49.657560 \tacc: 85.617539\tValidation Loss: 227.572483, time5.3\n",
      "Epoch: 36 \tTraining Loss: 49.630777 \tacc: 85.743802\tValidation Loss: 228.282739, time5.3\n",
      "Epoch: 37 \tTraining Loss: 49.454775 \tacc: 85.554408\tValidation Loss: 228.997426, time5.3\n",
      "Epoch: 38 \tTraining Loss: 49.362948 \tacc: 85.577365\tValidation Loss: 228.954056, time5.2\n",
      "Epoch: 39 \tTraining Loss: 49.093205 \tacc: 85.565886\tValidation Loss: 228.872582, time5.3\n",
      "Epoch: 40 \tTraining Loss: 49.569559 \tacc: 85.812672\tValidation Loss: 228.011838, time5.3\n",
      "Epoch: 41 \tTraining Loss: 49.548515 \tacc: 85.634757\tValidation Loss: 229.556831, time5.3\n",
      "Epoch: 42 \tTraining Loss: 49.644169 \tacc: 85.491276\tValidation Loss: 230.157714, time5.3\n",
      "Epoch: 43 \tTraining Loss: 49.322773 \tacc: 85.577365\tValidation Loss: 230.305701, time5.3\n",
      "Epoch: 44 \tTraining Loss: 49.688170 \tacc: 85.611800\tValidation Loss: 229.655014, time5.3\n",
      "Epoch: 45 \tTraining Loss: 49.087466 \tacc: 85.531451\tValidation Loss: 231.162267, time5.3\n",
      "Epoch: 46 \tTraining Loss: 49.081726 \tacc: 85.657713\tValidation Loss: 230.311487, time5.3\n",
      "Epoch: 47 \tTraining Loss: 49.460514 \tacc: 85.468320\tValidation Loss: 229.848990, time5.3\n",
      "Epoch: 48 \tTraining Loss: 49.584864 \tacc: 85.554408\tValidation Loss: 229.476023, time5.3\n",
      "Epoch: 49 \tTraining Loss: 49.538950 \tacc: 85.416667\tValidation Loss: 232.105782, time5.2\n",
      "Validation loss decreased (85.462580 --> 85.416667).  Saving model ...\n",
      "Epoch: 50 \tTraining Loss: 49.651821 \tacc: 85.565886\tValidation Loss: 230.764469, time5.3\n",
      "Epoch: 51 \tTraining Loss: 49.232859 \tacc: 85.514233\tValidation Loss: 231.313705, time5.2\n",
      "Epoch: 52 \tTraining Loss: 49.119988 \tacc: 85.617539\tValidation Loss: 231.036666, time5.3\n",
      "Epoch: 53 \tTraining Loss: 49.818258 \tacc: 85.720845\tValidation Loss: 231.068220, time5.2\n",
      "Epoch: 54 \tTraining Loss: 49.037726 \tacc: 85.445363\tValidation Loss: 231.421997, time5.3\n",
      "Epoch: 55 \tTraining Loss: 48.716330 \tacc: 85.577365\tValidation Loss: 232.023706, time5.2\n",
      "Epoch: 56 \tTraining Loss: 49.491123 \tacc: 85.726584\tValidation Loss: 230.568850, time5.3\n",
      "Epoch: 57 \tTraining Loss: 49.487297 \tacc: 85.623278\tValidation Loss: 231.959892, time5.2\n",
      "Epoch: 58 \tTraining Loss: 49.112335 \tacc: 85.542929\tValidation Loss: 232.940363, time5.3\n",
      "Epoch: 59 \tTraining Loss: 49.364861 \tacc: 85.428145\tValidation Loss: 232.443735, time5.2\n",
      "Epoch: 60 \tTraining Loss: 49.263468 \tacc: 85.393710\tValidation Loss: 233.812218, time5.3\n",
      "Validation loss decreased (85.416667 --> 85.393710).  Saving model ...\n",
      "Epoch: 61 \tTraining Loss: 49.041552 \tacc: 85.554408\tValidation Loss: 232.882829, time5.2\n",
      "Epoch: 62 \tTraining Loss: 49.397383 \tacc: 85.405188\tValidation Loss: 233.006702, time5.3\n",
      "Epoch: 63 \tTraining Loss: 49.175467 \tacc: 85.611800\tValidation Loss: 232.922217, time5.3\n",
      "Epoch: 64 \tTraining Loss: 49.288338 \tacc: 85.829890\tValidation Loss: 233.084234, time5.3\n",
      "Epoch: 65 \tTraining Loss: 49.408861 \tacc: 85.485537\tValidation Loss: 233.378514, time5.3\n",
      "Epoch: 66 \tTraining Loss: 49.389731 \tacc: 85.824151\tValidation Loss: 232.807229, time5.3\n",
      "Epoch: 67 \tTraining Loss: 49.519819 \tacc: 85.502755\tValidation Loss: 234.257309, time5.3\n",
      "Epoch: 68 \tTraining Loss: 49.175467 \tacc: 85.606061\tValidation Loss: 233.862099, time5.3\n",
      "Epoch: 69 \tTraining Loss: 49.031987 \tacc: 85.640496\tValidation Loss: 233.204843, time5.3\n",
      "Epoch: 70 \tTraining Loss: 48.986073 \tacc: 85.571625\tValidation Loss: 234.882448, time5.4\n",
      "Epoch: 71 \tTraining Loss: 48.884680 \tacc: 85.577365\tValidation Loss: 233.905158, time5.4\n",
      "Epoch: 72 \tTraining Loss: 48.951638 \tacc: 85.502755\tValidation Loss: 235.587220, time5.4\n",
      "Epoch: 73 \tTraining Loss: 48.921028 \tacc: 85.485537\tValidation Loss: 235.928545, time5.5\n",
      "Epoch: 74 \tTraining Loss: 49.018595 \tacc: 85.674931\tValidation Loss: 236.154734, time5.6\n",
      "Epoch: 75 \tTraining Loss: 49.146771 \tacc: 85.485537\tValidation Loss: 236.160279, time5.5\n",
      "Epoch: 76 \tTraining Loss: 49.215641 \tacc: 85.548669\tValidation Loss: 235.779346, time5.4\n",
      "Epoch: 77 \tTraining Loss: 49.749388 \tacc: 85.651974\tValidation Loss: 233.738781, time5.5\n",
      "Epoch: 78 \tTraining Loss: 49.368687 \tacc: 85.709366\tValidation Loss: 236.132530, time5.4\n",
      "Epoch: 79 \tTraining Loss: 49.087466 \tacc: 85.583104\tValidation Loss: 237.575443, time5.5\n",
      "Epoch: 80 \tTraining Loss: 49.431818 \tacc: 85.537190\tValidation Loss: 235.405806, time5.3\n",
      "Epoch: 81 \tTraining Loss: 49.280686 \tacc: 85.732323\tValidation Loss: 235.803779, time5.3\n",
      "Epoch: 82 \tTraining Loss: 48.921028 \tacc: 85.542929\tValidation Loss: 236.792281, time5.3\n",
      "Epoch: 83 \tTraining Loss: 49.353382 \tacc: 85.875803\tValidation Loss: 236.295674, time5.3\n",
      "Epoch: 84 \tTraining Loss: 49.146771 \tacc: 85.537190\tValidation Loss: 237.172726, time5.3\n",
      "Epoch: 85 \tTraining Loss: 48.865549 \tacc: 85.611800\tValidation Loss: 235.991089, time5.3\n",
      "Epoch: 86 \tTraining Loss: 49.324686 \tacc: 85.571625\tValidation Loss: 235.176599, time5.3\n",
      "Epoch: 87 \tTraining Loss: 49.045378 \tacc: 85.789715\tValidation Loss: 234.490772, time5.3\n",
      "Epoch: 88 \tTraining Loss: 48.959290 \tacc: 85.640496\tValidation Loss: 235.562906, time5.3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 89 \tTraining Loss: 49.022421 \tacc: 85.709366\tValidation Loss: 236.737421, time5.3\n",
      "Epoch: 90 \tTraining Loss: 49.339991 \tacc: 85.611800\tValidation Loss: 237.221610, time5.3\n",
      "Epoch: 91 \tTraining Loss: 49.324686 \tacc: 85.629017\tValidation Loss: 237.970879, time5.3\n",
      "Epoch: 92 \tTraining Loss: 49.389731 \tacc: 85.749541\tValidation Loss: 237.458214, time5.4\n",
      "Epoch: 93 \tTraining Loss: 49.144858 \tacc: 85.583104\tValidation Loss: 237.860057, time5.5\n",
      "Epoch: 94 \tTraining Loss: 49.125727 \tacc: 85.554408\tValidation Loss: 238.239106, time5.4\n",
      "Epoch: 95 \tTraining Loss: 48.823462 \tacc: 85.606061\tValidation Loss: 236.800661, time5.5\n",
      "Epoch: 96 \tTraining Loss: 49.389731 \tacc: 85.497016\tValidation Loss: 238.078285, time5.5\n",
      "Epoch: 97 \tTraining Loss: 49.334252 \tacc: 85.537190\tValidation Loss: 237.398318, time5.5\n",
      "Epoch: 98 \tTraining Loss: 49.064509 \tacc: 85.692149\tValidation Loss: 235.904341, time5.5\n",
      "Epoch: 99 \tTraining Loss: 49.242424 \tacc: 85.525712\tValidation Loss: 236.559409, time5.5\n",
      "Epoch: 100 \tTraining Loss: 49.441384 \tacc: 85.560147\tValidation Loss: 236.894926, time5.5\n",
      "Epoch: 101 \tTraining Loss: 48.884680 \tacc: 85.594582\tValidation Loss: 237.744175, time5.5\n",
      "Epoch: 102 \tTraining Loss: 49.163988 \tacc: 85.514233\tValidation Loss: 237.813853, time5.5\n",
      "Epoch: 103 \tTraining Loss: 49.162075 \tacc: 85.433884\tValidation Loss: 237.888665, time5.5\n",
      "Epoch: 104 \tTraining Loss: 49.035813 \tacc: 85.634757\tValidation Loss: 236.681214, time5.4\n",
      "Epoch: 105 \tTraining Loss: 49.242424 \tacc: 85.514233\tValidation Loss: 237.402104, time5.5\n",
      "Epoch: 106 \tTraining Loss: 49.131466 \tacc: 85.663453\tValidation Loss: 237.925086, time5.5\n",
      "Epoch: 107 \tTraining Loss: 48.928681 \tacc: 86.214417\tValidation Loss: 236.536219, time5.5\n",
      "Epoch: 108 \tTraining Loss: 49.376339 \tacc: 85.594582\tValidation Loss: 237.850627, time5.5\n",
      "Epoch: 109 \tTraining Loss: 49.012856 \tacc: 85.542929\tValidation Loss: 238.641991, time5.5\n",
      "Epoch: 110 \tTraining Loss: 48.945898 \tacc: 85.519972\tValidation Loss: 237.376368, time5.5\n",
      "Epoch: 111 \tTraining Loss: 48.884680 \tacc: 85.715106\tValidation Loss: 237.572274, time5.5\n",
      "Epoch: 112 \tTraining Loss: 48.936333 \tacc: 85.686410\tValidation Loss: 237.867764, time5.5\n",
      "Epoch: 113 \tTraining Loss: 48.790940 \tacc: 85.617539\tValidation Loss: 239.539339, time5.5\n",
      "Epoch: 114 \tTraining Loss: 49.072161 \tacc: 85.703627\tValidation Loss: 240.537685, time5.5\n",
      "Epoch: 115 \tTraining Loss: 48.878941 \tacc: 85.485537\tValidation Loss: 239.551005, time5.4\n",
      "Epoch: 116 \tTraining Loss: 49.022421 \tacc: 85.617539\tValidation Loss: 239.089938, time5.5\n",
      "Epoch: 117 \tTraining Loss: 49.473906 \tacc: 85.594582\tValidation Loss: 238.485211, time5.5\n",
      "Epoch: 118 \tTraining Loss: 49.238598 \tacc: 85.606061\tValidation Loss: 237.246502, time5.5\n",
      "Epoch: 119 \tTraining Loss: 48.875115 \tacc: 85.542929\tValidation Loss: 239.542022, time5.5\n",
      "Epoch: 120 \tTraining Loss: 48.574763 \tacc: 85.617539\tValidation Loss: 240.347131, time5.5\n",
      "Epoch: 121 \tTraining Loss: 48.890419 \tacc: 85.669192\tValidation Loss: 239.489967, time5.5\n",
      "Epoch: 122 \tTraining Loss: 49.043465 \tacc: 85.508494\tValidation Loss: 240.733887, time5.5\n",
      "Epoch: 123 \tTraining Loss: 49.167815 \tacc: 85.519972\tValidation Loss: 241.335140, time5.5\n",
      "Epoch: 124 \tTraining Loss: 48.861723 \tacc: 85.583104\tValidation Loss: 241.259494, time5.5\n",
      "Epoch: 125 \tTraining Loss: 49.066422 \tacc: 85.709366\tValidation Loss: 238.669305, time5.5\n",
      "Epoch: 126 \tTraining Loss: 48.622590 \tacc: 85.588843\tValidation Loss: 241.407808, time5.4\n",
      "Epoch: 127 \tTraining Loss: 49.079813 \tacc: 85.347796\tValidation Loss: 241.315529, time5.5\n",
      "Validation loss decreased (85.393710 --> 85.347796).  Saving model ...\n",
      "Epoch: 128 \tTraining Loss: 48.989899 \tacc: 85.640496\tValidation Loss: 239.813258, time5.5\n",
      "Epoch: 129 \tTraining Loss: 48.810070 \tacc: 85.548669\tValidation Loss: 239.762838, time5.4\n",
      "Epoch: 130 \tTraining Loss: 48.653199 \tacc: 85.485537\tValidation Loss: 241.398679, time5.5\n",
      "Epoch: 131 \tTraining Loss: 48.787114 \tacc: 85.709366\tValidation Loss: 239.720322, time5.4\n",
      "Epoch: 132 \tTraining Loss: 49.100857 \tacc: 85.519972\tValidation Loss: 242.188027, time5.5\n",
      "Epoch: 133 \tTraining Loss: 48.993725 \tacc: 85.611800\tValidation Loss: 242.195589, time5.4\n",
      "Epoch: 134 \tTraining Loss: 48.588154 \tacc: 85.508494\tValidation Loss: 241.077921, time5.5\n",
      "Epoch: 135 \tTraining Loss: 49.106596 \tacc: 85.531451\tValidation Loss: 241.478463, time5.4\n",
      "Epoch: 136 \tTraining Loss: 49.031987 \tacc: 85.560147\tValidation Loss: 241.024029, time5.5\n",
      "Epoch: 137 \tTraining Loss: 48.840680 \tacc: 85.410927\tValidation Loss: 241.479535, time5.4\n",
      "Epoch: 138 \tTraining Loss: 48.991812 \tacc: 85.497016\tValidation Loss: 241.867045, time5.4\n",
      "Epoch: 139 \tTraining Loss: 48.871289 \tacc: 85.548669\tValidation Loss: 240.670172, time5.5\n",
      "Epoch: 140 \tTraining Loss: 48.817723 \tacc: 85.502755\tValidation Loss: 242.206058, time5.4\n",
      "Epoch: 141 \tTraining Loss: 48.620676 \tacc: 85.720845\tValidation Loss: 242.271359, time5.5\n",
      "Epoch: 142 \tTraining Loss: 48.519284 \tacc: 85.560147\tValidation Loss: 242.170127, time5.4\n",
      "Epoch: 143 \tTraining Loss: 48.723982 \tacc: 85.634757\tValidation Loss: 241.526092, time5.5\n",
      "Epoch: 144 \tTraining Loss: 48.634068 \tacc: 85.766758\tValidation Loss: 241.512355, time5.4\n",
      "Epoch: 145 \tTraining Loss: 48.710591 \tacc: 85.606061\tValidation Loss: 241.228559, time5.5\n",
      "Epoch: 146 \tTraining Loss: 48.790940 \tacc: 85.875803\tValidation Loss: 241.620457, time5.3\n",
      "Epoch: 147 \tTraining Loss: 48.373890 \tacc: 85.410927\tValidation Loss: 244.262348, time5.4\n",
      "Epoch: 148 \tTraining Loss: 48.852158 \tacc: 85.720845\tValidation Loss: 242.868122, time5.3\n",
      "Epoch: 149 \tTraining Loss: 48.997551 \tacc: 85.674931\tValidation Loss: 242.929961, time5.4\n",
      "Epoch: 150 \tTraining Loss: 48.750765 \tacc: 85.651974\tValidation Loss: 242.208563, time5.4\n",
      "Epoch: 151 \tTraining Loss: 48.911463 \tacc: 85.606061\tValidation Loss: 242.640571, time5.5\n",
      "Epoch: 152 \tTraining Loss: 49.355295 \tacc: 85.749541\tValidation Loss: 241.938229, time5.6\n",
      "Epoch: 153 \tTraining Loss: 48.603459 \tacc: 85.617539\tValidation Loss: 243.612709, time5.4\n",
      "Epoch: 154 \tTraining Loss: 48.848332 \tacc: 85.692149\tValidation Loss: 243.079825, time5.5\n",
      "Epoch: 155 \tTraining Loss: 48.821549 \tacc: 85.761019\tValidation Loss: 242.894955, time5.6\n",
      "Epoch: 156 \tTraining Loss: 48.896159 \tacc: 85.617539\tValidation Loss: 243.790426, time5.5\n",
      "Epoch: 157 \tTraining Loss: 48.894245 \tacc: 85.623278\tValidation Loss: 242.052359, time5.4\n",
      "Epoch: 158 \tTraining Loss: 48.580502 \tacc: 85.571625\tValidation Loss: 243.292200, time5.5\n",
      "Epoch: 159 \tTraining Loss: 48.584328 \tacc: 85.806933\tValidation Loss: 242.425543, time5.5\n",
      "Epoch: 160 \tTraining Loss: 48.746939 \tacc: 85.588843\tValidation Loss: 243.272578, time5.5\n",
      "Epoch: 161 \tTraining Loss: 48.898072 \tacc: 85.640496\tValidation Loss: 243.407769, time5.5\n",
      "Epoch: 162 \tTraining Loss: 48.697199 \tacc: 85.542929\tValidation Loss: 243.307782, time5.5\n",
      "Epoch: 163 \tTraining Loss: 48.733548 \tacc: 85.766758\tValidation Loss: 244.652219, time5.4\n",
      "Epoch: 164 \tTraining Loss: 49.005204 \tacc: 85.370753\tValidation Loss: 245.245844, time5.3\n",
      "Epoch: 165 \tTraining Loss: 48.446587 \tacc: 85.634757\tValidation Loss: 244.386270, time5.3\n",
      "Epoch: 166 \tTraining Loss: 48.459979 \tacc: 85.502755\tValidation Loss: 245.092498, time5.2\n",
      "Epoch: 167 \tTraining Loss: 48.624503 \tacc: 85.519972\tValidation Loss: 244.155631, time5.3\n",
      "Epoch: 168 \tTraining Loss: 48.660851 \tacc: 85.588843\tValidation Loss: 244.815773, time5.2\n",
      "Epoch: 169 \tTraining Loss: 48.869376 \tacc: 86.007805\tValidation Loss: 244.410232, time5.3\n",
      "Epoch: 170 \tTraining Loss: 48.764157 \tacc: 85.445363\tValidation Loss: 245.545241, time5.3\n",
      "Epoch: 171 \tTraining Loss: 48.723982 \tacc: 85.531451\tValidation Loss: 244.672636, time5.3\n",
      "Epoch: 172 \tTraining Loss: 48.668503 \tacc: 85.485537\tValidation Loss: 245.962185, time5.2\n",
      "Epoch: 173 \tTraining Loss: 48.739287 \tacc: 85.669192\tValidation Loss: 243.231189, time5.3\n",
      "Epoch: 174 \tTraining Loss: 48.865549 \tacc: 85.583104\tValidation Loss: 245.360825, time5.2\n",
      "Epoch: 175 \tTraining Loss: 48.710591 \tacc: 85.697888\tValidation Loss: 244.681597, time5.3\n",
      "Epoch: 176 \tTraining Loss: 48.559458 \tacc: 85.789715\tValidation Loss: 244.531807, time5.2\n",
      "Epoch: 177 \tTraining Loss: 48.664677 \tacc: 85.778237\tValidation Loss: 243.823578, time5.2\n",
      "Epoch: 178 \tTraining Loss: 48.702938 \tacc: 85.588843\tValidation Loss: 246.565665, time5.3\n",
      "Epoch: 179 \tTraining Loss: 48.762244 \tacc: 85.686410\tValidation Loss: 246.507615, time5.3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 180 \tTraining Loss: 48.863636 \tacc: 85.674931\tValidation Loss: 246.223032, time5.3\n",
      "Epoch: 181 \tTraining Loss: 48.226584 \tacc: 85.709366\tValidation Loss: 244.669474, time5.2\n",
      "Epoch: 182 \tTraining Loss: 48.798592 \tacc: 85.669192\tValidation Loss: 245.472221, time5.3\n",
      "Epoch: 183 \tTraining Loss: 48.760331 \tacc: 85.674931\tValidation Loss: 245.177773, time5.3\n",
      "Epoch: 184 \tTraining Loss: 48.681895 \tacc: 85.611800\tValidation Loss: 245.837872, time5.3\n",
      "Epoch: 185 \tTraining Loss: 48.896159 \tacc: 85.715106\tValidation Loss: 244.837601, time5.3\n",
      "Epoch: 186 \tTraining Loss: 48.842593 \tacc: 85.709366\tValidation Loss: 244.745372, time5.3\n",
      "Epoch: 187 \tTraining Loss: 48.737374 \tacc: 85.611800\tValidation Loss: 245.181215, time5.2\n",
      "Epoch: 188 \tTraining Loss: 48.863636 \tacc: 85.629017\tValidation Loss: 244.747394, time5.4\n",
      "Epoch: 189 \tTraining Loss: 48.704852 \tacc: 85.519972\tValidation Loss: 244.486968, time5.3\n",
      "Epoch: 190 \tTraining Loss: 48.789027 \tacc: 85.680670\tValidation Loss: 244.703749, time5.4\n",
      "Epoch: 191 \tTraining Loss: 48.444674 \tacc: 85.571625\tValidation Loss: 244.837545, time5.3\n",
      "Epoch: 192 \tTraining Loss: 48.308846 \tacc: 85.606061\tValidation Loss: 245.168778, time5.2\n",
      "Epoch: 193 \tTraining Loss: 48.808157 \tacc: 85.651974\tValidation Loss: 244.125847, time5.3\n",
      "Epoch: 194 \tTraining Loss: 48.875115 \tacc: 85.623278\tValidation Loss: 244.214913, time5.2\n",
      "Epoch: 195 \tTraining Loss: 48.559458 \tacc: 85.703627\tValidation Loss: 243.540699, time5.3\n",
      "Epoch: 196 \tTraining Loss: 48.655112 \tacc: 85.588843\tValidation Loss: 245.216536, time5.2\n",
      "Epoch: 197 \tTraining Loss: 48.605372 \tacc: 85.646235\tValidation Loss: 244.489215, time5.3\n",
      "Epoch: 198 \tTraining Loss: 48.739287 \tacc: 85.479798\tValidation Loss: 246.008902, time5.2\n",
      "Epoch: 199 \tTraining Loss: 49.106596 \tacc: 85.491276\tValidation Loss: 245.218232, time5.3\n",
      "Epoch: 200 \tTraining Loss: 48.410239 \tacc: 85.634757\tValidation Loss: 245.507307, time5.2\n",
      "Epoch: 201 \tTraining Loss: 48.268672 \tacc: 85.485537\tValidation Loss: 246.545647, time5.3\n",
      "Epoch: 202 \tTraining Loss: 48.106061 \tacc: 85.629017\tValidation Loss: 246.159244, time5.2\n",
      "Epoch: 203 \tTraining Loss: 48.792853 \tacc: 85.583104\tValidation Loss: 246.098632, time5.3\n",
      "Epoch: 204 \tTraining Loss: 48.557545 \tacc: 85.600321\tValidation Loss: 246.199587, time5.2\n",
      "Epoch: 205 \tTraining Loss: 48.477196 \tacc: 85.600321\tValidation Loss: 245.640905, time5.2\n",
      "Epoch: 206 \tTraining Loss: 48.502066 \tacc: 85.738062\tValidation Loss: 247.048856, time5.3\n",
      "Epoch: 207 \tTraining Loss: 48.477196 \tacc: 85.829890\tValidation Loss: 247.637889, time5.2\n",
      "Epoch: 208 \tTraining Loss: 48.769896 \tacc: 85.514233\tValidation Loss: 247.409251, time5.3\n",
      "Epoch: 209 \tTraining Loss: 48.813897 \tacc: 85.519972\tValidation Loss: 246.505108, time5.3\n",
      "Epoch: 210 \tTraining Loss: 48.806244 \tacc: 85.692149\tValidation Loss: 245.414251, time5.3\n",
      "Epoch: 211 \tTraining Loss: 48.285889 \tacc: 85.651974\tValidation Loss: 247.797761, time5.2\n",
      "Epoch: 212 \tTraining Loss: 48.624503 \tacc: 85.893021\tValidation Loss: 246.036694, time5.3\n",
      "Epoch: 213 \tTraining Loss: 48.605372 \tacc: 85.634757\tValidation Loss: 247.417323, time5.2\n",
      "Epoch: 214 \tTraining Loss: 48.616850 \tacc: 85.583104\tValidation Loss: 247.227650, time5.3\n",
      "Epoch: 215 \tTraining Loss: 48.435109 \tacc: 85.611800\tValidation Loss: 246.454934, time5.2\n",
      "Epoch: 216 \tTraining Loss: 48.391108 \tacc: 85.347796\tValidation Loss: 248.640476, time5.3\n",
      "Validation loss decreased (85.347796 --> 85.347796).  Saving model ...\n",
      "Epoch: 217 \tTraining Loss: 48.463805 \tacc: 85.720845\tValidation Loss: 247.595723, time5.4\n",
      "Epoch: 218 \tTraining Loss: 48.305020 \tacc: 85.451102\tValidation Loss: 246.984474, time5.5\n",
      "Epoch: 219 \tTraining Loss: 48.394934 \tacc: 85.812672\tValidation Loss: 248.024021, time5.5\n",
      "Epoch: 220 \tTraining Loss: 48.458066 \tacc: 85.824151\tValidation Loss: 245.689537, time5.5\n",
      "Epoch: 221 \tTraining Loss: 48.513545 \tacc: 85.600321\tValidation Loss: 247.122660, time5.5\n",
      "Epoch: 222 \tTraining Loss: 48.959290 \tacc: 85.629017\tValidation Loss: 246.476988, time5.5\n",
      "Epoch: 223 \tTraining Loss: 48.712504 \tacc: 85.617539\tValidation Loss: 246.934117, time5.5\n",
      "Epoch: 224 \tTraining Loss: 48.341368 \tacc: 85.525712\tValidation Loss: 247.645155, time5.5\n",
      "Epoch: 225 \tTraining Loss: 48.496327 \tacc: 85.669192\tValidation Loss: 246.629542, time5.5\n",
      "Epoch: 226 \tTraining Loss: 48.429369 \tacc: 85.502755\tValidation Loss: 247.861243, time5.5\n",
      "Epoch: 227 \tTraining Loss: 48.063973 \tacc: 85.692149\tValidation Loss: 247.828242, time5.5\n",
      "Epoch: 228 \tTraining Loss: 48.758418 \tacc: 85.715106\tValidation Loss: 249.622669, time5.4\n",
      "Epoch: 229 \tTraining Loss: 48.345194 \tacc: 85.761019\tValidation Loss: 247.636539, time5.5\n",
      "Epoch: 230 \tTraining Loss: 48.750765 \tacc: 85.554408\tValidation Loss: 249.135269, time5.5\n",
      "Epoch: 231 \tTraining Loss: 48.607285 \tacc: 85.405188\tValidation Loss: 248.229918, time5.5\n",
      "Epoch: 232 \tTraining Loss: 48.347107 \tacc: 85.560147\tValidation Loss: 247.330324, time5.5\n",
      "Epoch: 233 \tTraining Loss: 48.551806 \tacc: 85.634757\tValidation Loss: 248.248603, time5.5\n",
      "Epoch: 234 \tTraining Loss: 48.488675 \tacc: 85.864325\tValidation Loss: 247.908396, time5.5\n",
      "Epoch: 235 \tTraining Loss: 48.333716 \tacc: 85.703627\tValidation Loss: 249.594508, time5.5\n",
      "Epoch: 236 \tTraining Loss: 48.280150 \tacc: 85.611800\tValidation Loss: 246.990733, time5.5\n",
      "Epoch: 237 \tTraining Loss: 48.188323 \tacc: 85.697888\tValidation Loss: 246.674050, time5.5\n",
      "Epoch: 238 \tTraining Loss: 48.743113 \tacc: 85.887282\tValidation Loss: 248.180018, time5.5\n",
      "Epoch: 239 \tTraining Loss: 48.502066 \tacc: 85.772498\tValidation Loss: 247.248890, time5.4\n",
      "Epoch: 240 \tTraining Loss: 48.701025 \tacc: 85.749541\tValidation Loss: 247.584521, time5.5\n",
      "Epoch: 241 \tTraining Loss: 48.551806 \tacc: 85.703627\tValidation Loss: 248.358379, time5.5\n",
      "Epoch: 242 \tTraining Loss: 48.251454 \tacc: 85.571625\tValidation Loss: 248.173039, time5.5\n",
      "Epoch: 243 \tTraining Loss: 48.611111 \tacc: 85.651974\tValidation Loss: 248.513503, time5.5\n",
      "Epoch: 244 \tTraining Loss: 48.412152 \tacc: 85.531451\tValidation Loss: 248.599323, time5.5\n",
      "Epoch: 245 \tTraining Loss: 48.195975 \tacc: 85.583104\tValidation Loss: 247.924952, time5.5\n",
      "Epoch: 246 \tTraining Loss: 48.257193 \tacc: 85.611800\tValidation Loss: 248.804348, time5.5\n",
      "Epoch: 247 \tTraining Loss: 48.341368 \tacc: 85.686410\tValidation Loss: 248.267974, time5.5\n",
      "Epoch: 248 \tTraining Loss: 48.320324 \tacc: 85.497016\tValidation Loss: 248.105248, time5.5\n",
      "Epoch: 249 \tTraining Loss: 48.507805 \tacc: 85.577365\tValidation Loss: 249.002383, time5.4\n",
      "Epoch: 250 \tTraining Loss: 48.314585 \tacc: 85.715106\tValidation Loss: 248.416472, time5.5\n",
      "Epoch: 251 \tTraining Loss: 48.544154 \tacc: 85.674931\tValidation Loss: 249.242708, time5.5\n",
      "Epoch: 252 \tTraining Loss: 48.790940 \tacc: 85.778237\tValidation Loss: 250.202674, time5.5\n",
      "Epoch: 253 \tTraining Loss: 48.502066 \tacc: 85.646235\tValidation Loss: 248.492299, time5.5\n",
      "Epoch: 254 \tTraining Loss: 48.662764 \tacc: 85.990588\tValidation Loss: 248.834808, time5.5\n",
      "Epoch: 255 \tTraining Loss: 48.438935 \tacc: 85.720845\tValidation Loss: 247.761773, time5.5\n",
      "Epoch: 256 \tTraining Loss: 48.496327 \tacc: 85.680670\tValidation Loss: 247.552825, time5.5\n",
      "Epoch: 257 \tTraining Loss: 48.459979 \tacc: 85.847107\tValidation Loss: 248.299736, time5.5\n",
      "Epoch: 258 \tTraining Loss: 48.683808 \tacc: 85.709366\tValidation Loss: 249.745097, time5.5\n",
      "Epoch: 259 \tTraining Loss: 48.414065 \tacc: 85.606061\tValidation Loss: 249.708056, time5.5\n",
      "Epoch: 260 \tTraining Loss: 48.151974 \tacc: 85.588843\tValidation Loss: 248.832960, time5.4\n",
      "Epoch: 261 \tTraining Loss: 48.463805 \tacc: 85.646235\tValidation Loss: 250.152160, time5.5\n",
      "Epoch: 262 \tTraining Loss: 48.303107 \tacc: 85.686410\tValidation Loss: 249.712654, time5.5\n",
      "Epoch: 263 \tTraining Loss: 48.031451 \tacc: 85.657713\tValidation Loss: 249.758356, time5.5\n",
      "Epoch: 264 \tTraining Loss: 48.195975 \tacc: 85.577365\tValidation Loss: 251.129602, time5.5\n",
      "Epoch: 265 \tTraining Loss: 48.588154 \tacc: 85.583104\tValidation Loss: 251.079766, time5.5\n",
      "Epoch: 266 \tTraining Loss: 48.435109 \tacc: 85.583104\tValidation Loss: 249.783810, time5.5\n",
      "Epoch: 267 \tTraining Loss: 48.561371 \tacc: 85.715106\tValidation Loss: 249.789506, time5.5\n",
      "Epoch: 268 \tTraining Loss: 48.113713 \tacc: 85.646235\tValidation Loss: 251.290835, time5.5\n",
      "Epoch: 269 \tTraining Loss: 48.498240 \tacc: 85.778237\tValidation Loss: 249.732076, time5.5\n",
      "Epoch: 270 \tTraining Loss: 48.393021 \tacc: 85.904500\tValidation Loss: 249.775476, time5.5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 271 \tTraining Loss: 48.613024 \tacc: 85.623278\tValidation Loss: 250.599883, time5.4\n",
      "Epoch: 272 \tTraining Loss: 48.576676 \tacc: 85.629017\tValidation Loss: 250.053728, time5.5\n",
      "Epoch: 273 \tTraining Loss: 48.421717 \tacc: 85.537190\tValidation Loss: 250.132370, time5.5\n",
      "Epoch: 274 \tTraining Loss: 48.276324 \tacc: 85.651974\tValidation Loss: 249.340296, time5.5\n",
      "Epoch: 275 \tTraining Loss: 48.209366 \tacc: 85.565886\tValidation Loss: 250.554465, time5.5\n",
      "Epoch: 276 \tTraining Loss: 48.561371 \tacc: 85.824151\tValidation Loss: 249.674629, time5.5\n",
      "Epoch: 277 \tTraining Loss: 48.381543 \tacc: 85.577365\tValidation Loss: 251.053787, time5.5\n",
      "Epoch: 278 \tTraining Loss: 48.542241 \tacc: 85.692149\tValidation Loss: 250.090981, time5.5\n",
      "Epoch: 279 \tTraining Loss: 48.454239 \tacc: 85.697888\tValidation Loss: 250.803042, time5.5\n",
      "Epoch: 280 \tTraining Loss: 48.327977 \tacc: 85.812672\tValidation Loss: 251.416510, time5.5\n",
      "Epoch: 281 \tTraining Loss: 48.561371 \tacc: 85.606061\tValidation Loss: 250.241531, time5.4\n",
      "Epoch: 282 \tTraining Loss: 48.387282 \tacc: 85.657713\tValidation Loss: 249.514135, time5.5\n",
      "Epoch: 283 \tTraining Loss: 48.521197 \tacc: 85.778237\tValidation Loss: 251.559558, time5.5\n",
      "Epoch: 284 \tTraining Loss: 48.415978 \tacc: 85.663453\tValidation Loss: 250.038292, time5.5\n",
      "Epoch: 285 \tTraining Loss: 48.406413 \tacc: 85.720845\tValidation Loss: 250.858397, time5.5\n",
      "Epoch: 286 \tTraining Loss: 47.989363 \tacc: 85.583104\tValidation Loss: 250.637170, time5.5\n",
      "Epoch: 287 \tTraining Loss: 48.257193 \tacc: 85.629017\tValidation Loss: 250.405785, time5.5\n",
      "Epoch: 288 \tTraining Loss: 48.505892 \tacc: 85.881543\tValidation Loss: 250.481021, time5.5\n",
      "Epoch: 289 \tTraining Loss: 48.239976 \tacc: 85.537190\tValidation Loss: 249.986577, time5.5\n",
      "Epoch: 290 \tTraining Loss: 48.492501 \tacc: 85.651974\tValidation Loss: 251.116217, time5.5\n",
      "Epoch: 291 \tTraining Loss: 48.339455 \tacc: 85.548669\tValidation Loss: 251.149360, time5.5\n",
      "Epoch: 292 \tTraining Loss: 48.829201 \tacc: 85.382231\tValidation Loss: 252.139852, time5.4\n",
      "Epoch: 293 \tTraining Loss: 48.299281 \tacc: 85.629017\tValidation Loss: 250.523925, time5.5\n",
      "Epoch: 294 \tTraining Loss: 48.326064 \tacc: 85.720845\tValidation Loss: 250.551264, time5.5\n",
      "Epoch: 295 \tTraining Loss: 48.720156 \tacc: 85.577365\tValidation Loss: 250.436796, time5.5\n",
      "Epoch: 296 \tTraining Loss: 48.167279 \tacc: 85.720845\tValidation Loss: 250.081705, time5.5\n",
      "Epoch: 297 \tTraining Loss: 48.637894 \tacc: 85.525712\tValidation Loss: 249.448728, time5.5\n",
      "Epoch: 298 \tTraining Loss: 48.402586 \tacc: 85.623278\tValidation Loss: 249.676746, time5.5\n",
      "Epoch: 299 \tTraining Loss: 48.597720 \tacc: 85.864325\tValidation Loss: 250.011371, time5.5\n",
      "Epoch: 300 \tTraining Loss: 48.282063 \tacc: 85.560147\tValidation Loss: 250.722055, time5.5\n",
      "Epoch: 301 \tTraining Loss: 48.569024 \tacc: 85.634757\tValidation Loss: 250.919812, time5.4\n",
      "Epoch: 302 \tTraining Loss: 48.394934 \tacc: 85.565886\tValidation Loss: 251.079198, time5.4\n",
      "Epoch: 303 \tTraining Loss: 48.458066 \tacc: 85.715106\tValidation Loss: 251.374635, time5.3\n",
      "Epoch: 304 \tTraining Loss: 48.681895 \tacc: 85.646235\tValidation Loss: 250.162395, time5.4\n",
      "Epoch: 305 \tTraining Loss: 48.630242 \tacc: 85.680670\tValidation Loss: 251.001450, time5.5\n",
      "Epoch: 306 \tTraining Loss: 48.599633 \tacc: 85.600321\tValidation Loss: 249.432102, time5.5\n",
      "Epoch: 307 \tTraining Loss: 48.274411 \tacc: 85.657713\tValidation Loss: 248.675861, time5.5\n",
      "Epoch: 308 \tTraining Loss: 48.534588 \tacc: 85.433884\tValidation Loss: 251.387069, time5.5\n",
      "Epoch: 309 \tTraining Loss: 47.987450 \tacc: 85.795455\tValidation Loss: 250.764698, time5.5\n",
      "Epoch: 310 \tTraining Loss: 48.144322 \tacc: 85.399449\tValidation Loss: 251.757476, time5.5\n",
      "Epoch: 311 \tTraining Loss: 48.014233 \tacc: 85.755280\tValidation Loss: 249.238148, time5.5\n",
      "Epoch: 312 \tTraining Loss: 48.136670 \tacc: 85.439624\tValidation Loss: 251.719854, time5.4\n",
      "Epoch: 313 \tTraining Loss: 48.171105 \tacc: 85.577365\tValidation Loss: 251.333990, time5.4\n",
      "Epoch: 314 \tTraining Loss: 48.201714 \tacc: 85.726584\tValidation Loss: 250.720445, time5.3\n",
      "Epoch: 315 \tTraining Loss: 48.349021 \tacc: 85.703627\tValidation Loss: 248.830234, time5.3\n",
      "Epoch: 316 \tTraining Loss: 48.406413 \tacc: 85.789715\tValidation Loss: 250.325640, time5.3\n",
      "Epoch: 317 \tTraining Loss: 48.098408 \tacc: 85.600321\tValidation Loss: 250.638337, time5.3\n",
      "Epoch: 318 \tTraining Loss: 48.151974 \tacc: 85.927456\tValidation Loss: 252.052847, time5.5\n",
      "Epoch: 319 \tTraining Loss: 48.331803 \tacc: 85.703627\tValidation Loss: 250.888737, time5.5\n",
      "Epoch: 320 \tTraining Loss: 48.410239 \tacc: 85.600321\tValidation Loss: 250.714374, time5.5\n",
      "Epoch: 321 \tTraining Loss: 48.347107 \tacc: 85.657713\tValidation Loss: 251.649981, time5.5\n",
      "Epoch: 322 \tTraining Loss: 48.415978 \tacc: 85.617539\tValidation Loss: 250.382089, time5.4\n",
      "Epoch: 323 \tTraining Loss: 48.125191 \tacc: 85.726584\tValidation Loss: 250.839222, time5.5\n",
      "Epoch: 324 \tTraining Loss: 48.387282 \tacc: 85.789715\tValidation Loss: 249.493739, time5.3\n",
      "Epoch: 325 \tTraining Loss: 48.239976 \tacc: 85.761019\tValidation Loss: 251.134071, time5.3\n",
      "Epoch: 326 \tTraining Loss: 48.163453 \tacc: 85.617539\tValidation Loss: 251.746613, time5.3\n",
      "Epoch: 327 \tTraining Loss: 48.217019 \tacc: 85.560147\tValidation Loss: 250.347199, time5.4\n",
      "Epoch: 328 \tTraining Loss: 48.236149 \tacc: 85.600321\tValidation Loss: 250.530675, time5.3\n",
      "Epoch: 329 \tTraining Loss: 48.377717 \tacc: 85.669192\tValidation Loss: 250.652696, time5.3\n",
      "Epoch: 330 \tTraining Loss: 48.398760 \tacc: 85.697888\tValidation Loss: 251.886311, time5.3\n",
      "Epoch: 331 \tTraining Loss: 48.370064 \tacc: 85.640496\tValidation Loss: 252.068330, time5.3\n",
      "Epoch: 332 \tTraining Loss: 48.211279 \tacc: 85.795455\tValidation Loss: 252.809947, time5.3\n",
      "Epoch: 333 \tTraining Loss: 48.291628 \tacc: 85.893021\tValidation Loss: 250.495891, time5.4\n",
      "Epoch: 334 \tTraining Loss: 48.067799 \tacc: 85.692149\tValidation Loss: 250.802905, time5.4\n",
      "Epoch: 335 \tTraining Loss: 48.303107 \tacc: 85.806933\tValidation Loss: 250.252187, time5.4\n",
      "Epoch: 336 \tTraining Loss: 48.678069 \tacc: 85.709366\tValidation Loss: 251.799783, time5.3\n",
      "Epoch: 337 \tTraining Loss: 48.534588 \tacc: 85.629017\tValidation Loss: 251.546773, time5.5\n",
      "Epoch: 338 \tTraining Loss: 48.077365 \tacc: 85.657713\tValidation Loss: 250.051371, time5.6\n",
      "Epoch: 339 \tTraining Loss: 48.029538 \tacc: 85.600321\tValidation Loss: 251.489967, time5.6\n",
      "Epoch: 340 \tTraining Loss: 48.622590 \tacc: 85.680670\tValidation Loss: 251.004421, time5.5\n",
      "Epoch: 341 \tTraining Loss: 48.062060 \tacc: 85.577365\tValidation Loss: 252.529316, time5.4\n",
      "Epoch: 342 \tTraining Loss: 48.437022 \tacc: 85.743802\tValidation Loss: 252.537928, time5.4\n",
      "Epoch: 343 \tTraining Loss: 48.127104 \tacc: 85.571625\tValidation Loss: 251.770679, time5.3\n",
      "Epoch: 344 \tTraining Loss: 48.360499 \tacc: 85.703627\tValidation Loss: 251.950655, time5.3\n",
      "Epoch: 345 \tTraining Loss: 47.958754 \tacc: 85.525712\tValidation Loss: 250.915865, time5.3\n",
      "Epoch: 346 \tTraining Loss: 48.205540 \tacc: 85.703627\tValidation Loss: 251.722483, time5.2\n",
      "Epoch: 347 \tTraining Loss: 48.530762 \tacc: 85.686410\tValidation Loss: 251.088015, time5.3\n",
      "Epoch: 348 \tTraining Loss: 47.983624 \tacc: 85.571625\tValidation Loss: 250.664181, time5.3\n",
      "Epoch: 349 \tTraining Loss: 47.943450 \tacc: 85.651974\tValidation Loss: 251.973526, time5.3\n",
      "Epoch: 350 \tTraining Loss: 48.192149 \tacc: 85.594582\tValidation Loss: 251.832725, time5.3\n",
      "Epoch: 351 \tTraining Loss: 48.220845 \tacc: 85.611800\tValidation Loss: 251.681767, time5.3\n",
      "Epoch: 352 \tTraining Loss: 48.435109 \tacc: 85.778237\tValidation Loss: 251.060510, time5.3\n",
      "Epoch: 353 \tTraining Loss: 48.245715 \tacc: 85.623278\tValidation Loss: 252.035816, time5.3\n",
      "Epoch: 354 \tTraining Loss: 48.293541 \tacc: 85.669192\tValidation Loss: 250.523964, time5.3\n",
      "Epoch: 355 \tTraining Loss: 48.674242 \tacc: 85.611800\tValidation Loss: 251.060643, time5.3\n",
      "Epoch: 356 \tTraining Loss: 48.079278 \tacc: 85.749541\tValidation Loss: 251.585532, time5.2\n",
      "Epoch: 357 \tTraining Loss: 48.098408 \tacc: 85.542929\tValidation Loss: 251.173054, time5.3\n",
      "Epoch: 358 \tTraining Loss: 47.991276 \tacc: 85.502755\tValidation Loss: 250.868578, time5.3\n",
      "Epoch: 359 \tTraining Loss: 47.956841 \tacc: 85.468320\tValidation Loss: 251.756553, time5.3\n",
      "Epoch: 360 \tTraining Loss: 48.257193 \tacc: 85.984848\tValidation Loss: 251.592421, time5.3\n",
      "Epoch: 361 \tTraining Loss: 48.404500 \tacc: 85.422406\tValidation Loss: 252.956668, time5.3\n",
      "Epoch: 362 \tTraining Loss: 48.014233 \tacc: 85.640496\tValidation Loss: 250.873436, time5.3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 363 \tTraining Loss: 48.303107 \tacc: 85.600321\tValidation Loss: 251.435419, time5.3\n",
      "Epoch: 364 \tTraining Loss: 48.586241 \tacc: 85.634757\tValidation Loss: 251.694276, time5.3\n",
      "Epoch: 365 \tTraining Loss: 48.165366 \tacc: 85.583104\tValidation Loss: 251.865300, time5.4\n",
      "Epoch: 366 \tTraining Loss: 47.878405 \tacc: 85.657713\tValidation Loss: 251.806623, time5.5\n",
      "Epoch: 367 \tTraining Loss: 48.048669 \tacc: 85.502755\tValidation Loss: 253.792244, time5.5\n",
      "Epoch: 368 \tTraining Loss: 48.167279 \tacc: 85.778237\tValidation Loss: 253.080118, time5.4\n",
      "Epoch: 369 \tTraining Loss: 48.679982 \tacc: 85.433884\tValidation Loss: 250.567208, time5.3\n",
      "Epoch: 370 \tTraining Loss: 47.972146 \tacc: 85.405188\tValidation Loss: 251.282888, time5.3\n",
      "Epoch: 371 \tTraining Loss: 48.148148 \tacc: 85.588843\tValidation Loss: 252.753646, time5.3\n",
      "Epoch: 372 \tTraining Loss: 48.243802 \tacc: 85.560147\tValidation Loss: 250.417191, time5.3\n",
      "Epoch: 373 \tTraining Loss: 47.899449 \tacc: 85.485537\tValidation Loss: 252.452239, time5.3\n",
      "Epoch: 374 \tTraining Loss: 48.486762 \tacc: 85.456841\tValidation Loss: 252.673291, time5.3\n",
      "Epoch: 375 \tTraining Loss: 48.333716 \tacc: 85.571625\tValidation Loss: 251.059267, time5.3\n",
      "Epoch: 376 \tTraining Loss: 48.391108 \tacc: 85.697888\tValidation Loss: 252.033663, time5.3\n",
      "Epoch: 377 \tTraining Loss: 48.016146 \tacc: 85.456841\tValidation Loss: 252.054188, time5.3\n",
      "Epoch: 378 \tTraining Loss: 47.918580 \tacc: 85.514233\tValidation Loss: 253.994188, time5.3\n",
      "Epoch: 379 \tTraining Loss: 48.107974 \tacc: 85.588843\tValidation Loss: 253.665676, time5.3\n",
      "Epoch: 380 \tTraining Loss: 48.523110 \tacc: 85.531451\tValidation Loss: 252.329286, time5.3\n",
      "Epoch: 381 \tTraining Loss: 47.918580 \tacc: 85.686410\tValidation Loss: 252.805828, time5.3\n",
      "Epoch: 382 \tTraining Loss: 48.134757 \tacc: 85.514233\tValidation Loss: 253.577204, time5.3\n",
      "Epoch: 383 \tTraining Loss: 48.454239 \tacc: 85.743802\tValidation Loss: 252.602906, time5.3\n",
      "Epoch: 384 \tTraining Loss: 48.295455 \tacc: 85.732323\tValidation Loss: 252.926684, time5.3\n",
      "Epoch: 385 \tTraining Loss: 48.398760 \tacc: 85.617539\tValidation Loss: 254.012978, time5.3\n",
      "Epoch: 386 \tTraining Loss: 48.201714 \tacc: 85.703627\tValidation Loss: 252.869132, time5.3\n",
      "Epoch: 387 \tTraining Loss: 48.431283 \tacc: 85.525712\tValidation Loss: 252.474077, time5.3\n",
      "Epoch: 388 \tTraining Loss: 47.778926 \tacc: 85.600321\tValidation Loss: 253.223730, time5.3\n",
      "Epoch: 389 \tTraining Loss: 48.205540 \tacc: 85.806933\tValidation Loss: 253.627663, time5.2\n",
      "Epoch: 390 \tTraining Loss: 48.329890 \tacc: 85.600321\tValidation Loss: 253.950986, time5.3\n",
      "Epoch: 391 \tTraining Loss: 48.335629 \tacc: 85.657713\tValidation Loss: 254.062947, time5.3\n",
      "Epoch: 392 \tTraining Loss: 48.215106 \tacc: 85.686410\tValidation Loss: 253.560917, time5.3\n",
      "Epoch: 393 \tTraining Loss: 47.991276 \tacc: 85.732323\tValidation Loss: 252.755871, time5.3\n",
      "Epoch: 394 \tTraining Loss: 47.847796 \tacc: 85.617539\tValidation Loss: 254.818939, time5.3\n",
      "Epoch: 395 \tTraining Loss: 48.035277 \tacc: 85.416667\tValidation Loss: 253.150059, time5.3\n",
      "Epoch: 396 \tTraining Loss: 47.755969 \tacc: 85.634757\tValidation Loss: 253.087171, time5.3\n",
      "Epoch: 397 \tTraining Loss: 48.102234 \tacc: 85.623278\tValidation Loss: 254.376747, time5.3\n",
      "Epoch: 398 \tTraining Loss: 48.098408 \tacc: 85.726584\tValidation Loss: 253.520534, time5.3\n",
      "Epoch: 399 \tTraining Loss: 48.220845 \tacc: 85.824151\tValidation Loss: 254.487631, time5.2\n",
      "Epoch: 400 \tTraining Loss: 48.106061 \tacc: 85.577365\tValidation Loss: 252.942076, time5.3\n",
      "Epoch: 401 \tTraining Loss: 48.297368 \tacc: 85.623278\tValidation Loss: 253.117291, time5.3\n",
      "Epoch: 402 \tTraining Loss: 48.381543 \tacc: 85.657713\tValidation Loss: 253.370792, time5.3\n",
      "Epoch: 403 \tTraining Loss: 48.159627 \tacc: 85.634757\tValidation Loss: 254.138320, time5.3\n",
      "Epoch: 404 \tTraining Loss: 48.069712 \tacc: 85.433884\tValidation Loss: 254.659439, time5.3\n",
      "Epoch: 405 \tTraining Loss: 48.299281 \tacc: 85.692149\tValidation Loss: 254.853865, time5.3\n",
      "Epoch: 406 \tTraining Loss: 48.058234 \tacc: 85.692149\tValidation Loss: 254.221299, time5.3\n",
      "Epoch: 407 \tTraining Loss: 48.503979 \tacc: 85.692149\tValidation Loss: 253.250628, time5.3\n",
      "Epoch: 408 \tTraining Loss: 48.077365 \tacc: 85.525712\tValidation Loss: 253.154568, time5.3\n",
      "Epoch: 409 \tTraining Loss: 48.010407 \tacc: 85.514233\tValidation Loss: 254.247964, time5.3\n",
      "Epoch: 410 \tTraining Loss: 48.035277 \tacc: 85.657713\tValidation Loss: 254.580504, time5.3\n",
      "Epoch: 411 \tTraining Loss: 47.914754 \tacc: 85.508494\tValidation Loss: 254.377705, time5.3\n",
      "Epoch: 412 \tTraining Loss: 47.979798 \tacc: 85.594582\tValidation Loss: 253.928895, time5.3\n",
      "Epoch: 413 \tTraining Loss: 48.104148 \tacc: 85.835629\tValidation Loss: 252.987288, time5.3\n",
      "Epoch: 414 \tTraining Loss: 48.140496 \tacc: 85.663453\tValidation Loss: 252.687511, time5.3\n",
      "Epoch: 415 \tTraining Loss: 47.974059 \tacc: 85.651974\tValidation Loss: 255.204003, time5.3\n",
      "Epoch: 416 \tTraining Loss: 48.016146 \tacc: 85.629017\tValidation Loss: 253.962280, time5.3\n",
      "Epoch: 417 \tTraining Loss: 47.941537 \tacc: 85.674931\tValidation Loss: 253.066731, time5.3\n",
      "Epoch: 418 \tTraining Loss: 48.151974 \tacc: 85.416667\tValidation Loss: 254.807308, time5.3\n",
      "Epoch: 419 \tTraining Loss: 48.295455 \tacc: 85.893021\tValidation Loss: 254.030415, time5.3\n",
      "Epoch: 420 \tTraining Loss: 47.953015 \tacc: 85.703627\tValidation Loss: 254.882863, time5.3\n",
      "Epoch: 421 \tTraining Loss: 48.448500 \tacc: 85.738062\tValidation Loss: 253.879100, time5.3\n",
      "Epoch: 422 \tTraining Loss: 47.979798 \tacc: 85.588843\tValidation Loss: 253.331653, time5.3\n",
      "Epoch: 423 \tTraining Loss: 47.847796 \tacc: 85.835629\tValidation Loss: 253.541814, time5.3\n",
      "Epoch: 424 \tTraining Loss: 48.071625 \tacc: 85.560147\tValidation Loss: 254.747474, time5.3\n",
      "Epoch: 425 \tTraining Loss: 47.979798 \tacc: 85.519972\tValidation Loss: 256.695927, time5.3\n",
      "Epoch: 426 \tTraining Loss: 47.876492 \tacc: 85.651974\tValidation Loss: 255.764053, time5.3\n",
      "Epoch: 427 \tTraining Loss: 48.044842 \tacc: 85.674931\tValidation Loss: 256.020473, time5.3\n",
      "Epoch: 428 \tTraining Loss: 48.211279 \tacc: 85.703627\tValidation Loss: 254.401571, time5.3\n",
      "Epoch: 429 \tTraining Loss: 48.016146 \tacc: 86.007805\tValidation Loss: 253.755651, time5.3\n",
      "Epoch: 430 \tTraining Loss: 48.398760 \tacc: 85.686410\tValidation Loss: 254.567629, time5.3\n",
      "Epoch: 431 \tTraining Loss: 47.842057 \tacc: 85.669192\tValidation Loss: 254.180619, time5.2\n",
      "Epoch: 432 \tTraining Loss: 47.855448 \tacc: 85.715106\tValidation Loss: 254.495568, time5.3\n",
      "Epoch: 433 \tTraining Loss: 47.744490 \tacc: 85.669192\tValidation Loss: 254.480991, time5.3\n",
      "Epoch: 434 \tTraining Loss: 47.838231 \tacc: 85.554408\tValidation Loss: 254.452413, time5.3\n",
      "Epoch: 435 \tTraining Loss: 48.257193 \tacc: 85.703627\tValidation Loss: 254.382773, time5.3\n",
      "Epoch: 436 \tTraining Loss: 48.002755 \tacc: 85.399449\tValidation Loss: 255.662307, time5.4\n",
      "Epoch: 437 \tTraining Loss: 47.648837 \tacc: 85.560147\tValidation Loss: 255.531471, time5.3\n",
      "Epoch: 438 \tTraining Loss: 48.130931 \tacc: 85.485537\tValidation Loss: 255.341022, time5.3\n",
      "Epoch: 439 \tTraining Loss: 47.993189 \tacc: 85.571625\tValidation Loss: 256.587875, time5.3\n",
      "Epoch: 440 \tTraining Loss: 48.019972 \tacc: 85.686410\tValidation Loss: 256.607086, time5.3\n",
      "Epoch: 441 \tTraining Loss: 48.259106 \tacc: 85.560147\tValidation Loss: 256.563159, time5.3\n",
      "Epoch: 442 \tTraining Loss: 48.180670 \tacc: 85.600321\tValidation Loss: 254.752335, time5.2\n",
      "Epoch: 443 \tTraining Loss: 47.928145 \tacc: 85.990588\tValidation Loss: 254.300007, time5.3\n",
      "Epoch: 444 \tTraining Loss: 48.027625 \tacc: 85.703627\tValidation Loss: 253.481274, time5.3\n",
      "Epoch: 445 \tTraining Loss: 47.928145 \tacc: 85.743802\tValidation Loss: 254.289722, time5.4\n",
      "Epoch: 446 \tTraining Loss: 47.897536 \tacc: 85.537190\tValidation Loss: 256.052906, time5.3\n",
      "Epoch: 447 \tTraining Loss: 48.058234 \tacc: 85.571625\tValidation Loss: 253.030624, time5.3\n",
      "Epoch: 448 \tTraining Loss: 48.090756 \tacc: 85.778237\tValidation Loss: 254.821551, time5.3\n",
      "Epoch: 449 \tTraining Loss: 48.129017 \tacc: 85.468320\tValidation Loss: 255.480045, time5.3\n",
      "Epoch: 450 \tTraining Loss: 47.815274 \tacc: 85.445363\tValidation Loss: 256.160697, time5.3\n",
      "Epoch: 451 \tTraining Loss: 48.148148 \tacc: 85.439624\tValidation Loss: 254.527251, time5.3\n",
      "Epoch: 452 \tTraining Loss: 48.048669 \tacc: 85.606061\tValidation Loss: 254.512855, time5.3\n",
      "Epoch: 453 \tTraining Loss: 48.280150 \tacc: 85.439624\tValidation Loss: 254.290008, time5.2\n",
      "Epoch: 454 \tTraining Loss: 48.058234 \tacc: 85.606061\tValidation Loss: 255.416815, time5.3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 455 \tTraining Loss: 47.922406 \tacc: 85.778237\tValidation Loss: 254.878018, time5.3\n",
      "Epoch: 456 \tTraining Loss: 48.044842 \tacc: 85.629017\tValidation Loss: 255.365965, time5.3\n",
      "Epoch: 457 \tTraining Loss: 47.836318 \tacc: 85.709366\tValidation Loss: 255.117330, time5.3\n",
      "Epoch: 458 \tTraining Loss: 48.217019 \tacc: 85.583104\tValidation Loss: 255.140751, time5.3\n",
      "Epoch: 459 \tTraining Loss: 48.008494 \tacc: 85.732323\tValidation Loss: 256.116070, time5.3\n",
      "Epoch: 460 \tTraining Loss: 47.746403 \tacc: 85.806933\tValidation Loss: 255.443198, time5.3\n",
      "Epoch: 461 \tTraining Loss: 48.144322 \tacc: 85.445363\tValidation Loss: 255.721088, time5.3\n",
      "Epoch: 462 \tTraining Loss: 47.763621 \tacc: 85.680670\tValidation Loss: 257.647625, time5.3\n",
      "Epoch: 463 \tTraining Loss: 48.370064 \tacc: 85.783976\tValidation Loss: 255.266905, time5.3\n",
      "Epoch: 464 \tTraining Loss: 47.884144 \tacc: 85.548669\tValidation Loss: 255.556856, time5.2\n",
      "Epoch: 465 \tTraining Loss: 48.129017 \tacc: 85.617539\tValidation Loss: 255.291610, time5.3\n",
      "Epoch: 466 \tTraining Loss: 47.719620 \tacc: 85.646235\tValidation Loss: 255.118121, time5.3\n",
      "Epoch: 467 \tTraining Loss: 47.838231 \tacc: 85.594582\tValidation Loss: 254.089533, time5.3\n",
      "Epoch: 468 \tTraining Loss: 47.635445 \tacc: 85.692149\tValidation Loss: 255.884984, time5.3\n",
      "Epoch: 469 \tTraining Loss: 47.918580 \tacc: 85.697888\tValidation Loss: 254.940150, time5.3\n",
      "Epoch: 470 \tTraining Loss: 47.895623 \tacc: 85.640496\tValidation Loss: 256.466714, time5.3\n",
      "Epoch: 471 \tTraining Loss: 47.874579 \tacc: 85.709366\tValidation Loss: 254.555341, time5.3\n",
      "Epoch: 472 \tTraining Loss: 48.297368 \tacc: 85.651974\tValidation Loss: 254.238800, time5.3\n",
      "Epoch: 473 \tTraining Loss: 47.924319 \tacc: 85.565886\tValidation Loss: 255.875768, time5.3\n",
      "Epoch: 474 \tTraining Loss: 47.819100 \tacc: 85.393710\tValidation Loss: 257.045536, time5.3\n",
      "Epoch: 475 \tTraining Loss: 47.870753 \tacc: 85.789715\tValidation Loss: 257.119218, time5.3\n",
      "Epoch: 476 \tTraining Loss: 48.048669 \tacc: 85.847107\tValidation Loss: 257.164543, time5.3\n",
      "Epoch: 477 \tTraining Loss: 47.930058 \tacc: 85.761019\tValidation Loss: 255.014049, time5.3\n",
      "Epoch: 478 \tTraining Loss: 47.675620 \tacc: 85.674931\tValidation Loss: 256.291307, time5.3\n",
      "Epoch: 479 \tTraining Loss: 48.029538 \tacc: 85.554408\tValidation Loss: 257.563285, time5.3\n",
      "Epoch: 480 \tTraining Loss: 47.931971 \tacc: 85.531451\tValidation Loss: 256.927315, time5.3\n",
      "Epoch: 481 \tTraining Loss: 48.060147 \tacc: 85.709366\tValidation Loss: 257.617036, time5.3\n",
      "Epoch: 482 \tTraining Loss: 47.717707 \tacc: 85.686410\tValidation Loss: 257.032173, time5.3\n",
      "Epoch: 483 \tTraining Loss: 48.018059 \tacc: 85.709366\tValidation Loss: 256.951263, time5.3\n",
      "Epoch: 484 \tTraining Loss: 47.734925 \tacc: 85.583104\tValidation Loss: 257.450563, time5.3\n",
      "Epoch: 485 \tTraining Loss: 47.865014 \tacc: 85.560147\tValidation Loss: 258.943813, time5.3\n",
      "Epoch: 486 \tTraining Loss: 48.197888 \tacc: 85.519972\tValidation Loss: 257.155059, time5.3\n",
      "Epoch: 487 \tTraining Loss: 47.947276 \tacc: 85.680670\tValidation Loss: 257.362608, time5.3\n",
      "Epoch: 488 \tTraining Loss: 47.826752 \tacc: 85.726584\tValidation Loss: 257.215888, time5.3\n",
      "Epoch: 489 \tTraining Loss: 47.767447 \tacc: 85.651974\tValidation Loss: 256.065624, time5.3\n",
      "Epoch: 490 \tTraining Loss: 47.524487 \tacc: 85.611800\tValidation Loss: 255.920765, time5.3\n",
      "Epoch: 491 \tTraining Loss: 47.798056 \tacc: 85.502755\tValidation Loss: 254.711981, time5.3\n",
      "Epoch: 492 \tTraining Loss: 47.514922 \tacc: 85.519972\tValidation Loss: 256.572166, time5.3\n",
      "Epoch: 493 \tTraining Loss: 47.717707 \tacc: 85.629017\tValidation Loss: 255.594613, time5.3\n",
      "Epoch: 494 \tTraining Loss: 47.847796 \tacc: 85.525712\tValidation Loss: 255.038501, time5.3\n",
      "Epoch: 495 \tTraining Loss: 47.958754 \tacc: 85.606061\tValidation Loss: 255.018598, time5.3\n",
      "Epoch: 496 \tTraining Loss: 48.062060 \tacc: 85.542929\tValidation Loss: 255.408889, time5.2\n",
      "Epoch: 497 \tTraining Loss: 47.876492 \tacc: 85.519972\tValidation Loss: 255.842593, time5.3\n",
      "Epoch: 498 \tTraining Loss: 47.620141 \tacc: 85.514233\tValidation Loss: 255.055669, time5.3\n",
      "Epoch: 499 \tTraining Loss: 47.822926 \tacc: 85.588843\tValidation Loss: 255.052751, time5.3\n",
      "Epoch: 500 \tTraining Loss: 48.081191 \tacc: 85.531451\tValidation Loss: 256.933634, time5.3\n",
      "Epoch: 501 \tTraining Loss: 47.568488 \tacc: 85.651974\tValidation Loss: 255.622390, time5.3\n",
      "Epoch: 502 \tTraining Loss: 47.689011 \tacc: 85.703627\tValidation Loss: 256.605424, time5.3\n",
      "Epoch: 503 \tTraining Loss: 48.046755 \tacc: 85.623278\tValidation Loss: 255.583190, time5.3\n",
      "Epoch: 504 \tTraining Loss: 47.924319 \tacc: 85.560147\tValidation Loss: 256.192236, time5.3\n",
      "Epoch: 505 \tTraining Loss: 47.951102 \tacc: 85.663453\tValidation Loss: 255.908239, time5.3\n",
      "Epoch: 506 \tTraining Loss: 48.151974 \tacc: 85.646235\tValidation Loss: 255.803983, time5.2\n",
      "Epoch: 507 \tTraining Loss: 48.232323 \tacc: 85.600321\tValidation Loss: 254.875771, time5.3\n",
      "Epoch: 508 \tTraining Loss: 47.878405 \tacc: 85.629017\tValidation Loss: 254.901273, time5.3\n",
      "Epoch: 509 \tTraining Loss: 48.002755 \tacc: 85.548669\tValidation Loss: 255.919613, time5.3\n",
      "Epoch: 510 \tTraining Loss: 47.780839 \tacc: 85.428145\tValidation Loss: 256.605327, time5.3\n",
      "Epoch: 511 \tTraining Loss: 48.157713 \tacc: 85.502755\tValidation Loss: 256.729414, time5.3\n",
      "Epoch: 512 \tTraining Loss: 47.884144 \tacc: 85.611800\tValidation Loss: 256.988377, time5.3\n",
      "Epoch: 513 \tTraining Loss: 48.283976 \tacc: 85.577365\tValidation Loss: 256.914572, time5.3\n",
      "Epoch: 514 \tTraining Loss: 47.319789 \tacc: 85.531451\tValidation Loss: 257.950698, time5.3\n",
      "Epoch: 515 \tTraining Loss: 47.780839 \tacc: 85.657713\tValidation Loss: 258.957550, time5.3\n",
      "Epoch: 516 \tTraining Loss: 47.725360 \tacc: 85.663453\tValidation Loss: 258.442611, time5.3\n",
      "Epoch: 517 \tTraining Loss: 48.006581 \tacc: 85.491276\tValidation Loss: 259.078902, time5.2\n",
      "Epoch: 518 \tTraining Loss: 47.777013 \tacc: 85.686410\tValidation Loss: 257.224642, time5.3\n",
      "Epoch: 519 \tTraining Loss: 47.993189 \tacc: 85.623278\tValidation Loss: 257.182878, time5.3\n",
      "Epoch: 520 \tTraining Loss: 47.761708 \tacc: 85.651974\tValidation Loss: 257.395462, time5.3\n",
      "Epoch: 521 \tTraining Loss: 47.687098 \tacc: 85.565886\tValidation Loss: 258.106577, time5.3\n",
      "Epoch: 522 \tTraining Loss: 47.704316 \tacc: 85.548669\tValidation Loss: 256.322580, time5.3\n",
      "Epoch: 523 \tTraining Loss: 48.157713 \tacc: 85.560147\tValidation Loss: 255.987615, time5.3\n",
      "Epoch: 524 \tTraining Loss: 48.035277 \tacc: 85.657713\tValidation Loss: 256.467058, time5.3\n",
      "Epoch: 525 \tTraining Loss: 47.671794 \tacc: 85.554408\tValidation Loss: 258.118635, time5.3\n",
      "Epoch: 526 \tTraining Loss: 47.627793 \tacc: 85.468320\tValidation Loss: 259.562874, time5.3\n",
      "Epoch: 527 \tTraining Loss: 47.819100 \tacc: 85.548669\tValidation Loss: 258.042995, time5.3\n",
      "Epoch: 528 \tTraining Loss: 47.886058 \tacc: 85.508494\tValidation Loss: 258.127255, time5.2\n",
      "Epoch: 529 \tTraining Loss: 47.394399 \tacc: 85.577365\tValidation Loss: 259.714576, time5.3\n",
      "Epoch: 530 \tTraining Loss: 48.190236 \tacc: 85.663453\tValidation Loss: 260.030174, time5.3\n",
      "Epoch: 531 \tTraining Loss: 47.534053 \tacc: 85.491276\tValidation Loss: 257.694873, time5.3\n",
      "Epoch: 532 \tTraining Loss: 47.696664 \tacc: 85.692149\tValidation Loss: 256.871480, time5.4\n",
      "Epoch: 533 \tTraining Loss: 47.912841 \tacc: 85.588843\tValidation Loss: 258.295178, time5.3\n",
      "Epoch: 534 \tTraining Loss: 47.907101 \tacc: 85.502755\tValidation Loss: 258.539126, time5.3\n",
      "Epoch: 535 \tTraining Loss: 48.115626 \tacc: 85.577365\tValidation Loss: 258.567212, time5.3\n",
      "Epoch: 536 \tTraining Loss: 48.052495 \tacc: 85.703627\tValidation Loss: 257.824002, time5.3\n",
      "Epoch: 537 \tTraining Loss: 47.721534 \tacc: 85.657713\tValidation Loss: 258.887539, time5.3\n",
      "Epoch: 538 \tTraining Loss: 47.834405 \tacc: 85.571625\tValidation Loss: 258.614997, time5.4\n",
      "Epoch: 539 \tTraining Loss: 47.744490 \tacc: 85.542929\tValidation Loss: 257.488733, time5.2\n",
      "Epoch: 540 \tTraining Loss: 47.855448 \tacc: 85.456841\tValidation Loss: 258.882706, time5.3\n",
      "Epoch: 541 \tTraining Loss: 47.757882 \tacc: 85.439624\tValidation Loss: 257.324113, time5.3\n",
      "Epoch: 542 \tTraining Loss: 47.652663 \tacc: 85.606061\tValidation Loss: 257.063580, time5.3\n",
      "Epoch: 543 \tTraining Loss: 47.843970 \tacc: 85.387971\tValidation Loss: 258.716424, time5.4\n",
      "Epoch: 544 \tTraining Loss: 48.085017 \tacc: 85.548669\tValidation Loss: 258.459021, time5.3\n",
      "Epoch: 545 \tTraining Loss: 48.004668 \tacc: 85.583104\tValidation Loss: 258.051523, time5.3\n",
      "Epoch: 546 \tTraining Loss: 47.761708 \tacc: 85.502755\tValidation Loss: 256.608954, time5.5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 547 \tTraining Loss: 47.794230 \tacc: 85.743802\tValidation Loss: 257.278059, time5.5\n",
      "Epoch: 548 \tTraining Loss: 48.000842 \tacc: 85.657713\tValidation Loss: 258.398758, time5.5\n",
      "Epoch: 549 \tTraining Loss: 47.646924 \tacc: 85.560147\tValidation Loss: 258.776334, time5.5\n",
      "Epoch: 550 \tTraining Loss: 47.859275 \tacc: 85.623278\tValidation Loss: 259.194797, time5.5\n",
      "Epoch: 551 \tTraining Loss: 47.954928 \tacc: 85.651974\tValidation Loss: 258.442276, time5.5\n",
      "Epoch: 552 \tTraining Loss: 48.010407 \tacc: 85.594582\tValidation Loss: 258.599555, time5.5\n",
      "Epoch: 553 \tTraining Loss: 47.576140 \tacc: 85.571625\tValidation Loss: 257.452895, time5.5\n",
      "Epoch: 554 \tTraining Loss: 47.612489 \tacc: 85.491276\tValidation Loss: 258.914123, time5.5\n",
      "Epoch: 555 \tTraining Loss: 47.815274 \tacc: 85.508494\tValidation Loss: 256.517364, time5.5\n",
      "Epoch: 556 \tTraining Loss: 47.761708 \tacc: 85.571625\tValidation Loss: 258.697702, time5.5\n",
      "Epoch: 557 \tTraining Loss: 48.092669 \tacc: 85.577365\tValidation Loss: 258.274007, time5.4\n",
      "Epoch: 558 \tTraining Loss: 47.945363 \tacc: 85.703627\tValidation Loss: 256.535236, time5.4\n",
      "Epoch: 559 \tTraining Loss: 47.943450 \tacc: 85.485537\tValidation Loss: 258.211617, time5.3\n",
      "Epoch: 560 \tTraining Loss: 47.983624 \tacc: 85.554408\tValidation Loss: 257.247309, time5.3\n",
      "Epoch: 561 \tTraining Loss: 48.102234 \tacc: 85.537190\tValidation Loss: 257.595589, time5.5\n",
      "Epoch: 562 \tTraining Loss: 47.513009 \tacc: 85.583104\tValidation Loss: 259.505874, time5.4\n",
      "Epoch: 563 \tTraining Loss: 47.740664 \tacc: 85.583104\tValidation Loss: 257.921337, time5.4\n",
      "Epoch: 564 \tTraining Loss: 47.729186 \tacc: 85.651974\tValidation Loss: 257.303980, time5.4\n",
      "Epoch: 565 \tTraining Loss: 47.486226 \tacc: 85.497016\tValidation Loss: 259.078319, time5.3\n",
      "Epoch: 566 \tTraining Loss: 47.805709 \tacc: 85.497016\tValidation Loss: 258.860346, time5.4\n",
      "Epoch: 567 \tTraining Loss: 47.601010 \tacc: 85.726584\tValidation Loss: 257.938677, time5.3\n",
      "Epoch: 568 \tTraining Loss: 47.968320 \tacc: 85.497016\tValidation Loss: 259.185089, time5.3\n",
      "Epoch: 569 \tTraining Loss: 47.809535 \tacc: 85.709366\tValidation Loss: 257.993457, time5.3\n",
      "Epoch: 570 \tTraining Loss: 47.513009 \tacc: 85.669192\tValidation Loss: 258.266985, time5.3\n",
      "Epoch: 571 \tTraining Loss: 47.425008 \tacc: 85.715106\tValidation Loss: 258.474972, time5.2\n",
      "Epoch: 572 \tTraining Loss: 48.335629 \tacc: 85.829890\tValidation Loss: 258.770329, time5.3\n",
      "Epoch: 573 \tTraining Loss: 47.731099 \tacc: 85.703627\tValidation Loss: 258.450820, time5.3\n",
      "Epoch: 574 \tTraining Loss: 47.520661 \tacc: 85.738062\tValidation Loss: 259.416545, time5.3\n",
      "Epoch: 575 \tTraining Loss: 47.612489 \tacc: 85.560147\tValidation Loss: 259.769430, time5.3\n",
      "Epoch: 576 \tTraining Loss: 47.501530 \tacc: 85.531451\tValidation Loss: 259.866034, time5.3\n",
      "Epoch: 577 \tTraining Loss: 47.734925 \tacc: 85.542929\tValidation Loss: 260.273724, time5.3\n",
      "Epoch: 578 \tTraining Loss: 47.690924 \tacc: 85.548669\tValidation Loss: 259.735802, time5.3\n",
      "Epoch: 579 \tTraining Loss: 47.679446 \tacc: 85.629017\tValidation Loss: 260.792360, time5.3\n",
      "Epoch: 580 \tTraining Loss: 48.096495 \tacc: 85.646235\tValidation Loss: 259.517630, time5.3\n",
      "Epoch: 581 \tTraining Loss: 48.329890 \tacc: 85.709366\tValidation Loss: 257.890575, time5.3\n",
      "Epoch: 582 \tTraining Loss: 47.449878 \tacc: 85.818411\tValidation Loss: 258.222268, time5.4\n",
      "Epoch: 583 \tTraining Loss: 48.002755 \tacc: 85.468320\tValidation Loss: 258.824719, time5.4\n",
      "Epoch: 584 \tTraining Loss: 47.870753 \tacc: 85.755280\tValidation Loss: 258.914217, time5.5\n",
      "Epoch: 585 \tTraining Loss: 47.576140 \tacc: 85.583104\tValidation Loss: 258.260178, time5.5\n",
      "Epoch: 586 \tTraining Loss: 48.159627 \tacc: 85.726584\tValidation Loss: 258.441159, time5.5\n",
      "Epoch: 587 \tTraining Loss: 47.931971 \tacc: 85.433884\tValidation Loss: 258.952372, time5.5\n",
      "Epoch: 588 \tTraining Loss: 47.991276 \tacc: 85.525712\tValidation Loss: 259.164549, time5.6\n",
      "Epoch: 589 \tTraining Loss: 48.008494 \tacc: 85.640496\tValidation Loss: 259.236048, time5.6\n",
      "Epoch: 590 \tTraining Loss: 47.773186 \tacc: 85.772498\tValidation Loss: 259.043114, time5.6\n",
      "Epoch: 591 \tTraining Loss: 47.754056 \tacc: 85.703627\tValidation Loss: 258.532436, time5.6\n",
      "Epoch: 592 \tTraining Loss: 47.920493 \tacc: 85.651974\tValidation Loss: 259.439648, time5.5\n",
      "Epoch: 593 \tTraining Loss: 48.125191 \tacc: 85.416667\tValidation Loss: 259.848297, time5.6\n",
      "Epoch: 594 \tTraining Loss: 48.102234 \tacc: 85.410927\tValidation Loss: 258.546704, time5.5\n",
      "Epoch: 595 \tTraining Loss: 47.671794 \tacc: 85.485537\tValidation Loss: 260.558548, time5.5\n",
      "Epoch: 596 \tTraining Loss: 47.740664 \tacc: 85.428145\tValidation Loss: 260.707075, time5.5\n",
      "Epoch: 597 \tTraining Loss: 47.591445 \tacc: 85.548669\tValidation Loss: 259.406154, time5.5\n",
      "Epoch: 598 \tTraining Loss: 47.794230 \tacc: 85.554408\tValidation Loss: 260.872152, time5.5\n",
      "Epoch: 599 \tTraining Loss: 47.782752 \tacc: 85.560147\tValidation Loss: 261.327228, time5.4\n",
      "Epoch: 600 \tTraining Loss: 48.090756 \tacc: 85.474059\tValidation Loss: 258.612905, time5.3\n",
      "Epoch: 601 \tTraining Loss: 47.558923 \tacc: 85.542929\tValidation Loss: 259.296384, time5.3\n",
      "Epoch: 602 \tTraining Loss: 47.828665 \tacc: 85.428145\tValidation Loss: 259.881677, time5.3\n",
      "Epoch: 603 \tTraining Loss: 47.662228 \tacc: 85.571625\tValidation Loss: 259.370466, time5.3\n",
      "Epoch: 604 \tTraining Loss: 47.914754 \tacc: 85.497016\tValidation Loss: 259.636299, time5.4\n",
      "Epoch: 605 \tTraining Loss: 48.073538 \tacc: 85.761019\tValidation Loss: 257.522112, time5.4\n",
      "Epoch: 606 \tTraining Loss: 47.939624 \tacc: 85.565886\tValidation Loss: 259.865177, time5.4\n",
      "Epoch: 607 \tTraining Loss: 47.713881 \tacc: 85.491276\tValidation Loss: 259.364693, time5.5\n",
      "Epoch: 608 \tTraining Loss: 47.574227 \tacc: 85.629017\tValidation Loss: 260.263063, time5.6\n",
      "Epoch: 609 \tTraining Loss: 47.491965 \tacc: 85.514233\tValidation Loss: 259.150385, time5.4\n",
      "Epoch: 610 \tTraining Loss: 48.002755 \tacc: 85.560147\tValidation Loss: 258.426820, time5.5\n",
      "Epoch: 611 \tTraining Loss: 47.727273 \tacc: 85.594582\tValidation Loss: 259.105882, time5.4\n",
      "Epoch: 612 \tTraining Loss: 47.995103 \tacc: 85.519972\tValidation Loss: 259.682476, time5.4\n",
      "Epoch: 613 \tTraining Loss: 48.044842 \tacc: 85.479798\tValidation Loss: 259.530772, time5.4\n",
      "Epoch: 614 \tTraining Loss: 47.629706 \tacc: 85.755280\tValidation Loss: 260.524631, time5.3\n",
      "Epoch: 615 \tTraining Loss: 47.928145 \tacc: 85.898760\tValidation Loss: 259.350554, time5.5\n",
      "Epoch: 616 \tTraining Loss: 47.786578 \tacc: 85.502755\tValidation Loss: 260.300704, time5.5\n",
      "Epoch: 617 \tTraining Loss: 47.798056 \tacc: 85.542929\tValidation Loss: 259.407438, time5.4\n",
      "Epoch: 618 \tTraining Loss: 47.744490 \tacc: 85.847107\tValidation Loss: 260.939573, time5.4\n",
      "Epoch: 619 \tTraining Loss: 47.660315 \tacc: 85.623278\tValidation Loss: 261.087531, time5.4\n",
      "Epoch: 620 \tTraining Loss: 47.667968 \tacc: 85.629017\tValidation Loss: 258.480305, time5.4\n",
      "Epoch: 621 \tTraining Loss: 47.673707 \tacc: 85.554408\tValidation Loss: 258.483689, time5.4\n",
      "Epoch: 622 \tTraining Loss: 47.784665 \tacc: 86.082415\tValidation Loss: 256.520334, time5.5\n",
      "Epoch: 623 \tTraining Loss: 47.935797 \tacc: 85.893021\tValidation Loss: 259.063008, time5.5\n",
      "Epoch: 624 \tTraining Loss: 47.620141 \tacc: 85.766758\tValidation Loss: 257.818498, time5.4\n",
      "Epoch: 625 \tTraining Loss: 47.863101 \tacc: 85.611800\tValidation Loss: 257.189977, time5.3\n",
      "Epoch: 626 \tTraining Loss: 48.199801 \tacc: 85.835629\tValidation Loss: 259.491378, time5.4\n",
      "Epoch: 627 \tTraining Loss: 47.801882 \tacc: 85.761019\tValidation Loss: 259.457367, time5.4\n",
      "Epoch: 628 \tTraining Loss: 47.830579 \tacc: 85.583104\tValidation Loss: 259.038717, time5.5\n",
      "Epoch: 629 \tTraining Loss: 47.899449 \tacc: 85.657713\tValidation Loss: 259.638668, time5.5\n",
      "Epoch: 630 \tTraining Loss: 47.245179 \tacc: 85.485537\tValidation Loss: 260.963943, time5.5\n",
      "Epoch: 631 \tTraining Loss: 47.847796 \tacc: 85.686410\tValidation Loss: 260.565781, time5.5\n",
      "Epoch: 632 \tTraining Loss: 48.100321 \tacc: 85.761019\tValidation Loss: 261.635468, time5.4\n",
      "Epoch: 633 \tTraining Loss: 47.861188 \tacc: 85.416667\tValidation Loss: 260.649752, time5.4\n",
      "Epoch: 634 \tTraining Loss: 47.851622 \tacc: 85.686410\tValidation Loss: 259.297510, time5.4\n",
      "Epoch: 635 \tTraining Loss: 47.555096 \tacc: 85.594582\tValidation Loss: 260.814676, time5.5\n",
      "Epoch: 636 \tTraining Loss: 47.842057 \tacc: 85.583104\tValidation Loss: 259.956149, time5.4\n",
      "Epoch: 637 \tTraining Loss: 47.771273 \tacc: 85.812672\tValidation Loss: 260.620771, time5.4\n",
      "Epoch: 638 \tTraining Loss: 47.931971 \tacc: 85.640496\tValidation Loss: 261.654975, time5.5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 639 \tTraining Loss: 47.394399 \tacc: 85.600321\tValidation Loss: 260.164167, time5.5\n",
      "Epoch: 640 \tTraining Loss: 47.402051 \tacc: 85.669192\tValidation Loss: 259.402108, time5.4\n",
      "Epoch: 641 \tTraining Loss: 47.432660 \tacc: 85.606061\tValidation Loss: 260.319460, time5.4\n",
      "Epoch: 642 \tTraining Loss: 47.731099 \tacc: 85.611800\tValidation Loss: 260.455715, time5.6\n",
      "Epoch: 643 \tTraining Loss: 47.752143 \tacc: 85.669192\tValidation Loss: 260.101955, time5.5\n",
      "Epoch: 644 \tTraining Loss: 47.813361 \tacc: 85.560147\tValidation Loss: 261.028519, time5.5\n",
      "Epoch: 645 \tTraining Loss: 47.534053 \tacc: 85.600321\tValidation Loss: 260.186279, time5.5\n",
      "Epoch: 646 \tTraining Loss: 47.572314 \tacc: 85.491276\tValidation Loss: 260.807302, time5.4\n",
      "Epoch: 647 \tTraining Loss: 47.535966 \tacc: 85.554408\tValidation Loss: 261.170193, time5.4\n",
      "Epoch: 648 \tTraining Loss: 47.585706 \tacc: 85.542929\tValidation Loss: 260.386482, time5.4\n",
      "Epoch: 649 \tTraining Loss: 47.897536 \tacc: 85.583104\tValidation Loss: 260.628898, time5.4\n",
      "Epoch: 650 \tTraining Loss: 47.857361 \tacc: 85.485537\tValidation Loss: 259.881056, time5.4\n",
      "Epoch: 651 \tTraining Loss: 47.685185 \tacc: 85.531451\tValidation Loss: 259.935756, time5.4\n",
      "Epoch: 652 \tTraining Loss: 48.060147 \tacc: 85.468320\tValidation Loss: 260.980698, time5.5\n",
      "Epoch: 653 \tTraining Loss: 47.893710 \tacc: 85.439624\tValidation Loss: 261.102485, time5.5\n",
      "Epoch: 654 \tTraining Loss: 47.656489 \tacc: 85.623278\tValidation Loss: 260.050701, time5.4\n",
      "Epoch: 655 \tTraining Loss: 47.979798 \tacc: 85.812672\tValidation Loss: 259.939589, time5.5\n",
      "Epoch: 656 \tTraining Loss: 47.822926 \tacc: 85.583104\tValidation Loss: 259.674499, time5.4\n",
      "Epoch: 657 \tTraining Loss: 47.713881 \tacc: 85.709366\tValidation Loss: 259.212566, time5.5\n",
      "Epoch: 658 \tTraining Loss: 47.711968 \tacc: 85.571625\tValidation Loss: 261.110794, time5.5\n",
      "Epoch: 659 \tTraining Loss: 47.595271 \tacc: 85.967631\tValidation Loss: 259.721284, time5.4\n",
      "Epoch: 660 \tTraining Loss: 47.849709 \tacc: 85.646235\tValidation Loss: 260.546209, time5.4\n",
      "Epoch: 661 \tTraining Loss: 47.843970 \tacc: 85.531451\tValidation Loss: 260.341856, time5.4\n",
      "Epoch: 662 \tTraining Loss: 47.727273 \tacc: 85.588843\tValidation Loss: 262.122673, time5.5\n",
      "Epoch: 663 \tTraining Loss: 47.953015 \tacc: 85.617539\tValidation Loss: 261.965139, time5.5\n",
      "Epoch: 664 \tTraining Loss: 47.796143 \tacc: 85.514233\tValidation Loss: 262.631972, time5.5\n",
      "Epoch: 665 \tTraining Loss: 47.218396 \tacc: 85.519972\tValidation Loss: 261.197589, time5.5\n",
      "Epoch: 666 \tTraining Loss: 47.903275 \tacc: 85.537190\tValidation Loss: 260.354363, time5.5\n",
      "Epoch: 667 \tTraining Loss: 47.826752 \tacc: 85.479798\tValidation Loss: 261.708779, time5.4\n",
      "Epoch: 668 \tTraining Loss: 47.643098 \tacc: 85.858586\tValidation Loss: 259.416905, time5.4\n",
      "Epoch: 669 \tTraining Loss: 47.746403 \tacc: 85.514233\tValidation Loss: 260.623909, time5.5\n",
      "Epoch: 670 \tTraining Loss: 47.493878 \tacc: 85.703627\tValidation Loss: 260.797798, time5.5\n",
      "Epoch: 671 \tTraining Loss: 47.891797 \tacc: 85.451102\tValidation Loss: 262.575937, time5.3\n",
      "Epoch: 672 \tTraining Loss: 47.367616 \tacc: 85.491276\tValidation Loss: 260.676829, time5.3\n",
      "Epoch: 673 \tTraining Loss: 47.516835 \tacc: 85.560147\tValidation Loss: 261.075850, time5.5\n",
      "Epoch: 674 \tTraining Loss: 47.662228 \tacc: 85.640496\tValidation Loss: 259.991068, time5.4\n",
      "Epoch: 675 \tTraining Loss: 47.493878 \tacc: 85.537190\tValidation Loss: 261.012062, time5.3\n",
      "Epoch: 676 \tTraining Loss: 47.528313 \tacc: 85.669192\tValidation Loss: 260.476357, time5.4\n",
      "Epoch: 677 \tTraining Loss: 47.499617 \tacc: 85.629017\tValidation Loss: 261.564220, time5.4\n",
      "Epoch: 678 \tTraining Loss: 47.591445 \tacc: 85.755280\tValidation Loss: 261.275660, time5.4\n",
      "Epoch: 679 \tTraining Loss: 47.486226 \tacc: 85.497016\tValidation Loss: 262.090066, time5.5\n",
      "Epoch: 680 \tTraining Loss: 47.660315 \tacc: 85.594582\tValidation Loss: 262.075053, time5.5\n",
      "Epoch: 681 \tTraining Loss: 47.700490 \tacc: 85.565886\tValidation Loss: 261.503883, time5.5\n",
      "Epoch: 682 \tTraining Loss: 47.954928 \tacc: 85.692149\tValidation Loss: 258.611870, time5.5\n",
      "Epoch: 683 \tTraining Loss: 47.689011 \tacc: 85.663453\tValidation Loss: 260.869346, time5.4\n",
      "Epoch: 684 \tTraining Loss: 47.639272 \tacc: 85.657713\tValidation Loss: 261.281948, time5.4\n",
      "Epoch: 685 \tTraining Loss: 47.805709 \tacc: 85.686410\tValidation Loss: 260.977274, time5.4\n",
      "Epoch: 686 \tTraining Loss: 47.855448 \tacc: 85.617539\tValidation Loss: 260.054893, time5.4\n",
      "Epoch: 687 \tTraining Loss: 47.599097 \tacc: 85.646235\tValidation Loss: 262.023570, time5.3\n",
      "Epoch: 688 \tTraining Loss: 47.564662 \tacc: 85.571625\tValidation Loss: 262.734490, time5.4\n",
      "Epoch: 689 \tTraining Loss: 47.325528 \tacc: 85.651974\tValidation Loss: 261.859618, time5.4\n",
      "Epoch: 690 \tTraining Loss: 47.778926 \tacc: 85.801194\tValidation Loss: 262.106175, time5.5\n",
      "Epoch: 691 \tTraining Loss: 47.727273 \tacc: 85.732323\tValidation Loss: 260.653034, time5.4\n",
      "Epoch: 692 \tTraining Loss: 47.912841 \tacc: 85.835629\tValidation Loss: 259.712994, time5.5\n",
      "Epoch: 693 \tTraining Loss: 47.874579 \tacc: 85.519972\tValidation Loss: 261.924992, time5.5\n",
      "Epoch: 694 \tTraining Loss: 48.016146 \tacc: 85.422406\tValidation Loss: 262.811340, time5.5\n",
      "Epoch: 695 \tTraining Loss: 47.369529 \tacc: 85.542929\tValidation Loss: 260.581099, time5.4\n",
      "Epoch: 696 \tTraining Loss: 47.843970 \tacc: 85.623278\tValidation Loss: 260.239416, time5.4\n",
      "Epoch: 697 \tTraining Loss: 47.765534 \tacc: 85.474059\tValidation Loss: 260.161426, time5.4\n",
      "Epoch: 698 \tTraining Loss: 47.507270 \tacc: 85.514233\tValidation Loss: 260.867755, time5.4\n",
      "Epoch: 699 \tTraining Loss: 47.794230 \tacc: 85.554408\tValidation Loss: 261.468343, time5.4\n",
      "Epoch: 700 \tTraining Loss: 47.983624 \tacc: 85.720845\tValidation Loss: 258.856255, time5.4\n",
      "Epoch: 701 \tTraining Loss: 47.453704 \tacc: 85.462580\tValidation Loss: 260.202613, time5.4\n",
      "Epoch: 702 \tTraining Loss: 47.545531 \tacc: 85.560147\tValidation Loss: 261.388199, time5.6\n",
      "Epoch: 703 \tTraining Loss: 47.673707 \tacc: 85.640496\tValidation Loss: 259.778925, time5.5\n",
      "Epoch: 704 \tTraining Loss: 47.771273 \tacc: 85.594582\tValidation Loss: 259.666580, time5.5\n",
      "Epoch: 705 \tTraining Loss: 47.733012 \tacc: 85.692149\tValidation Loss: 260.541418, time5.5\n",
      "Epoch: 706 \tTraining Loss: 47.595271 \tacc: 85.588843\tValidation Loss: 260.276120, time5.6\n",
      "Epoch: 707 \tTraining Loss: 47.679446 \tacc: 85.387971\tValidation Loss: 260.518941, time5.4\n",
      "Epoch: 708 \tTraining Loss: 47.604836 \tacc: 85.508494\tValidation Loss: 261.523821, time5.6\n",
      "Epoch: 709 \tTraining Loss: 48.056321 \tacc: 85.709366\tValidation Loss: 261.342096, time5.6\n",
      "Epoch: 710 \tTraining Loss: 47.306397 \tacc: 85.428145\tValidation Loss: 260.685827, time5.4\n",
      "Epoch: 711 \tTraining Loss: 47.428834 \tacc: 85.674931\tValidation Loss: 261.868749, time5.6\n",
      "Epoch: 712 \tTraining Loss: 47.352311 \tacc: 85.537190\tValidation Loss: 263.142014, time5.6\n",
      "Epoch: 713 \tTraining Loss: 47.212657 \tacc: 85.485537\tValidation Loss: 261.730058, time5.5\n",
      "Epoch: 714 \tTraining Loss: 47.390572 \tacc: 85.433884\tValidation Loss: 261.820411, time5.5\n",
      "Epoch: 715 \tTraining Loss: 47.599097 \tacc: 85.508494\tValidation Loss: 261.912340, time5.5\n",
      "Epoch: 716 \tTraining Loss: 47.740664 \tacc: 85.703627\tValidation Loss: 260.928653, time5.5\n",
      "Epoch: 717 \tTraining Loss: 47.805709 \tacc: 85.743802\tValidation Loss: 260.915090, time5.4\n",
      "Epoch: 718 \tTraining Loss: 47.463269 \tacc: 85.583104\tValidation Loss: 260.228742, time5.5\n",
      "Epoch: 719 \tTraining Loss: 47.983624 \tacc: 85.485537\tValidation Loss: 261.152332, time5.4\n",
      "Epoch: 720 \tTraining Loss: 47.478574 \tacc: 85.634757\tValidation Loss: 261.808338, time5.5\n",
      "Epoch: 721 \tTraining Loss: 47.409703 \tacc: 85.491276\tValidation Loss: 260.331901, time5.4\n",
      "Epoch: 722 \tTraining Loss: 47.528313 \tacc: 85.629017\tValidation Loss: 259.272850, time5.5\n",
      "Epoch: 723 \tTraining Loss: 47.459443 \tacc: 85.623278\tValidation Loss: 262.241658, time5.5\n",
      "Epoch: 724 \tTraining Loss: 47.514922 \tacc: 85.663453\tValidation Loss: 261.075342, time5.5\n",
      "Epoch: 725 \tTraining Loss: 48.019972 \tacc: 86.128329\tValidation Loss: 260.177707, time5.5\n",
      "Epoch: 726 \tTraining Loss: 47.725360 \tacc: 85.548669\tValidation Loss: 261.864353, time5.5\n",
      "Epoch: 727 \tTraining Loss: 47.476661 \tacc: 85.485537\tValidation Loss: 263.459202, time5.5\n",
      "Epoch: 728 \tTraining Loss: 47.734925 \tacc: 85.491276\tValidation Loss: 262.279864, time5.5\n",
      "Epoch: 729 \tTraining Loss: 47.633532 \tacc: 85.588843\tValidation Loss: 262.088495, time5.5\n",
      "Epoch: 730 \tTraining Loss: 47.843970 \tacc: 85.789715\tValidation Loss: 259.886753, time5.5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 731 \tTraining Loss: 47.924319 \tacc: 85.508494\tValidation Loss: 262.726957, time5.5\n",
      "Epoch: 732 \tTraining Loss: 47.886058 \tacc: 85.898760\tValidation Loss: 260.201775, time5.5\n",
      "Epoch: 733 \tTraining Loss: 47.941537 \tacc: 85.537190\tValidation Loss: 259.903692, time5.4\n",
      "Epoch: 734 \tTraining Loss: 47.277701 \tacc: 85.692149\tValidation Loss: 260.487526, time5.4\n",
      "Epoch: 735 \tTraining Loss: 47.882231 \tacc: 85.571625\tValidation Loss: 261.630750, time5.5\n",
      "Epoch: 736 \tTraining Loss: 48.023799 \tacc: 85.772498\tValidation Loss: 261.792995, time5.5\n",
      "Epoch: 737 \tTraining Loss: 47.417355 \tacc: 85.789715\tValidation Loss: 260.865565, time5.5\n",
      "Epoch: 738 \tTraining Loss: 48.012320 \tacc: 85.542929\tValidation Loss: 261.246982, time5.4\n",
      "Epoch: 739 \tTraining Loss: 47.799969 \tacc: 85.669192\tValidation Loss: 260.416459, time5.6\n",
      "Epoch: 740 \tTraining Loss: 47.656489 \tacc: 85.451102\tValidation Loss: 261.055578, time5.5\n",
      "Epoch: 741 \tTraining Loss: 48.173018 \tacc: 85.554408\tValidation Loss: 260.718928, time5.4\n",
      "Epoch: 742 \tTraining Loss: 47.995103 \tacc: 85.910239\tValidation Loss: 262.099072, time5.4\n",
      "Epoch: 743 \tTraining Loss: 47.819100 \tacc: 85.565886\tValidation Loss: 260.626539, time5.5\n",
      "Epoch: 744 \tTraining Loss: 48.027625 \tacc: 85.938935\tValidation Loss: 260.987773, time5.5\n",
      "Epoch: 745 \tTraining Loss: 47.478574 \tacc: 85.600321\tValidation Loss: 261.058814, time5.5\n",
      "Epoch: 746 \tTraining Loss: 47.715794 \tacc: 85.508494\tValidation Loss: 262.056932, time5.7\n",
      "Epoch: 747 \tTraining Loss: 47.660315 \tacc: 85.508494\tValidation Loss: 262.289363, time5.6\n",
      "Epoch: 748 \tTraining Loss: 47.463269 \tacc: 85.766758\tValidation Loss: 261.251938, time5.5\n",
      "Epoch: 749 \tTraining Loss: 47.710055 \tacc: 85.548669\tValidation Loss: 261.602172, time5.5\n",
      "Epoch: 750 \tTraining Loss: 47.566575 \tacc: 85.497016\tValidation Loss: 261.987392, time5.5\n",
      "Epoch: 751 \tTraining Loss: 47.625880 \tacc: 85.686410\tValidation Loss: 262.734176, time5.4\n",
      "Epoch: 752 \tTraining Loss: 47.717707 \tacc: 85.571625\tValidation Loss: 261.449243, time5.5\n",
      "Epoch: 753 \tTraining Loss: 48.016146 \tacc: 85.594582\tValidation Loss: 261.923682, time5.3\n",
      "Epoch: 754 \tTraining Loss: 47.543618 \tacc: 85.479798\tValidation Loss: 261.428947, time5.5\n",
      "Epoch: 755 \tTraining Loss: 47.891797 \tacc: 85.841368\tValidation Loss: 261.789227, time5.5\n",
      "Epoch: 756 \tTraining Loss: 47.555096 \tacc: 85.565886\tValidation Loss: 260.878940, time5.5\n",
      "Epoch: 757 \tTraining Loss: 47.729186 \tacc: 85.749541\tValidation Loss: 261.229820, time5.5\n",
      "Epoch: 758 \tTraining Loss: 48.077365 \tacc: 85.548669\tValidation Loss: 261.955750, time5.5\n",
      "Epoch: 759 \tTraining Loss: 47.426921 \tacc: 85.531451\tValidation Loss: 262.310551, time5.4\n",
      "Epoch: 760 \tTraining Loss: 47.568488 \tacc: 85.761019\tValidation Loss: 261.695434, time5.4\n",
      "Epoch: 761 \tTraining Loss: 47.589532 \tacc: 85.651974\tValidation Loss: 262.443422, time5.5\n",
      "Epoch: 762 \tTraining Loss: 47.432660 \tacc: 85.560147\tValidation Loss: 263.077663, time5.5\n",
      "Epoch: 763 \tTraining Loss: 47.428834 \tacc: 85.548669\tValidation Loss: 260.240136, time5.5\n",
      "Epoch: 764 \tTraining Loss: 47.792317 \tacc: 85.606061\tValidation Loss: 259.197051, time5.4\n",
      "Epoch: 765 \tTraining Loss: 47.761708 \tacc: 85.531451\tValidation Loss: 261.359396, time5.4\n",
      "Epoch: 766 \tTraining Loss: 47.618228 \tacc: 85.359275\tValidation Loss: 261.560976, time5.3\n",
      "Epoch: 767 \tTraining Loss: 47.734925 \tacc: 85.663453\tValidation Loss: 261.838850, time5.3\n",
      "Epoch: 768 \tTraining Loss: 47.344659 \tacc: 85.720845\tValidation Loss: 261.921824, time5.3\n",
      "Epoch: 769 \tTraining Loss: 47.497704 \tacc: 85.462580\tValidation Loss: 262.864718, time5.5\n",
      "Epoch: 770 \tTraining Loss: 47.409703 \tacc: 85.537190\tValidation Loss: 263.478137, time5.5\n",
      "Epoch: 771 \tTraining Loss: 47.465182 \tacc: 85.497016\tValidation Loss: 261.386522, time5.5\n",
      "Epoch: 772 \tTraining Loss: 47.444138 \tacc: 85.497016\tValidation Loss: 262.811513, time5.5\n",
      "Epoch: 773 \tTraining Loss: 47.480487 \tacc: 85.445363\tValidation Loss: 262.346579, time5.6\n",
      "Epoch: 774 \tTraining Loss: 47.558923 \tacc: 85.663453\tValidation Loss: 261.169511, time5.5\n",
      "Epoch: 775 \tTraining Loss: 47.428834 \tacc: 85.594582\tValidation Loss: 261.375755, time5.5\n",
      "Epoch: 776 \tTraining Loss: 47.654576 \tacc: 85.508494\tValidation Loss: 261.791022, time5.5\n",
      "Epoch: 777 \tTraining Loss: 47.482400 \tacc: 85.824151\tValidation Loss: 262.328588, time5.4\n",
      "Epoch: 778 \tTraining Loss: 47.587619 \tacc: 85.692149\tValidation Loss: 261.706738, time5.5\n",
      "Epoch: 779 \tTraining Loss: 47.719620 \tacc: 85.456841\tValidation Loss: 261.176157, time5.5\n",
      "Epoch: 780 \tTraining Loss: 48.004668 \tacc: 85.921717\tValidation Loss: 261.091844, time5.5\n",
      "Epoch: 781 \tTraining Loss: 47.815274 \tacc: 85.640496\tValidation Loss: 260.374790, time5.5\n",
      "Epoch: 782 \tTraining Loss: 47.474747 \tacc: 85.686410\tValidation Loss: 260.546886, time5.5\n",
      "Epoch: 783 \tTraining Loss: 47.610575 \tacc: 85.646235\tValidation Loss: 262.771120, time5.5\n",
      "Epoch: 784 \tTraining Loss: 47.606749 \tacc: 85.451102\tValidation Loss: 261.528797, time5.5\n",
      "Epoch: 785 \tTraining Loss: 47.530227 \tacc: 85.640496\tValidation Loss: 261.620961, time5.4\n",
      "Epoch: 786 \tTraining Loss: 47.491965 \tacc: 85.852847\tValidation Loss: 261.393749, time5.5\n",
      "Epoch: 787 \tTraining Loss: 47.227961 \tacc: 85.709366\tValidation Loss: 260.323934, time5.5\n",
      "Epoch: 788 \tTraining Loss: 47.298745 \tacc: 85.651974\tValidation Loss: 262.521166, time5.5\n",
      "Epoch: 789 \tTraining Loss: 47.711968 \tacc: 85.755280\tValidation Loss: 262.496985, time5.5\n",
      "Epoch: 790 \tTraining Loss: 47.956841 \tacc: 85.594582\tValidation Loss: 261.443089, time5.5\n",
      "Epoch: 791 \tTraining Loss: 47.409703 \tacc: 85.726584\tValidation Loss: 262.496921, time5.5\n",
      "Epoch: 792 \tTraining Loss: 47.583792 \tacc: 85.617539\tValidation Loss: 263.180343, time5.5\n",
      "Epoch: 793 \tTraining Loss: 47.535966 \tacc: 85.399449\tValidation Loss: 262.577847, time5.5\n",
      "Epoch: 794 \tTraining Loss: 47.432660 \tacc: 85.680670\tValidation Loss: 263.897665, time5.5\n",
      "Epoch: 795 \tTraining Loss: 47.344659 \tacc: 85.497016\tValidation Loss: 262.766446, time5.5\n",
      "Epoch: 796 \tTraining Loss: 47.711968 \tacc: 85.554408\tValidation Loss: 263.183828, time5.4\n",
      "Epoch: 797 \tTraining Loss: 47.819100 \tacc: 85.738062\tValidation Loss: 262.082122, time5.5\n",
      "Epoch: 798 \tTraining Loss: 47.723447 \tacc: 85.663453\tValidation Loss: 262.525425, time5.5\n",
      "Epoch: 799 \tTraining Loss: 47.535966 \tacc: 85.824151\tValidation Loss: 262.872744, time5.5\n",
      "Epoch: 800 \tTraining Loss: 47.710055 \tacc: 85.663453\tValidation Loss: 262.685634, time5.5\n",
      "Epoch: 801 \tTraining Loss: 47.436486 \tacc: 85.474059\tValidation Loss: 262.457967, time5.5\n",
      "Epoch: 802 \tTraining Loss: 47.629706 \tacc: 85.720845\tValidation Loss: 262.469656, time5.5\n",
      "Epoch: 803 \tTraining Loss: 47.256657 \tacc: 85.462580\tValidation Loss: 262.847813, time5.5\n",
      "Epoch: 804 \tTraining Loss: 47.566575 \tacc: 85.669192\tValidation Loss: 262.313397, time5.5\n",
      "Epoch: 805 \tTraining Loss: 47.733012 \tacc: 85.715106\tValidation Loss: 261.937687, time5.5\n",
      "Epoch: 806 \tTraining Loss: 47.727273 \tacc: 85.606061\tValidation Loss: 262.269455, time5.4\n",
      "Epoch: 807 \tTraining Loss: 47.679446 \tacc: 85.761019\tValidation Loss: 261.696716, time5.5\n",
      "Epoch: 808 \tTraining Loss: 47.604836 \tacc: 85.554408\tValidation Loss: 263.321108, time5.5\n",
      "Epoch: 809 \tTraining Loss: 47.843970 \tacc: 85.697888\tValidation Loss: 263.107293, time5.5\n",
      "Epoch: 810 \tTraining Loss: 47.907101 \tacc: 85.542929\tValidation Loss: 262.328554, time5.5\n",
      "Epoch: 811 \tTraining Loss: 47.704316 \tacc: 85.474059\tValidation Loss: 261.850232, time5.5\n",
      "Epoch: 812 \tTraining Loss: 47.916667 \tacc: 85.479798\tValidation Loss: 263.760356, time5.5\n",
      "Epoch: 813 \tTraining Loss: 47.790404 \tacc: 85.548669\tValidation Loss: 262.700917, time5.5\n",
      "Epoch: 814 \tTraining Loss: 47.356137 \tacc: 85.611800\tValidation Loss: 262.653412, time5.5\n",
      "Epoch: 815 \tTraining Loss: 48.092669 \tacc: 85.468320\tValidation Loss: 263.811490, time5.5\n",
      "Epoch: 816 \tTraining Loss: 47.698577 \tacc: 85.651974\tValidation Loss: 263.093387, time5.5\n",
      "Epoch: 817 \tTraining Loss: 47.815274 \tacc: 85.646235\tValidation Loss: 261.505861, time5.4\n",
      "Epoch: 818 \tTraining Loss: 47.438399 \tacc: 85.674931\tValidation Loss: 261.207718, time5.5\n",
      "Epoch: 819 \tTraining Loss: 47.635445 \tacc: 85.669192\tValidation Loss: 261.786502, time5.5\n",
      "Epoch: 820 \tTraining Loss: 47.403964 \tacc: 85.749541\tValidation Loss: 263.152264, time5.5\n",
      "Epoch: 821 \tTraining Loss: 47.759795 \tacc: 85.491276\tValidation Loss: 264.381662, time5.5\n",
      "Epoch: 822 \tTraining Loss: 47.195439 \tacc: 85.686410\tValidation Loss: 262.086682, time5.5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 823 \tTraining Loss: 47.553183 \tacc: 85.835629\tValidation Loss: 261.953182, time5.5\n",
      "Epoch: 824 \tTraining Loss: 47.809535 \tacc: 85.428145\tValidation Loss: 263.027152, time5.5\n",
      "Epoch: 825 \tTraining Loss: 47.440312 \tacc: 85.600321\tValidation Loss: 262.757712, time5.5\n",
      "Epoch: 826 \tTraining Loss: 47.539792 \tacc: 85.554408\tValidation Loss: 264.417873, time5.5\n",
      "Epoch: 827 \tTraining Loss: 47.796143 \tacc: 85.629017\tValidation Loss: 263.004359, time5.5\n",
      "Epoch: 828 \tTraining Loss: 47.654576 \tacc: 85.376492\tValidation Loss: 262.958723, time5.4\n",
      "Epoch: 829 \tTraining Loss: 46.969697 \tacc: 85.663453\tValidation Loss: 262.074942, time5.5\n",
      "Epoch: 830 \tTraining Loss: 47.333180 \tacc: 85.606061\tValidation Loss: 263.161239, time5.5\n",
      "Epoch: 831 \tTraining Loss: 47.949189 \tacc: 85.663453\tValidation Loss: 263.876520, time5.5\n",
      "Epoch: 832 \tTraining Loss: 47.757882 \tacc: 85.715106\tValidation Loss: 263.764237, time5.5\n",
      "Epoch: 833 \tTraining Loss: 47.534053 \tacc: 85.606061\tValidation Loss: 261.593923, time5.5\n",
      "Epoch: 834 \tTraining Loss: 47.319789 \tacc: 85.720845\tValidation Loss: 261.794318, time5.5\n",
      "Epoch: 835 \tTraining Loss: 47.367616 \tacc: 85.835629\tValidation Loss: 262.115574, time5.5\n",
      "Epoch: 836 \tTraining Loss: 47.687098 \tacc: 85.669192\tValidation Loss: 263.413467, time5.5\n",
      "Epoch: 837 \tTraining Loss: 47.509183 \tacc: 85.783976\tValidation Loss: 263.438889, time5.5\n",
      "Epoch: 838 \tTraining Loss: 47.451791 \tacc: 85.502755\tValidation Loss: 262.999070, time5.5\n",
      "Epoch: 839 \tTraining Loss: 47.497704 \tacc: 85.783976\tValidation Loss: 261.511853, time5.4\n",
      "Epoch: 840 \tTraining Loss: 47.836318 \tacc: 85.829890\tValidation Loss: 263.030061, time5.5\n",
      "Epoch: 841 \tTraining Loss: 47.664141 \tacc: 85.634757\tValidation Loss: 263.672228, time5.5\n",
      "Epoch: 842 \tTraining Loss: 47.748316 \tacc: 85.789715\tValidation Loss: 261.981854, time5.5\n",
      "Epoch: 843 \tTraining Loss: 47.595271 \tacc: 85.634757\tValidation Loss: 262.902195, time5.5\n",
      "Epoch: 844 \tTraining Loss: 47.252831 \tacc: 85.583104\tValidation Loss: 263.925722, time5.5\n",
      "Epoch: 845 \tTraining Loss: 47.886058 \tacc: 85.847107\tValidation Loss: 262.430791, time5.5\n",
      "Epoch: 846 \tTraining Loss: 47.664141 \tacc: 85.577365\tValidation Loss: 263.946943, time5.5\n",
      "Epoch: 847 \tTraining Loss: 47.449878 \tacc: 85.651974\tValidation Loss: 264.694587, time5.5\n",
      "Epoch: 848 \tTraining Loss: 47.658402 \tacc: 85.485537\tValidation Loss: 264.942730, time5.5\n",
      "Epoch: 849 \tTraining Loss: 47.912841 \tacc: 85.743802\tValidation Loss: 264.351105, time5.4\n",
      "Epoch: 850 \tTraining Loss: 47.277701 \tacc: 85.611800\tValidation Loss: 264.262215, time5.5\n",
      "Epoch: 851 \tTraining Loss: 47.191613 \tacc: 85.497016\tValidation Loss: 265.034032, time5.5\n",
      "Epoch: 852 \tTraining Loss: 47.254744 \tacc: 85.497016\tValidation Loss: 266.314097, time5.5\n",
      "Epoch: 853 \tTraining Loss: 47.635445 \tacc: 85.502755\tValidation Loss: 266.602644, time5.5\n",
      "Epoch: 854 \tTraining Loss: 47.608662 \tacc: 85.416667\tValidation Loss: 265.951650, time5.5\n",
      "Epoch: 855 \tTraining Loss: 47.472834 \tacc: 85.657713\tValidation Loss: 265.537543, time5.5\n",
      "Epoch: 856 \tTraining Loss: 47.451791 \tacc: 85.479798\tValidation Loss: 265.185011, time5.5\n",
      "Epoch: 857 \tTraining Loss: 47.478574 \tacc: 85.542929\tValidation Loss: 264.491482, time5.5\n",
      "Epoch: 858 \tTraining Loss: 47.513009 \tacc: 85.709366\tValidation Loss: 264.235974, time5.5\n",
      "Epoch: 859 \tTraining Loss: 47.564662 \tacc: 85.766758\tValidation Loss: 263.780315, time5.5\n",
      "Epoch: 860 \tTraining Loss: 47.503444 \tacc: 85.697888\tValidation Loss: 262.241930, time5.4\n",
      "Epoch: 861 \tTraining Loss: 47.641185 \tacc: 85.715106\tValidation Loss: 263.320310, time5.5\n",
      "Epoch: 862 \tTraining Loss: 47.394399 \tacc: 85.491276\tValidation Loss: 265.144247, time5.5\n",
      "Epoch: 863 \tTraining Loss: 47.708142 \tacc: 85.479798\tValidation Loss: 263.502686, time5.5\n",
      "Epoch: 864 \tTraining Loss: 47.436486 \tacc: 85.600321\tValidation Loss: 263.151208, time5.5\n",
      "Epoch: 865 \tTraining Loss: 47.388659 \tacc: 85.680670\tValidation Loss: 263.513268, time5.5\n",
      "Epoch: 866 \tTraining Loss: 47.396312 \tacc: 85.617539\tValidation Loss: 264.084057, time5.5\n",
      "Epoch: 867 \tTraining Loss: 47.480487 \tacc: 85.588843\tValidation Loss: 264.340556, time5.5\n",
      "Epoch: 868 \tTraining Loss: 47.434573 \tacc: 85.617539\tValidation Loss: 264.236682, time5.5\n",
      "Epoch: 869 \tTraining Loss: 47.614402 \tacc: 85.565886\tValidation Loss: 263.444853, time5.5\n",
      "Epoch: 870 \tTraining Loss: 47.656489 \tacc: 85.806933\tValidation Loss: 263.167882, time5.5\n",
      "Epoch: 871 \tTraining Loss: 47.591445 \tacc: 85.634757\tValidation Loss: 263.598899, time5.4\n",
      "Epoch: 872 \tTraining Loss: 47.273875 \tacc: 85.508494\tValidation Loss: 265.672613, time5.5\n",
      "Epoch: 873 \tTraining Loss: 47.962580 \tacc: 85.772498\tValidation Loss: 263.893343, time5.5\n",
      "Epoch: 874 \tTraining Loss: 47.551270 \tacc: 85.692149\tValidation Loss: 264.163508, time5.5\n",
      "Epoch: 875 \tTraining Loss: 47.991276 \tacc: 85.583104\tValidation Loss: 264.094060, time5.5\n",
      "Epoch: 876 \tTraining Loss: 47.641185 \tacc: 85.594582\tValidation Loss: 264.142617, time5.5\n",
      "Epoch: 877 \tTraining Loss: 47.388659 \tacc: 85.663453\tValidation Loss: 265.743974, time5.5\n",
      "Epoch: 878 \tTraining Loss: 47.970233 \tacc: 85.456841\tValidation Loss: 264.606610, time5.5\n",
      "Epoch: 879 \tTraining Loss: 47.382920 \tacc: 85.697888\tValidation Loss: 264.999685, time5.5\n",
      "Epoch: 880 \tTraining Loss: 47.317876 \tacc: 85.944674\tValidation Loss: 265.299491, time5.5\n",
      "Epoch: 881 \tTraining Loss: 47.314050 \tacc: 85.588843\tValidation Loss: 265.141783, time5.4\n",
      "Epoch: 882 \tTraining Loss: 47.402051 \tacc: 85.577365\tValidation Loss: 266.263160, time5.5\n",
      "Epoch: 883 \tTraining Loss: 47.428834 \tacc: 85.497016\tValidation Loss: 262.819918, time5.5\n",
      "Epoch: 884 \tTraining Loss: 47.551270 \tacc: 85.588843\tValidation Loss: 265.581190, time5.5\n",
      "Epoch: 885 \tTraining Loss: 47.884144 \tacc: 85.617539\tValidation Loss: 263.807649, time5.5\n",
      "Epoch: 886 \tTraining Loss: 47.845883 \tacc: 85.697888\tValidation Loss: 264.271657, time5.5\n",
      "Epoch: 887 \tTraining Loss: 47.539792 \tacc: 85.715106\tValidation Loss: 264.530162, time5.5\n",
      "Epoch: 888 \tTraining Loss: 47.507270 \tacc: 85.525712\tValidation Loss: 265.525814, time5.5\n",
      "Epoch: 889 \tTraining Loss: 47.746403 \tacc: 85.571625\tValidation Loss: 264.288763, time5.5\n",
      "Epoch: 890 \tTraining Loss: 47.616315 \tacc: 85.720845\tValidation Loss: 263.727833, time5.5\n",
      "Epoch: 891 \tTraining Loss: 47.373355 \tacc: 85.686410\tValidation Loss: 264.878745, time5.5\n",
      "Epoch: 892 \tTraining Loss: 47.354224 \tacc: 85.594582\tValidation Loss: 264.448967, time5.4\n",
      "Epoch: 893 \tTraining Loss: 47.218396 \tacc: 85.571625\tValidation Loss: 265.356029, time5.5\n",
      "Epoch: 894 \tTraining Loss: 47.723447 \tacc: 85.841368\tValidation Loss: 265.376277, time5.5\n",
      "Epoch: 895 \tTraining Loss: 47.593358 \tacc: 85.606061\tValidation Loss: 265.356694, time5.5\n",
      "Epoch: 896 \tTraining Loss: 47.679446 \tacc: 85.617539\tValidation Loss: 264.599520, time5.5\n",
      "Epoch: 897 \tTraining Loss: 47.535966 \tacc: 85.514233\tValidation Loss: 264.747256, time5.5\n",
      "Epoch: 898 \tTraining Loss: 47.560836 \tacc: 85.680670\tValidation Loss: 264.886022, time5.5\n",
      "Epoch: 899 \tTraining Loss: 47.403964 \tacc: 85.898760\tValidation Loss: 263.645024, time5.5\n",
      "Epoch: 900 \tTraining Loss: 47.382920 \tacc: 85.720845\tValidation Loss: 264.241510, time5.5\n",
      "Epoch: 901 \tTraining Loss: 47.698577 \tacc: 85.422406\tValidation Loss: 263.298981, time5.5\n",
      "Epoch: 902 \tTraining Loss: 47.403964 \tacc: 85.617539\tValidation Loss: 263.879811, time5.5\n",
      "Epoch: 903 \tTraining Loss: 47.514922 \tacc: 85.514233\tValidation Loss: 265.161917, time5.4\n",
      "Epoch: 904 \tTraining Loss: 46.902740 \tacc: 85.640496\tValidation Loss: 264.676541, time5.5\n",
      "Epoch: 905 \tTraining Loss: 47.327441 \tacc: 85.680670\tValidation Loss: 264.875732, time5.5\n",
      "Epoch: 906 \tTraining Loss: 47.683272 \tacc: 85.594582\tValidation Loss: 265.913063, time5.5\n",
      "Epoch: 907 \tTraining Loss: 47.545531 \tacc: 85.405188\tValidation Loss: 265.780683, time5.5\n",
      "Epoch: 908 \tTraining Loss: 47.782752 \tacc: 85.686410\tValidation Loss: 264.383558, time5.5\n",
      "Epoch: 909 \tTraining Loss: 47.581879 \tacc: 85.634757\tValidation Loss: 264.786992, time5.5\n",
      "Epoch: 910 \tTraining Loss: 47.717707 \tacc: 85.474059\tValidation Loss: 264.324991, time5.5\n",
      "Epoch: 911 \tTraining Loss: 47.690924 \tacc: 85.554408\tValidation Loss: 264.826787, time5.5\n",
      "Epoch: 912 \tTraining Loss: 47.314050 \tacc: 85.525712\tValidation Loss: 265.766774, time5.5\n",
      "Epoch: 913 \tTraining Loss: 47.214570 \tacc: 85.514233\tValidation Loss: 264.814368, time5.5\n",
      "Epoch: 914 \tTraining Loss: 47.434573 \tacc: 85.623278\tValidation Loss: 263.687175, time5.4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 915 \tTraining Loss: 47.748316 \tacc: 85.514233\tValidation Loss: 263.996857, time5.5\n",
      "Epoch: 916 \tTraining Loss: 47.551270 \tacc: 85.680670\tValidation Loss: 262.472872, time5.5\n",
      "Epoch: 917 \tTraining Loss: 47.564662 \tacc: 85.474059\tValidation Loss: 264.200389, time5.5\n",
      "Epoch: 918 \tTraining Loss: 47.694751 \tacc: 85.548669\tValidation Loss: 263.197133, time5.5\n",
      "Epoch: 919 \tTraining Loss: 47.258571 \tacc: 85.611800\tValidation Loss: 263.550209, time5.5\n",
      "Epoch: 920 \tTraining Loss: 47.721534 \tacc: 85.376492\tValidation Loss: 264.482229, time5.5\n",
      "Epoch: 921 \tTraining Loss: 47.325528 \tacc: 85.560147\tValidation Loss: 266.146021, time5.5\n",
      "Epoch: 922 \tTraining Loss: 47.291093 \tacc: 85.422406\tValidation Loss: 264.821190, time5.5\n",
      "Epoch: 923 \tTraining Loss: 47.262397 \tacc: 85.508494\tValidation Loss: 264.995428, time5.5\n",
      "Epoch: 924 \tTraining Loss: 47.356137 \tacc: 85.674931\tValidation Loss: 265.276741, time5.4\n",
      "Epoch: 925 \tTraining Loss: 47.928145 \tacc: 85.594582\tValidation Loss: 264.184914, time5.5\n",
      "Epoch: 926 \tTraining Loss: 47.275788 \tacc: 85.548669\tValidation Loss: 263.279736, time5.5\n",
      "Epoch: 927 \tTraining Loss: 47.631619 \tacc: 85.623278\tValidation Loss: 264.727068, time5.5\n",
      "Epoch: 928 \tTraining Loss: 47.388659 \tacc: 85.565886\tValidation Loss: 264.544648, time5.5\n",
      "Epoch: 929 \tTraining Loss: 47.434573 \tacc: 85.560147\tValidation Loss: 264.544878, time5.5\n",
      "Epoch: 930 \tTraining Loss: 47.340833 \tacc: 85.697888\tValidation Loss: 264.873746, time5.5\n",
      "Epoch: 931 \tTraining Loss: 47.354224 \tacc: 85.485537\tValidation Loss: 265.608185, time5.5\n",
      "Epoch: 932 \tTraining Loss: 47.161004 \tacc: 85.634757\tValidation Loss: 265.965387, time5.5\n",
      "Epoch: 933 \tTraining Loss: 47.446051 \tacc: 85.600321\tValidation Loss: 265.148599, time5.5\n",
      "Epoch: 934 \tTraining Loss: 47.377181 \tacc: 85.594582\tValidation Loss: 265.825120, time5.5\n",
      "Epoch: 935 \tTraining Loss: 47.562749 \tacc: 85.479798\tValidation Loss: 266.114836, time5.4\n",
      "Epoch: 936 \tTraining Loss: 47.543618 \tacc: 85.485537\tValidation Loss: 264.288827, time5.5\n",
      "Epoch: 937 \tTraining Loss: 47.291093 \tacc: 85.611800\tValidation Loss: 265.555085, time5.5\n",
      "Epoch: 938 \tTraining Loss: 47.413529 \tacc: 85.680670\tValidation Loss: 265.000492, time5.5\n",
      "Epoch: 939 \tTraining Loss: 47.583792 \tacc: 85.611800\tValidation Loss: 266.626804, time5.5\n",
      "Epoch: 940 \tTraining Loss: 47.294919 \tacc: 85.514233\tValidation Loss: 265.451346, time5.5\n",
      "Epoch: 941 \tTraining Loss: 47.637358 \tacc: 85.577365\tValidation Loss: 264.733156, time5.5\n",
      "Epoch: 942 \tTraining Loss: 47.578053 \tacc: 85.680670\tValidation Loss: 263.627337, time5.5\n",
      "Epoch: 943 \tTraining Loss: 47.438399 \tacc: 85.479798\tValidation Loss: 265.822388, time5.5\n",
      "Epoch: 944 \tTraining Loss: 47.648837 \tacc: 85.720845\tValidation Loss: 265.772652, time5.5\n",
      "Epoch: 945 \tTraining Loss: 47.425008 \tacc: 85.537190\tValidation Loss: 266.000014, time5.5\n",
      "Epoch: 946 \tTraining Loss: 47.514922 \tacc: 85.542929\tValidation Loss: 266.024863, time5.4\n",
      "Epoch: 947 \tTraining Loss: 47.503444 \tacc: 85.686410\tValidation Loss: 264.942125, time5.5\n",
      "Epoch: 948 \tTraining Loss: 47.197352 \tacc: 85.502755\tValidation Loss: 266.212223, time5.5\n",
      "Epoch: 949 \tTraining Loss: 47.897536 \tacc: 85.548669\tValidation Loss: 264.166202, time5.5\n",
      "Epoch: 950 \tTraining Loss: 47.579966 \tacc: 85.497016\tValidation Loss: 264.887805, time5.5\n",
      "Epoch: 951 \tTraining Loss: 47.392485 \tacc: 85.657713\tValidation Loss: 265.084657, time5.5\n",
      "Epoch: 952 \tTraining Loss: 47.321702 \tacc: 85.382231\tValidation Loss: 266.321684, time5.5\n",
      "Epoch: 953 \tTraining Loss: 47.444138 \tacc: 85.468320\tValidation Loss: 266.857707, time5.5\n",
      "Epoch: 954 \tTraining Loss: 47.400138 \tacc: 85.623278\tValidation Loss: 266.378654, time5.5\n",
      "Epoch: 955 \tTraining Loss: 47.335093 \tacc: 85.847107\tValidation Loss: 264.572393, time5.5\n",
      "Epoch: 956 \tTraining Loss: 47.193526 \tacc: 85.600321\tValidation Loss: 266.685394, time5.4\n",
      "Epoch: 957 \tTraining Loss: 47.417355 \tacc: 85.663453\tValidation Loss: 266.198561, time5.5\n",
      "Epoch: 958 \tTraining Loss: 47.361876 \tacc: 85.594582\tValidation Loss: 266.498011, time5.5\n",
      "Epoch: 959 \tTraining Loss: 47.403964 \tacc: 85.537190\tValidation Loss: 266.942420, time5.5\n",
      "Epoch: 960 \tTraining Loss: 47.503444 \tacc: 85.554408\tValidation Loss: 265.623384, time5.5\n",
      "Epoch: 961 \tTraining Loss: 47.021350 \tacc: 85.359275\tValidation Loss: 265.234474, time5.5\n",
      "Epoch: 962 \tTraining Loss: 47.373355 \tacc: 85.497016\tValidation Loss: 266.185401, time5.5\n",
      "Epoch: 963 \tTraining Loss: 47.518748 \tacc: 85.491276\tValidation Loss: 264.852593, time5.5\n",
      "Epoch: 964 \tTraining Loss: 47.400138 \tacc: 85.491276\tValidation Loss: 265.489403, time5.5\n",
      "Epoch: 965 \tTraining Loss: 47.685185 \tacc: 85.439624\tValidation Loss: 266.692643, time5.5\n",
      "Epoch: 966 \tTraining Loss: 47.182048 \tacc: 85.697888\tValidation Loss: 266.148508, time5.5\n",
      "Epoch: 967 \tTraining Loss: 47.509183 \tacc: 85.565886\tValidation Loss: 266.203926, time5.4\n",
      "Epoch: 968 \tTraining Loss: 47.775099 \tacc: 85.772498\tValidation Loss: 264.117022, time5.5\n",
      "Epoch: 969 \tTraining Loss: 47.403964 \tacc: 85.548669\tValidation Loss: 263.465636, time5.5\n",
      "Epoch: 970 \tTraining Loss: 47.551270 \tacc: 85.491276\tValidation Loss: 262.978599, time5.5\n",
      "Epoch: 971 \tTraining Loss: 47.084481 \tacc: 85.697888\tValidation Loss: 264.665581, time5.5\n",
      "Epoch: 972 \tTraining Loss: 47.155265 \tacc: 85.462580\tValidation Loss: 265.897617, time5.5\n",
      "Epoch: 973 \tTraining Loss: 47.442225 \tacc: 85.497016\tValidation Loss: 265.368460, time5.5\n",
      "Epoch: 974 \tTraining Loss: 47.633532 \tacc: 85.629017\tValidation Loss: 266.346461, time5.5\n",
      "Epoch: 975 \tTraining Loss: 47.344659 \tacc: 85.611800\tValidation Loss: 265.769387, time5.5\n",
      "Epoch: 976 \tTraining Loss: 47.658402 \tacc: 85.617539\tValidation Loss: 265.309743, time5.5\n",
      "Epoch: 977 \tTraining Loss: 47.602923 \tacc: 85.720845\tValidation Loss: 265.567040, time5.5\n",
      "Epoch: 978 \tTraining Loss: 47.392485 \tacc: 85.623278\tValidation Loss: 266.505027, time5.4\n",
      "Epoch: 979 \tTraining Loss: 47.671794 \tacc: 85.588843\tValidation Loss: 265.074135, time5.5\n",
      "Epoch: 980 \tTraining Loss: 47.317876 \tacc: 85.491276\tValidation Loss: 265.837750, time5.5\n",
      "Epoch: 981 \tTraining Loss: 47.149526 \tacc: 85.594582\tValidation Loss: 264.410693, time5.5\n",
      "Epoch: 982 \tTraining Loss: 47.017524 \tacc: 85.531451\tValidation Loss: 265.450887, time5.5\n",
      "Epoch: 983 \tTraining Loss: 47.578053 \tacc: 85.491276\tValidation Loss: 264.781053, time5.5\n",
      "Epoch: 984 \tTraining Loss: 47.048133 \tacc: 85.416667\tValidation Loss: 266.570160, time5.5\n",
      "Epoch: 985 \tTraining Loss: 47.463269 \tacc: 85.795455\tValidation Loss: 265.609887, time5.5\n",
      "Epoch: 986 \tTraining Loss: 47.032828 \tacc: 85.818411\tValidation Loss: 266.953676, time5.5\n",
      "Epoch: 987 \tTraining Loss: 47.505357 \tacc: 85.629017\tValidation Loss: 264.630862, time5.5\n",
      "Epoch: 988 \tTraining Loss: 47.467095 \tacc: 85.824151\tValidation Loss: 265.374364, time5.5\n",
      "Epoch: 989 \tTraining Loss: 47.149526 \tacc: 85.829890\tValidation Loss: 265.351994, time5.4\n",
      "Epoch: 990 \tTraining Loss: 47.602923 \tacc: 85.674931\tValidation Loss: 265.570616, time5.5\n",
      "Epoch: 991 \tTraining Loss: 47.618228 \tacc: 85.732323\tValidation Loss: 265.719769, time5.5\n",
      "Epoch: 992 \tTraining Loss: 47.522574 \tacc: 85.514233\tValidation Loss: 265.780123, time5.5\n",
      "Epoch: 993 \tTraining Loss: 47.090220 \tacc: 85.669192\tValidation Loss: 265.054898, time5.5\n",
      "Epoch: 994 \tTraining Loss: 47.224135 \tacc: 85.433884\tValidation Loss: 266.504762, time5.5\n",
      "Epoch: 995 \tTraining Loss: 47.434573 \tacc: 85.514233\tValidation Loss: 265.158489, time5.5\n",
      "Epoch: 996 \tTraining Loss: 47.180135 \tacc: 85.565886\tValidation Loss: 266.658290, time5.5\n",
      "Epoch: 997 \tTraining Loss: 47.396312 \tacc: 85.594582\tValidation Loss: 266.016422, time5.5\n",
      "Epoch: 998 \tTraining Loss: 47.750230 \tacc: 85.514233\tValidation Loss: 266.063522, time5.5\n",
      "Epoch: 999 \tTraining Loss: 47.132308 \tacc: 85.353535\tValidation Loss: 266.093514, time5.4\n",
      "Epoch: 1000 \tTraining Loss: 47.474747 \tacc: 85.594582\tValidation Loss: 266.684632, time5.5\n",
      "Epoch: 1001 \tTraining Loss: 47.350398 \tacc: 85.646235\tValidation Loss: 266.714938, time5.5\n",
      "Epoch: 1002 \tTraining Loss: 47.407790 \tacc: 85.525712\tValidation Loss: 266.783606, time5.5\n",
      "Epoch: 1003 \tTraining Loss: 46.904653 \tacc: 85.657713\tValidation Loss: 265.219112, time5.5\n",
      "Epoch: 1004 \tTraining Loss: 47.161004 \tacc: 85.410927\tValidation Loss: 264.936406, time5.5\n",
      "Epoch: 1005 \tTraining Loss: 47.734925 \tacc: 85.623278\tValidation Loss: 264.868627, time5.5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1006 \tTraining Loss: 47.252831 \tacc: 85.611800\tValidation Loss: 266.654577, time5.5\n",
      "Epoch: 1007 \tTraining Loss: 47.358050 \tacc: 85.893021\tValidation Loss: 265.356016, time5.5\n",
      "Epoch: 1008 \tTraining Loss: 47.578053 \tacc: 85.554408\tValidation Loss: 265.733098, time5.5\n",
      "Epoch: 1009 \tTraining Loss: 47.392485 \tacc: 85.542929\tValidation Loss: 266.968403, time5.5\n",
      "Epoch: 1010 \tTraining Loss: 47.185874 \tacc: 85.514233\tValidation Loss: 265.049583, time5.4\n",
      "Epoch: 1011 \tTraining Loss: 47.304484 \tacc: 85.692149\tValidation Loss: 265.792452, time5.5\n",
      "Epoch: 1012 \tTraining Loss: 47.629706 \tacc: 85.824151\tValidation Loss: 265.418246, time5.5\n",
      "Epoch: 1013 \tTraining Loss: 47.440312 \tacc: 85.726584\tValidation Loss: 264.130253, time5.5\n",
      "Epoch: 1014 \tTraining Loss: 46.864478 \tacc: 85.542929\tValidation Loss: 265.669355, time5.5\n",
      "Epoch: 1015 \tTraining Loss: 47.426921 \tacc: 85.697888\tValidation Loss: 265.898406, time5.5\n",
      "Epoch: 1016 \tTraining Loss: 47.530227 \tacc: 85.577365\tValidation Loss: 265.071694, time5.5\n",
      "Epoch: 1017 \tTraining Loss: 47.625880 \tacc: 85.617539\tValidation Loss: 265.332207, time5.5\n",
      "Epoch: 1018 \tTraining Loss: 47.182048 \tacc: 85.514233\tValidation Loss: 265.273693, time5.5\n",
      "Epoch: 1019 \tTraining Loss: 47.564662 \tacc: 85.646235\tValidation Loss: 265.874050, time5.5\n",
      "Epoch: 1020 \tTraining Loss: 47.467095 \tacc: 85.456841\tValidation Loss: 265.086933, time5.5\n",
      "Epoch: 1021 \tTraining Loss: 47.679446 \tacc: 85.795455\tValidation Loss: 266.680411, time5.4\n",
      "Epoch: 1022 \tTraining Loss: 47.725360 \tacc: 85.577365\tValidation Loss: 266.955878, time5.5\n",
      "Epoch: 1023 \tTraining Loss: 47.528313 \tacc: 85.548669\tValidation Loss: 266.089586, time5.5\n",
      "Epoch: 1024 \tTraining Loss: 47.442225 \tacc: 85.519972\tValidation Loss: 267.651965, time5.5\n",
      "Epoch: 1025 \tTraining Loss: 47.113177 \tacc: 85.525712\tValidation Loss: 267.104668, time5.5\n",
      "Epoch: 1026 \tTraining Loss: 47.426921 \tacc: 85.560147\tValidation Loss: 266.286166, time5.5\n",
      "Epoch: 1027 \tTraining Loss: 47.539792 \tacc: 85.571625\tValidation Loss: 264.426101, time5.5\n",
      "Epoch: 1028 \tTraining Loss: 47.381007 \tacc: 85.961892\tValidation Loss: 267.081645, time5.5\n",
      "Epoch: 1029 \tTraining Loss: 47.363789 \tacc: 85.554408\tValidation Loss: 266.640337, time5.5\n",
      "Epoch: 1030 \tTraining Loss: 46.962045 \tacc: 85.600321\tValidation Loss: 266.855297, time5.5\n",
      "Epoch: 1031 \tTraining Loss: 47.516835 \tacc: 85.491276\tValidation Loss: 267.158977, time5.4\n",
      "Epoch: 1032 \tTraining Loss: 47.916667 \tacc: 85.755280\tValidation Loss: 267.150255, time5.5\n",
      "Epoch: 1033 \tTraining Loss: 47.578053 \tacc: 85.606061\tValidation Loss: 266.001712, time5.5\n",
      "Epoch: 1034 \tTraining Loss: 47.289180 \tacc: 85.697888\tValidation Loss: 268.288755, time5.5\n",
      "Epoch: 1035 \tTraining Loss: 47.918580 \tacc: 85.697888\tValidation Loss: 267.165564, time5.5\n",
      "Epoch: 1036 \tTraining Loss: 47.337006 \tacc: 85.525712\tValidation Loss: 268.108092, time5.5\n",
      "Epoch: 1037 \tTraining Loss: 47.363789 \tacc: 85.588843\tValidation Loss: 267.076300, time5.5\n",
      "Epoch: 1038 \tTraining Loss: 47.331267 \tacc: 85.560147\tValidation Loss: 265.798481, time5.5\n",
      "Epoch: 1039 \tTraining Loss: 47.040481 \tacc: 85.571625\tValidation Loss: 266.414304, time5.5\n",
      "Epoch: 1040 \tTraining Loss: 47.465182 \tacc: 85.600321\tValidation Loss: 265.496782, time5.5\n",
      "Epoch: 1041 \tTraining Loss: 47.067264 \tacc: 85.657713\tValidation Loss: 266.033511, time5.5\n",
      "Epoch: 1042 \tTraining Loss: 47.352311 \tacc: 85.531451\tValidation Loss: 267.167694, time5.4\n",
      "Epoch: 1043 \tTraining Loss: 47.499617 \tacc: 85.594582\tValidation Loss: 267.625990, time5.5\n",
      "Epoch: 1044 \tTraining Loss: 47.321702 \tacc: 85.921717\tValidation Loss: 266.454305, time5.5\n",
      "Epoch: 1045 \tTraining Loss: 47.526400 \tacc: 85.743802\tValidation Loss: 266.329394, time5.5\n",
      "Epoch: 1046 \tTraining Loss: 46.992654 \tacc: 86.002066\tValidation Loss: 265.844921, time5.5\n",
      "Epoch: 1047 \tTraining Loss: 47.579966 \tacc: 85.640496\tValidation Loss: 268.309797, time5.5\n",
      "Epoch: 1048 \tTraining Loss: 47.838231 \tacc: 85.491276\tValidation Loss: 266.894618, time5.5\n",
      "Epoch: 1049 \tTraining Loss: 47.382920 \tacc: 85.910239\tValidation Loss: 267.377095, time5.5\n",
      "Epoch: 1050 \tTraining Loss: 47.713881 \tacc: 85.801194\tValidation Loss: 266.539053, time5.5\n",
      "Epoch: 1051 \tTraining Loss: 47.819100 \tacc: 85.755280\tValidation Loss: 266.191968, time5.5\n",
      "Epoch: 1052 \tTraining Loss: 47.281527 \tacc: 85.657713\tValidation Loss: 267.018929, time5.5\n",
      "Epoch: 1053 \tTraining Loss: 47.312137 \tacc: 85.606061\tValidation Loss: 266.253006, time5.4\n",
      "Epoch: 1054 \tTraining Loss: 47.666054 \tacc: 85.640496\tValidation Loss: 267.600364, time5.5\n",
      "Epoch: 1055 \tTraining Loss: 47.210744 \tacc: 85.829890\tValidation Loss: 267.334143, time5.5\n",
      "Epoch: 1056 \tTraining Loss: 47.382920 \tacc: 85.588843\tValidation Loss: 266.744364, time5.5\n",
      "Epoch: 1057 \tTraining Loss: 47.711968 \tacc: 85.755280\tValidation Loss: 265.685954, time5.5\n",
      "Epoch: 1058 \tTraining Loss: 47.382920 \tacc: 85.915978\tValidation Loss: 263.561275, time5.5\n",
      "Epoch: 1059 \tTraining Loss: 47.721534 \tacc: 85.669192\tValidation Loss: 264.969411, time5.5\n",
      "Epoch: 1060 \tTraining Loss: 47.107438 \tacc: 85.703627\tValidation Loss: 263.322700, time5.5\n",
      "Epoch: 1061 \tTraining Loss: 47.587619 \tacc: 85.738062\tValidation Loss: 264.667269, time5.5\n",
      "Epoch: 1062 \tTraining Loss: 47.449878 \tacc: 85.651974\tValidation Loss: 266.447622, time5.5\n",
      "Epoch: 1063 \tTraining Loss: 47.040481 \tacc: 85.806933\tValidation Loss: 264.807501, time5.5\n",
      "Epoch: 1064 \tTraining Loss: 46.963958 \tacc: 85.542929\tValidation Loss: 266.187703, time5.4\n",
      "Epoch: 1065 \tTraining Loss: 46.793695 \tacc: 85.686410\tValidation Loss: 264.981713, time5.5\n",
      "Epoch: 1066 \tTraining Loss: 47.159091 \tacc: 85.548669\tValidation Loss: 266.122053, time5.5\n",
      "Epoch: 1067 \tTraining Loss: 47.323615 \tacc: 85.686410\tValidation Loss: 264.121823, time5.5\n",
      "Epoch: 1068 \tTraining Loss: 47.340833 \tacc: 85.474059\tValidation Loss: 265.561289, time5.5\n",
      "Epoch: 1069 \tTraining Loss: 47.386746 \tacc: 85.812672\tValidation Loss: 264.100996, time5.5\n",
      "Epoch: 1070 \tTraining Loss: 47.340833 \tacc: 85.766758\tValidation Loss: 265.257535, time5.5\n",
      "Epoch: 1071 \tTraining Loss: 46.780303 \tacc: 85.451102\tValidation Loss: 265.017753, time5.5\n",
      "Epoch: 1072 \tTraining Loss: 47.338919 \tacc: 85.674931\tValidation Loss: 266.707066, time5.5\n",
      "Epoch: 1073 \tTraining Loss: 47.338919 \tacc: 85.732323\tValidation Loss: 265.291626, time5.5\n",
      "Epoch: 1074 \tTraining Loss: 47.799969 \tacc: 85.542929\tValidation Loss: 265.865256, time5.4\n",
      "Epoch: 1075 \tTraining Loss: 47.392485 \tacc: 85.451102\tValidation Loss: 268.101695, time5.5\n",
      "Epoch: 1076 \tTraining Loss: 47.409703 \tacc: 85.674931\tValidation Loss: 266.796509, time5.5\n",
      "Epoch: 1077 \tTraining Loss: 47.535966 \tacc: 86.007805\tValidation Loss: 265.802677, time5.5\n",
      "Epoch: 1078 \tTraining Loss: 47.046220 \tacc: 85.583104\tValidation Loss: 267.031564, time5.5\n",
      "Epoch: 1079 \tTraining Loss: 47.245179 \tacc: 85.451102\tValidation Loss: 267.280437, time5.5\n",
      "Epoch: 1080 \tTraining Loss: 47.249005 \tacc: 85.720845\tValidation Loss: 267.023715, time5.5\n",
      "Epoch: 1081 \tTraining Loss: 46.981175 \tacc: 85.583104\tValidation Loss: 266.827438, time5.5\n",
      "Epoch: 1082 \tTraining Loss: 47.296832 \tacc: 85.548669\tValidation Loss: 269.159425, time5.5\n",
      "Epoch: 1083 \tTraining Loss: 47.193526 \tacc: 85.852847\tValidation Loss: 268.674041, time5.5\n",
      "Epoch: 1084 \tTraining Loss: 47.348485 \tacc: 85.674931\tValidation Loss: 267.646861, time5.5\n",
      "Epoch: 1085 \tTraining Loss: 47.687098 \tacc: 85.680670\tValidation Loss: 268.237376, time5.4\n",
      "Epoch: 1086 \tTraining Loss: 47.407790 \tacc: 85.629017\tValidation Loss: 266.756400, time5.5\n",
      "Epoch: 1087 \tTraining Loss: 47.578053 \tacc: 85.835629\tValidation Loss: 266.407758, time5.5\n",
      "Epoch: 1088 \tTraining Loss: 46.998393 \tacc: 85.841368\tValidation Loss: 267.602544, time5.5\n",
      "Epoch: 1089 \tTraining Loss: 47.535966 \tacc: 85.629017\tValidation Loss: 268.335499, time5.5\n",
      "Epoch: 1090 \tTraining Loss: 47.520661 \tacc: 85.640496\tValidation Loss: 265.688903, time5.5\n",
      "Epoch: 1091 \tTraining Loss: 47.310223 \tacc: 85.623278\tValidation Loss: 266.178211, time5.5\n",
      "Epoch: 1092 \tTraining Loss: 47.023263 \tacc: 85.422406\tValidation Loss: 266.713549, time5.5\n",
      "Epoch: 1093 \tTraining Loss: 46.629170 \tacc: 85.726584\tValidation Loss: 267.490920, time5.5\n",
      "Epoch: 1094 \tTraining Loss: 47.283440 \tacc: 85.749541\tValidation Loss: 265.788486, time5.5\n",
      "Epoch: 1095 \tTraining Loss: 47.279614 \tacc: 85.674931\tValidation Loss: 266.568303, time5.5\n",
      "Epoch: 1096 \tTraining Loss: 47.631619 \tacc: 85.324839\tValidation Loss: 265.969490, time5.4\n",
      "Validation loss decreased (85.347796 --> 85.324839).  Saving model ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1097 \tTraining Loss: 47.375268 \tacc: 85.497016\tValidation Loss: 266.144645, time5.5\n",
      "Epoch: 1098 \tTraining Loss: 47.455617 \tacc: 85.462580\tValidation Loss: 264.827922, time5.4\n",
      "Epoch: 1099 \tTraining Loss: 47.178222 \tacc: 85.525712\tValidation Loss: 265.685240, time5.5\n",
      "Epoch: 1100 \tTraining Loss: 46.998393 \tacc: 85.611800\tValidation Loss: 267.194007, time5.4\n",
      "Epoch: 1101 \tTraining Loss: 47.118916 \tacc: 85.789715\tValidation Loss: 268.112925, time5.5\n",
      "Epoch: 1102 \tTraining Loss: 47.488139 \tacc: 85.485537\tValidation Loss: 267.032392, time5.4\n",
      "Epoch: 1103 \tTraining Loss: 47.432660 \tacc: 85.583104\tValidation Loss: 265.522496, time5.5\n",
      "Epoch: 1104 \tTraining Loss: 47.371442 \tacc: 85.583104\tValidation Loss: 267.958065, time5.4\n",
      "Epoch: 1105 \tTraining Loss: 47.268136 \tacc: 85.766758\tValidation Loss: 266.243968, time5.5\n",
      "Epoch: 1106 \tTraining Loss: 47.258571 \tacc: 85.703627\tValidation Loss: 265.434131, time5.4\n",
      "Epoch: 1107 \tTraining Loss: 46.877870 \tacc: 85.783976\tValidation Loss: 266.682425, time5.5\n",
      "Epoch: 1108 \tTraining Loss: 47.593358 \tacc: 85.554408\tValidation Loss: 265.689364, time5.4\n",
      "Epoch: 1109 \tTraining Loss: 47.660315 \tacc: 85.451102\tValidation Loss: 268.219993, time5.4\n",
      "Epoch: 1110 \tTraining Loss: 47.379094 \tacc: 85.634757\tValidation Loss: 267.779415, time5.5\n",
      "Epoch: 1111 \tTraining Loss: 47.384833 \tacc: 85.502755\tValidation Loss: 268.785442, time5.4\n",
      "Epoch: 1112 \tTraining Loss: 47.189700 \tacc: 85.594582\tValidation Loss: 266.170001, time5.5\n",
      "Epoch: 1113 \tTraining Loss: 47.260484 \tacc: 85.422406\tValidation Loss: 266.822031, time5.4\n",
      "Epoch: 1114 \tTraining Loss: 47.195439 \tacc: 85.514233\tValidation Loss: 267.641632, time5.5\n",
      "Epoch: 1115 \tTraining Loss: 47.352311 \tacc: 85.565886\tValidation Loss: 267.873171, time5.4\n",
      "Epoch: 1116 \tTraining Loss: 47.101699 \tacc: 85.680670\tValidation Loss: 267.674077, time5.5\n",
      "Epoch: 1117 \tTraining Loss: 47.304484 \tacc: 85.468320\tValidation Loss: 268.562044, time5.4\n",
      "Epoch: 1118 \tTraining Loss: 47.195439 \tacc: 85.795455\tValidation Loss: 268.239058, time5.5\n",
      "Epoch: 1119 \tTraining Loss: 47.314050 \tacc: 85.577365\tValidation Loss: 268.330610, time5.4\n",
      "Epoch: 1120 \tTraining Loss: 47.086394 \tacc: 85.537190\tValidation Loss: 268.797655, time5.5\n",
      "Epoch: 1121 \tTraining Loss: 47.176309 \tacc: 85.474059\tValidation Loss: 268.827368, time5.4\n",
      "Epoch: 1122 \tTraining Loss: 47.358050 \tacc: 85.548669\tValidation Loss: 267.481491, time5.4\n",
      "Epoch: 1123 \tTraining Loss: 47.358050 \tacc: 85.904500\tValidation Loss: 267.802850, time5.5\n",
      "Epoch: 1124 \tTraining Loss: 47.180135 \tacc: 85.537190\tValidation Loss: 266.973449, time5.4\n",
      "Epoch: 1125 \tTraining Loss: 46.969697 \tacc: 85.961892\tValidation Loss: 265.425088, time5.5\n",
      "Epoch: 1126 \tTraining Loss: 46.962045 \tacc: 85.577365\tValidation Loss: 268.376406, time5.4\n",
      "Epoch: 1127 \tTraining Loss: 47.444138 \tacc: 85.583104\tValidation Loss: 267.683293, time5.5\n",
      "Epoch: 1128 \tTraining Loss: 47.396312 \tacc: 85.606061\tValidation Loss: 267.213436, time5.4\n",
      "Epoch: 1129 \tTraining Loss: 47.455617 \tacc: 85.531451\tValidation Loss: 267.692332, time5.5\n",
      "Epoch: 1130 \tTraining Loss: 47.304484 \tacc: 85.755280\tValidation Loss: 267.091259, time5.4\n",
      "Epoch: 1131 \tTraining Loss: 47.430747 \tacc: 85.680670\tValidation Loss: 268.316601, time5.5\n",
      "Epoch: 1132 \tTraining Loss: 47.472834 \tacc: 85.875803\tValidation Loss: 267.444850, time5.4\n",
      "Epoch: 1133 \tTraining Loss: 47.415442 \tacc: 85.623278\tValidation Loss: 268.797704, time5.5\n",
      "Epoch: 1134 \tTraining Loss: 47.271962 \tacc: 85.571625\tValidation Loss: 266.319472, time5.4\n",
      "Epoch: 1135 \tTraining Loss: 47.344659 \tacc: 85.623278\tValidation Loss: 267.946231, time5.4\n",
      "Epoch: 1136 \tTraining Loss: 47.298745 \tacc: 85.531451\tValidation Loss: 267.405009, time5.5\n",
      "Epoch: 1137 \tTraining Loss: 47.157178 \tacc: 85.365014\tValidation Loss: 265.758676, time5.4\n",
      "Epoch: 1138 \tTraining Loss: 47.413529 \tacc: 85.479798\tValidation Loss: 266.836741, time5.5\n",
      "Epoch: 1139 \tTraining Loss: 47.480487 \tacc: 85.410927\tValidation Loss: 267.566507, time5.4\n",
      "Epoch: 1140 \tTraining Loss: 47.053872 \tacc: 85.697888\tValidation Loss: 266.369406, time5.5\n",
      "Epoch: 1141 \tTraining Loss: 47.568488 \tacc: 85.525712\tValidation Loss: 268.848214, time5.4\n",
      "Epoch: 1142 \tTraining Loss: 47.359963 \tacc: 85.634757\tValidation Loss: 268.279482, time5.5\n",
      "Epoch: 1143 \tTraining Loss: 47.407790 \tacc: 85.858586\tValidation Loss: 267.045747, time5.4\n",
      "Epoch: 1144 \tTraining Loss: 47.421182 \tacc: 85.554408\tValidation Loss: 266.487144, time5.5\n",
      "Epoch: 1145 \tTraining Loss: 47.411616 \tacc: 85.497016\tValidation Loss: 266.934181, time5.4\n",
      "Epoch: 1146 \tTraining Loss: 47.296832 \tacc: 85.571625\tValidation Loss: 265.070528, time5.5\n",
      "Epoch: 1147 \tTraining Loss: 47.256657 \tacc: 85.749541\tValidation Loss: 266.146844, time5.4\n",
      "Epoch: 1148 \tTraining Loss: 46.887435 \tacc: 85.508494\tValidation Loss: 267.178228, time5.4\n",
      "Epoch: 1149 \tTraining Loss: 46.950566 \tacc: 85.818411\tValidation Loss: 267.344367, time5.5\n",
      "Epoch: 1150 \tTraining Loss: 47.046220 \tacc: 85.824151\tValidation Loss: 267.884367, time5.4\n",
      "Epoch: 1151 \tTraining Loss: 47.346572 \tacc: 85.606061\tValidation Loss: 268.124041, time5.5\n",
      "Epoch: 1152 \tTraining Loss: 47.413529 \tacc: 85.669192\tValidation Loss: 267.900598, time5.3\n",
      "Epoch: 1153 \tTraining Loss: 46.860652 \tacc: 85.761019\tValidation Loss: 267.027222, time5.4\n",
      "Epoch: 1154 \tTraining Loss: 47.233701 \tacc: 85.577365\tValidation Loss: 267.292357, time5.3\n",
      "Epoch: 1155 \tTraining Loss: 46.774564 \tacc: 85.525712\tValidation Loss: 267.612015, time5.3\n",
      "Epoch: 1156 \tTraining Loss: 47.268136 \tacc: 85.468320\tValidation Loss: 266.679914, time5.4\n",
      "Epoch: 1157 \tTraining Loss: 47.176309 \tacc: 85.565886\tValidation Loss: 268.046052, time5.3\n",
      "Epoch: 1158 \tTraining Loss: 47.268136 \tacc: 85.611800\tValidation Loss: 269.290450, time5.3\n",
      "Epoch: 1159 \tTraining Loss: 47.254744 \tacc: 85.565886\tValidation Loss: 267.845930, time5.3\n",
      "Epoch: 1160 \tTraining Loss: 47.036654 \tacc: 85.623278\tValidation Loss: 267.871110, time5.3\n",
      "Epoch: 1161 \tTraining Loss: 47.333180 \tacc: 85.806933\tValidation Loss: 267.983408, time5.3\n",
      "Epoch: 1162 \tTraining Loss: 47.218396 \tacc: 85.410927\tValidation Loss: 266.964721, time5.4\n",
      "Epoch: 1163 \tTraining Loss: 47.103612 \tacc: 85.761019\tValidation Loss: 267.255381, time5.4\n",
      "Epoch: 1164 \tTraining Loss: 47.405877 \tacc: 85.738062\tValidation Loss: 268.556614, time5.5\n",
      "Epoch: 1165 \tTraining Loss: 47.145699 \tacc: 85.542929\tValidation Loss: 268.792900, time5.5\n",
      "Epoch: 1166 \tTraining Loss: 47.268136 \tacc: 85.565886\tValidation Loss: 269.540941, time5.6\n",
      "Epoch: 1167 \tTraining Loss: 47.136134 \tacc: 85.479798\tValidation Loss: 267.769644, time5.4\n",
      "Epoch: 1168 \tTraining Loss: 47.289180 \tacc: 85.686410\tValidation Loss: 269.440559, time5.4\n",
      "Epoch: 1169 \tTraining Loss: 47.679446 \tacc: 85.600321\tValidation Loss: 270.068344, time5.5\n",
      "Epoch: 1170 \tTraining Loss: 46.912305 \tacc: 85.778237\tValidation Loss: 268.144164, time5.5\n",
      "Epoch: 1171 \tTraining Loss: 46.998393 \tacc: 85.468320\tValidation Loss: 269.610430, time5.4\n",
      "Epoch: 1172 \tTraining Loss: 46.895087 \tacc: 85.542929\tValidation Loss: 270.157420, time5.4\n",
      "Epoch: 1173 \tTraining Loss: 46.931436 \tacc: 85.623278\tValidation Loss: 269.721831, time5.4\n",
      "Epoch: 1174 \tTraining Loss: 47.375268 \tacc: 85.795455\tValidation Loss: 268.131936, time5.5\n",
      "Epoch: 1175 \tTraining Loss: 47.321702 \tacc: 85.537190\tValidation Loss: 269.965537, time5.4\n",
      "Epoch: 1176 \tTraining Loss: 47.298745 \tacc: 85.491276\tValidation Loss: 268.474501, time5.4\n",
      "Epoch: 1177 \tTraining Loss: 46.933349 \tacc: 85.554408\tValidation Loss: 269.371601, time5.5\n",
      "Epoch: 1178 \tTraining Loss: 47.455617 \tacc: 85.560147\tValidation Loss: 270.044387, time5.4\n",
      "Epoch: 1179 \tTraining Loss: 47.174395 \tacc: 85.640496\tValidation Loss: 268.465829, time5.5\n",
      "Epoch: 1180 \tTraining Loss: 47.099786 \tacc: 85.686410\tValidation Loss: 269.248344, time5.4\n",
      "Epoch: 1181 \tTraining Loss: 46.986915 \tacc: 85.629017\tValidation Loss: 268.878823, time5.5\n",
      "Epoch: 1182 \tTraining Loss: 47.220309 \tacc: 85.617539\tValidation Loss: 269.154433, time5.4\n",
      "Epoch: 1183 \tTraining Loss: 47.130395 \tacc: 85.617539\tValidation Loss: 270.093225, time5.5\n",
      "Epoch: 1184 \tTraining Loss: 47.138047 \tacc: 86.197199\tValidation Loss: 269.124307, time5.4\n",
      "Epoch: 1185 \tTraining Loss: 47.884144 \tacc: 85.548669\tValidation Loss: 267.118489, time5.5\n",
      "Epoch: 1186 \tTraining Loss: 46.881696 \tacc: 85.847107\tValidation Loss: 266.516779, time5.4\n",
      "Epoch: 1187 \tTraining Loss: 47.029002 \tacc: 85.617539\tValidation Loss: 267.603725, time5.5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1188 \tTraining Loss: 46.975436 \tacc: 85.565886\tValidation Loss: 269.010708, time5.4\n",
      "Epoch: 1189 \tTraining Loss: 47.187787 \tacc: 85.497016\tValidation Loss: 269.592079, time5.4\n",
      "Epoch: 1190 \tTraining Loss: 47.051959 \tacc: 85.560147\tValidation Loss: 269.275781, time5.5\n",
      "Epoch: 1191 \tTraining Loss: 47.260484 \tacc: 85.468320\tValidation Loss: 268.735958, time5.4\n",
      "Epoch: 1192 \tTraining Loss: 47.377181 \tacc: 85.629017\tValidation Loss: 270.183770, time5.5\n",
      "Epoch: 1193 \tTraining Loss: 47.583792 \tacc: 85.646235\tValidation Loss: 269.524891, time5.4\n",
      "Epoch: 1194 \tTraining Loss: 47.030915 \tacc: 85.663453\tValidation Loss: 268.312606, time5.5\n",
      "Epoch: 1195 \tTraining Loss: 47.038567 \tacc: 85.485537\tValidation Loss: 270.475138, time5.4\n",
      "Epoch: 1196 \tTraining Loss: 47.212657 \tacc: 85.761019\tValidation Loss: 268.719550, time5.5\n",
      "Epoch: 1197 \tTraining Loss: 47.287267 \tacc: 85.611800\tValidation Loss: 267.770761, time5.4\n",
      "Epoch: 1198 \tTraining Loss: 47.145699 \tacc: 85.548669\tValidation Loss: 267.277247, time5.5\n",
      "Epoch: 1199 \tTraining Loss: 47.009871 \tacc: 85.812672\tValidation Loss: 267.241937, time5.4\n",
      "Epoch: 1200 \tTraining Loss: 47.755969 \tacc: 85.571625\tValidation Loss: 268.799757, time5.5\n",
      "Epoch: 1201 \tTraining Loss: 47.139960 \tacc: 85.514233\tValidation Loss: 268.479460, time5.4\n",
      "Epoch: 1202 \tTraining Loss: 47.491965 \tacc: 85.738062\tValidation Loss: 266.985421, time5.4\n",
      "Epoch: 1203 \tTraining Loss: 47.287267 \tacc: 85.497016\tValidation Loss: 269.283014, time5.5\n",
      "Epoch: 1204 \tTraining Loss: 47.359963 \tacc: 85.497016\tValidation Loss: 268.135379, time5.4\n",
      "Epoch: 1205 \tTraining Loss: 47.128482 \tacc: 85.611800\tValidation Loss: 268.028068, time5.5\n",
      "Epoch: 1206 \tTraining Loss: 47.516835 \tacc: 85.910239\tValidation Loss: 268.073701, time5.4\n",
      "Epoch: 1207 \tTraining Loss: 47.323615 \tacc: 86.013545\tValidation Loss: 267.185651, time5.5\n",
      "Epoch: 1208 \tTraining Loss: 47.277701 \tacc: 85.686410\tValidation Loss: 267.305280, time5.4\n",
      "Epoch: 1209 \tTraining Loss: 47.117003 \tacc: 85.445363\tValidation Loss: 269.800769, time5.5\n",
      "Epoch: 1210 \tTraining Loss: 47.381007 \tacc: 85.542929\tValidation Loss: 270.000955, time5.4\n",
      "Epoch: 1211 \tTraining Loss: 47.742577 \tacc: 85.583104\tValidation Loss: 268.771412, time5.5\n",
      "Epoch: 1212 \tTraining Loss: 47.346572 \tacc: 85.646235\tValidation Loss: 267.624960, time5.4\n",
      "Epoch: 1213 \tTraining Loss: 46.885522 \tacc: 85.720845\tValidation Loss: 268.975141, time5.5\n",
      "Epoch: 1214 \tTraining Loss: 47.285354 \tacc: 85.772498\tValidation Loss: 267.774837, time5.4\n",
      "Epoch: 1215 \tTraining Loss: 46.935262 \tacc: 85.565886\tValidation Loss: 267.950756, time5.4\n",
      "Epoch: 1216 \tTraining Loss: 47.172482 \tacc: 85.772498\tValidation Loss: 269.726881, time5.5\n",
      "Epoch: 1217 \tTraining Loss: 47.403964 \tacc: 85.571625\tValidation Loss: 269.449954, time5.4\n",
      "Epoch: 1218 \tTraining Loss: 47.384833 \tacc: 85.841368\tValidation Loss: 269.341244, time5.5\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-39c61911b212>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     41\u001b[0m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_cn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_cn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel_cn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabel_cn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m         \u001b[0mtrain_meter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_cn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel_cn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m#train_meter.update(output_cn, loss_cn)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m     \u001b[0meval_meter_cn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMeter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/BRL/c0822_dynamicFinal/utils_mlp.py\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self, y_pred, y_true)\u001b[0m\n\u001b[1;32m     54\u001b[0m             \u001b[0mtruth\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mshape\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mB\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m         \"\"\"\n\u001b[0;32m---> 56\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m         \u001b[0;31m#self.mask.append(mask.detach().cpu())\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#number of epochs to train the model\n",
    "ep_list=[2000,4000,6000]\n",
    "n_epochs = 3000\n",
    "for epoch in range(n_epochs):\n",
    "    st=time.time()\n",
    "    '''if False:#epoch in ep_list:\n",
    "        torch.save({'model_state_dict': model.state_dict(),\n",
    "            'optimizer': optimizer.state_dict()}, 'model_saved/mlp_multitask/'+str(epoch)+'.pth')\n",
    "        for param_group in optimizer.param_groups: print(param_group['lr'])\n",
    "        for param_group in optimizer.param_groups: param_group['lr'] = param_group['lr']*0.1\n",
    "        for param_group in optimizer.param_groups: print(param_group['lr'])'''\n",
    "    # monitor losses\n",
    "    train_loss = 0\n",
    "    valid_loss = 0\n",
    "    # train the model\n",
    "    model.train() # prep model for training\n",
    "    \n",
    "    train_meter = Meter()\n",
    "    for data,label_cn,label_a in train_loader:\n",
    "        data = data.to(args['device'])\n",
    "        label_cn = label_cn.to(args['device'])\n",
    "        label_a = label_a.to(args['device'])\n",
    "        # clear the gradients of all optimized variables\n",
    "        optimizer.zero_grad()\n",
    "        # forward pass: compute predicted outputs by passing inputs to the model\n",
    "        output = model(data)\n",
    "        output_cn = output[0].to(args['device'])\n",
    "        output_a = output[1].to(args['device'])\n",
    "        # calculate the loss\n",
    "        #loss = (loss_fn(output,label).float()).mean()\n",
    "        loss_cn = (loss_fn_cn(output_cn, label_cn).float()).mean()\n",
    "        ###loss_area = (loss_fn_a(output_a,label_a).float()).mean()\n",
    "        loss_area1 = ((label_cn[:,1:]*loss_fn_a(output_a,label_a)).float()).mean()#/label_cn[:,1:].sum()\n",
    "        loss_area2 = (((1-label_cn[:,1:])*loss_fn_a(output_a,label_a)).float()).mean()#/(1-label_cn[:,1:]).sum()\n",
    "        loss=(5*loss_cn+120*loss_area1+loss_area2)\n",
    "        # backward pass: compute gradient of the loss with respect to model parameters\n",
    "        loss.backward()\n",
    "        # perform a single optimization step (parameter update)\n",
    "        optimizer.step()\n",
    "        # update running training loss\n",
    "        _, output_cn = torch.max(output_cn.data, 1)\n",
    "        _, label_cn = torch.max(label_cn.data, 1)\n",
    "        train_meter.update(output_cn, label_cn)#train_meter.update(output_cn, loss_cn)\n",
    "        \n",
    "    eval_meter_cn = Meter()\n",
    "    eval_meter_a = Meter()\n",
    "    # validate the model #\n",
    "    model.eval()  # prep model for evaluation\n",
    "    for data,label_cn,label_a in val_loader:\n",
    "        data = data.to(args['device'])\n",
    "        label_cn = label_cn.to(args['device'])\n",
    "        label_a = label_a.to(args['device'])\n",
    "        # forward pass: compute predicted outputs by passing inputs to the model\n",
    "        output = model(data)\n",
    "        output_cn = output[0].to(args['device'])\n",
    "        output_a = output[1].to(args['device'])\n",
    "        # update running validation loss\n",
    "        eval_meter_a.update(output_cn[:,1:]*output_a, output_cn[:,1:]*label_a)\n",
    "        _, output_cn = torch.max(output_cn.data, 1)\n",
    "        _, label_cn = torch.max(label_cn.data, 1)\n",
    "        eval_meter_cn.update(output_cn, label_cn)\n",
    "        #eval_meter_a.update(output_a, label_a)\n",
    "    \n",
    "    # print training/validation statistics # calculate average loss over an epoch\n",
    "    train_loss=train_meter.compute_metric('acc')\n",
    "    valid_loss_cn=eval_meter_cn.compute_metric('acc')\n",
    "    valid_loss_a=np.sum(eval_meter_a.compute_metric('rmse'))\n",
    "    \n",
    "    print('Epoch: {} \\tTraining Loss: {:.6f} \\tacc: {:.6f}\\tValidation Loss: {:.6f}, time{:.1f}'.format(epoch+1, train_loss,valid_loss_cn,valid_loss_a,time.time()-st))\n",
    "    \n",
    "    # save model if validation loss has decreased\n",
    "    if valid_loss_cn <= valid_loss_min:\n",
    "        print('Validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(valid_loss_min,valid_loss_cn))\n",
    "        #torch.save(model.state_dict(), 'model_saved/mlp_cn/model.pt')\n",
    "        torch.save({'model_state_dict': model.state_dict(),\n",
    "            'optimizer': optimizer.state_dict()}, 'model_saved/mlp/multitask/model.pth')\n",
    "        \n",
    "        valid_loss_min = valid_loss_cn\n",
    "        \n",
    "torch.save({'model_state_dict': model.state_dict(),\n",
    "            'optimizer': optimizer.state_dict()}, 'model_saved/mlp/multitask/last.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(288, device='cuda:1')"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(output_cn==label_cn).sum()#/512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "512"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(output_cn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 22,  65,  34, 104,  77,  10,  57,  36,  61,  26,  56, 119,  37,  51,\n",
       "          6,   2, 120,  49,  12,  61, 116,  63,  83,  61,   9,   6,  18,  77,\n",
       "         89,  60,  85,  22,  41,  18, 106,  47, 116,  81, 109,  65,   2,  21,\n",
       "         56,  64,  91, 115,  28,   4,  70,  25,  28,  29,  71,  95,  72,  63,\n",
       "         79, 115,  34,   7,  55,  15,  83,  76,  23,  93,  61,  39,  65,   3,\n",
       "         22,  92, 113,  34,  68, 114,  78,  57,  42,   4,  29,  57,   9,  24,\n",
       "         66, 117,  22,  17,  36,  76,  66,  10,  97,  74,  64,  86,  64,  49,\n",
       "         54, 117,  77,  85,  93,  97,   3,  27,  68,  66,  20,  36,  66,  51,\n",
       "         37,  78,  72,  73, 108,  73,  99,  72,   8,   8,  86,  66,  74,  61,\n",
       "        116,   1,  48,  89,  56,  29,  27,  51,  20,  71,   8,  20,  11,  41,\n",
       "        117,  61,  85,  85, 108,  80,  25,  83,  44, 102,  66,  38,  37, 115,\n",
       "         36, 119,  72, 108,  37,  35,  89,  16,  48,  91, 118,  36, 114,  94,\n",
       "        111,  83,  65,  39,  81,  94,  72, 110,  47,  10,  88, 116,   7,  99,\n",
       "         39,  38,  61,  77, 109,  81,  50, 107,  26,  39,  43,  21,  14,  76,\n",
       "         92,  97, 104,  97,  73,  83,  66,  94,  33,  34,  67,  36, 102,  64,\n",
       "         54,  96,  39,  53,  66,  55,  91,  10,  65,  61,  96,  55,  16,  52,\n",
       "         62,  71, 106, 113,   6,  11,  16,   7,  93,  38,  52,  40,  67,  60,\n",
       "         84,  13,  74,  80,   0,  79,  56,  42,  90, 115,  80, 107,  23, 119,\n",
       "         15, 111, 116,  51, 104, 101,  46,  88,  45,  80,  41,  38,  34,  16,\n",
       "         20,  38,  22,  73,  40,  77,  10,  88,  39, 103,  32,  29,  71,  69,\n",
       "        104, 108, 105,  82,  92,  17,  96,  11,  37,  78,  48,  39, 117,  67,\n",
       "        108, 104,  17,  41,   9,  19,  52,  23, 103, 114,  26, 102, 104,  62,\n",
       "         32,   0,  38,  25,  57,  92, 119, 112,  65,  48,  52,  29,  40,  50,\n",
       "         12,  11,  41, 113,  63, 109,  98,  18,  80,  33, 120,   2,  66, 116,\n",
       "         88,  44, 101,  80,  59,   7,  93,  14,  14, 114,  48,  88,  70,  30,\n",
       "          7,  81,  46,  43, 101,  30,  15, 112,  69,  75,  44,  83,  48,  77,\n",
       "         21, 119,  43,  47,  62,  64,  55, 120,  26,  90,  47,  39, 105,  13,\n",
       "         33,  53,  16,  70,  97,  35,  89,  83,  58, 112,  23, 108,  27, 101,\n",
       "        117,  54,  65,  34,  35,  62,  18,  57,  74,  19,  39,  99,  92,   5,\n",
       "         18,  17,  89, 114,  39,  38,  44,  42, 101, 118,  85,  10,  86,  31,\n",
       "        108,  26,  73,  13,  59,  77,  27,  13,  79,  18,  96,  26,  82,  98,\n",
       "         13,  38, 112,  90,  86,  22,  84, 108,  76,  82, 105,   9,  29,   3,\n",
       "         64,  53,  16,  74,   3,  33,   3,  71,  33, 113,  60,  88, 104,  88,\n",
       "        118,  94,  55,  79, 110, 100,  26,  84,  63,  47,  70, 110,  27,  49,\n",
       "         49, 118,  20,  25,  63,  91,  23,  29, 115,   1,  46, 120,  92,  43,\n",
       "        116,  68,  30,  52,  94,  68,  52,  74,  54,  17,  42,  84,  65,  53,\n",
       "         66, 116,  38,  78, 104, 120,  48,  94], device='cuda:1')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_cn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 66,  65,  32, 104,  92, 112,  57,  88,  61,  26,  56,  51,  37,  96,\n",
       "          6,   2, 120,  49,  12,  61, 116,  63,  89,  61,   9, 120,  18,  77,\n",
       "         89,  60,  37,  56, 120,  18,  72,  56, 120,  81, 120,  65,   2,  21,\n",
       "         56,  64,  66,  90,  28,   4,  70,  25,  28,  72,  50,  74,  72,  63,\n",
       "         70,  17,  34,  85,  73,  86,  83,  76,  10,  93,  73,   5, 104,   3,\n",
       "         22,  84,  98,   4,  68, 114,  66, 118,  16,   4,  29,  80, 106,  24,\n",
       "         65, 117,  22,  98,  36,  90,  66,  10,  16,  24,  64,  86,  97,  92,\n",
       "        112,  28, 101,  50, 114,  96,  58,  12,  68,  66,  20,  36,  66, 109,\n",
       "         37,  78,  72,  68, 108,  73,  99,  72,   8,   8,  86,  66, 100,  92,\n",
       "        116,   1,  48,  52,  56,   1, 102,  96,  12,  80,   8,  20,  60,  48,\n",
       "         81, 108,  70,  82,  76,  80,  25,   9,  44,  96,  66,  38,   8, 115,\n",
       "         36,  27,  72, 108,  37,  85,  89,  16,  48,  11, 100, 120, 114,  94,\n",
       "         98,  10,  65,  39,  81,  42,  72, 110,  83,  98,  88, 116,   4,   2,\n",
       "         39,  38,  12,  90,  42,  49,  50,  65,  26,  57,  25,  26,  14,  60,\n",
       "         32,  97, 104,  97,  70,  88,  66,  94,  54,  34,   5,  36,  15,  64,\n",
       "         42,  96, 107,  53,  24,  72,  85,  10,  58,  61,  96,  55, 105,  52,\n",
       "         62,  60, 106,  14,   6,  11,  16,  22,  65,  38,  52,  40,  41,  28,\n",
       "         84,  12,  60,  80,  33,  17,  56,  42,  60,   5,  80,  21,  23, 119,\n",
       "         58,  99, 116,  98, 104,  86,  56,  88,  44,  80,  41,  72,  34,  16,\n",
       "         20,  44,  66,  73,  40,  77,  10,  88,  39,  35,  32,  29,  25,  69,\n",
       "        104, 108,  70,  24,  92,  17,  96,  93,  37,  96,  48,  52, 118,   1,\n",
       "        108, 104,  17, 112,   9,  77,  52,  23, 103,  36,  16, 102, 104,  62,\n",
       "         32,   0,  38,  25,  57,  92,  82, 112, 120,  48,  52,  29,  40,  50,\n",
       "         12,  40,  17,  36,  73,  17,  98,  18,  80,  33, 120,   8,  72, 116,\n",
       "         88,  44,  40,  80,  90,  16,   8,  14,  92, 114,  48,  88,  40,  30,\n",
       "         64,  81,  46,  27, 101,  30,  37, 112,  69,  75,  44,   1,  48,  26,\n",
       "         21, 119,  13,  62,  62,  64,  94, 120,  26,  72,  49,  65,  25,  88,\n",
       "         40,  97,  16,  70, 116,  64,  89,  56,  58, 112,  77,  92,  34, 101,\n",
       "         38,  54,  65,  34,  35, 104, 108, 106,  74,  41,  62,  64,  92,   5,\n",
       "         18,  17,  89, 114,  32,  22,  44,  42, 101, 118,   1,  10,  64,  58,\n",
       "        108,  66,  73,  33,  83,  42,  27,  13,  73,  18,  96,  64,  75,  98,\n",
       "         16,  38, 112,  90,  86,  22,  84, 108,  76,  82,  42,  22,  34,  70,\n",
       "         64,  53,  16,  20,   3,  33,   3,  45,  98, 113,  48,  88, 104,  88,\n",
       "         96,  54,  68,  38, 110, 100,  26,  84,  96,  49,  70, 110,  27,  49,\n",
       "         49,  98,  20,  25,  63, 101,  58,  85, 115,   1,  46, 120,  92,  43,\n",
       "        116,  68,  30,  36,  86,  68,  24,  40,  54,  17,  42,  68,  82,  53,\n",
       "         66, 116,  38,  78, 104,   8,  48,  74], device='cuda:1')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_cn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save({'model_state_dict': model.state_dict(),\n",
    "            'optimizer': optimizer.state_dict()}, 'model_saved/mlp/multitask/last.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.0336, device='cuda:1', grad_fn=<MeanBackward0>),\n",
       " tensor(0.0006, device='cuda:1', grad_fn=<MeanBackward0>),\n",
       " tensor(0.0080, device='cuda:1', grad_fn=<MeanBackward0>))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_cn,loss_area1,loss_area2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.6934, device='cuda:1', grad_fn=<MeanBackward0>),\n",
       " tensor(0.0074, device='cuda:1', grad_fn=<MeanBackward0>),\n",
       " tensor(0.4885, device='cuda:1', grad_fn=<MeanBackward0>))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_cn,loss_area1,loss_area2"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
