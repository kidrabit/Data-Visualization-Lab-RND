{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from scipy.stats import multivariate_normal\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "from tqdm import trange\n",
    "import time\n",
    "from algorithm_utils import Algorithm, PyTorchUtils\n",
    "from sklearn import metrics\n",
    "from utils import LSTMEDModule\n",
    "\n",
    "class LSTMED(Algorithm, PyTorchUtils):\n",
    "    def __init__(self, name: str = 'LSTM-ED', num_epochs: int = 10, batch_size: int = 32, lr: float = 1e-3,\n",
    "                 hidden_size: int = 5, sequence_length: int = 30, train_gaussian_percentage: float = 0.25,\n",
    "                 n_layers: tuple = (1, 1), use_bias: tuple = (True, True), dropout: tuple = (0, 0),\n",
    "                 seed: int = None, gpu: int = 0, details=True):\n",
    "        Algorithm.__init__(self, __name__, name, seed, details=details)\n",
    "        PyTorchUtils.__init__(self, seed, gpu)\n",
    "        self.num_epochs = num_epochs\n",
    "        self.batch_size = batch_size\n",
    "        self.lr = lr\n",
    "\n",
    "        self.hidden_size = hidden_size\n",
    "        self.sequence_length = sequence_length\n",
    "        self.train_gaussian_percentage = train_gaussian_percentage\n",
    "\n",
    "        self.n_layers = n_layers\n",
    "        self.use_bias = use_bias\n",
    "        self.dropout = dropout\n",
    "\n",
    "        self.lstmed = None\n",
    "        self.mean, self.cov = None, None\n",
    "\n",
    "    def fit(self, X):\n",
    "        data =  X.copy()#X.values\n",
    "        sequences = [data[i:i + self.sequence_length] for i in range(0,data.shape[0] - self.sequence_length + 1, self.sequence_length)]\n",
    "        print(len(sequences))\n",
    "        indices = np.random.permutation(len(sequences))\n",
    "        split_point = int(self.train_gaussian_percentage * len(sequences))\n",
    "        train_loader = DataLoader(dataset=sequences, batch_size=self.batch_size, drop_last=True,\n",
    "                                  sampler=SubsetRandomSampler(indices[:-split_point]), pin_memory=True)\n",
    "        train_gaussian_loader = DataLoader(dataset=sequences, batch_size=self.batch_size, drop_last=True,\n",
    "                                           sampler=SubsetRandomSampler(indices[-split_point:]), pin_memory=True)\n",
    "\n",
    "        self.lstmed = LSTMEDModule(X.shape[1], self.hidden_size,\n",
    "                                   self.n_layers, self.use_bias, self.dropout,\n",
    "                                   seed=self.seed, gpu=self.gpu)\n",
    "        self.to_device(self.lstmed)\n",
    "        optimizer = torch.optim.Adam(self.lstmed.parameters(), lr=self.lr)\n",
    "\n",
    "        self.lstmed.train()\n",
    "        #loss_list=[]\n",
    "        for epoch in trange(self.num_epochs):\n",
    "            #st=time.time()\n",
    "            logging.debug(f'Epoch {epoch+1}/{self.num_epochs}.')\n",
    "            for ts_batch in train_loader:\n",
    "                output = self.lstmed(self.to_var(ts_batch))\n",
    "                loss = nn.MSELoss(size_average=False)(output, self.to_var(ts_batch.float()))\n",
    "                self.lstmed.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "        self.lstmed.eval()\n",
    "        error_vectors = []\n",
    "        for ts_batch in train_gaussian_loader:\n",
    "            output = self.lstmed(self.to_var(ts_batch))\n",
    "            error = nn.L1Loss(reduce=False)(output, self.to_var(ts_batch.float()))\n",
    "            error_vectors += list(error.view(-1, X.shape[1]).data.cpu().numpy())\n",
    "\n",
    "        self.mean = np.mean(error_vectors, axis=0)\n",
    "        self.cov = np.cov(error_vectors, rowvar=False)\n",
    "\n",
    "    def predict(self, X):\n",
    "        data = X.copy() #data = X.values\n",
    "        sequences = [data[i:i + self.sequence_length] for i in range(0,data.shape[0] - self.sequence_length + 1, self.sequence_length)]\n",
    "        data_loader = DataLoader(dataset=sequences, batch_size=self.batch_size, shuffle=False, drop_last=False)\n",
    "\n",
    "        self.lstmed.eval()\n",
    "        mvnormal = multivariate_normal(self.mean, self.cov, allow_singular=True)\n",
    "        scores = []\n",
    "        outputs = []\n",
    "        errors = []\n",
    "        for idx, ts in enumerate(data_loader):\n",
    "            output = self.lstmed(self.to_var(ts))\n",
    "            error = nn.L1Loss(reduce=False)(output, self.to_var(ts.float()))\n",
    "            score = -mvnormal.logpdf(error.view(-1, X.shape[1]).data.cpu().numpy())\n",
    "            #print(idx,'error',(error.float()).mean(),'score',np.mean(score))\n",
    "            scores.append(score.reshape(ts.size(0), self.sequence_length))\n",
    "            if self.details:\n",
    "                outputs.append(output.data.cpu().numpy())\n",
    "                errors.append(error.data.cpu().numpy())\n",
    "\n",
    "        # stores seq_len-many scores per timestamp and averages them\n",
    "        scores = np.concatenate(scores)\n",
    "        print('scores.shape',scores.shape,np.sum(np.isnan(scores)))#(5760, 90)\n",
    "\n",
    "        if self.details:\n",
    "            outputs = np.concatenate(outputs)\n",
    "            print(outputs.shape)#(5760, 90, 14)\n",
    "            self.prediction_details.update({'reconstructions_mean': outputs})\n",
    "\n",
    "            errors = np.concatenate(errors)\n",
    "            self.prediction_details.update({'errors_mean': errors})\n",
    "\n",
    "        return scores,outputs\n",
    "\n",
    "\n",
    "#data preprecessing\n",
    "data_list=[]\n",
    "file_names=['2006-05-13.csv','2006-05-14.csv','2006-05-15.csv','2006-05-16.csv','2006-05-17.csv',\n",
    "            '2006-05-18.csv','2006-05-19.csv','2007-12-14.csv','2009-05-05.csv','2011-11-01.csv']\n",
    "x_train=[];X=[]\n",
    "for f in file_names:\n",
    "    data=pd.read_csv('data/raw_data/'+f)\n",
    "    #anomaly detection\n",
    "    if f in ['2007-12-14.csv','2009-05-05.csv','2011-11-01.csv']:\n",
    "            q25,q75=np.quantile(data['SJS13'],[0.25,0.75]);print(q25-1.5*(q75-q25),q75+1.5*(q75-q25))\n",
    "            data['SJS13'][data['SJS13']<q25-1.5*(q75-q25)]=np.nan#3000\n",
    "            data['SJS13'][data['SJS13']>q75+1.5*(q75-q25)]=np.nan#2700\n",
    "    data=data.interpolate(method='linear')\n",
    "    data=data.iloc[90:-90]\n",
    "    #stack\n",
    "    data_list.append(data)\n",
    "    X.append(data.values)\n",
    "    if f in ['2006-05-13.csv','2006-05-14.csv','2006-05-15.csv','2006-05-16.csv','2006-05-17.csv',\n",
    "             '2006-05-18.csv','2006-05-19.csv','2007-12-14.csv']:\n",
    "        x_train.append(data.values)\n",
    "X=np.concatenate(X,axis=0)\n",
    "print(len(x_train))\n",
    "#scaling-training set\n",
    "x_train=np.concatenate(x_train,axis=0)#np.concatenate([data_list[0].values,data_list[1].values],axis=0);print(x_train.shape)#data_list[0].values\n",
    "m_train=np.mean(x_train,axis=0)#m_train=np.mean(X,axis=0)#mn=np.min(x_train,axis=0)#\n",
    "std_train=np.std(x_train,axis=0)#std_train=np.std(X,axis=0)#mx=np.max(x_train,axis=0)#\n",
    "x_train=(x_train-m_train)/std_train#x_train=(x_train-mn)/(mx-mn)#\n",
    "for i in range(14):  print(i,np.min(x_train[:,i]),np.max(x_train[:,i]),np.mean(x_train[:,i]),np.std(x_train[:,i]))\n",
    "#scaling-test set\n",
    "\n",
    "x_test1=data_list[-2].values\n",
    "#m_train=np.mean(x_test1,axis=0);std_train=np.std(x_test1,axis=0)\n",
    "x_test1=(x_test1-m_train)/std_train#x_test1=(x_test1-mn)/(mx-mn)##\n",
    "\n",
    "x_test2=data_list[-1].values\n",
    "#m_train=np.mean(x_test2,axis=0);std_train=np.std(x_test2,axis=0)\n",
    "x_test2=(x_test2-m_train)/std_train#x_test2=(x_test2-mn)/(mx-mn)##\n",
    "\n",
    "#x_test3=data_list[-1].values\n",
    "#m_train=np.mean(x_test2,axis=0);std_train=np.std(x_test2,axis=0)\n",
    "#x_test3=(x_test3-m_train)/std_train\n",
    "for i in range(14): print(i,np.min(x_test1[:,i]),np.max(x_test1[:,i]),np.mean(x_test1[:,i]),np.std(x_test1[:,i]))\n",
    "for i in range(14): print(i,np.min(x_test2[:,i]),np.max(x_test2[:,i]),np.mean(x_test2[:,i]),np.std(x_test1[:,i]))\n",
    "    \n",
    "lstmed=LSTMED(num_epochs=50,sequence_length=90)\n",
    "lstmed.fit(x_train)\n",
    "\n",
    "state=torch.load('model2.pth')\n",
    "lstmed.lstmed.load_state_dict(state['model_state_dict'])\n",
    "'''target0 = [x_train[i:i + 30] for i in range(0,x_train.shape[0] - 30 + 1,30)]\n",
    "target0=np.stack(target0,axis=0)\n",
    "target1 = [x_test1[i:i + 30] for i in range(0,x_test1.shape[0] - 30 + 1,30)]\n",
    "target1=np.stack(target1,axis=0)\n",
    "target2 = [x_test2[i:i + 30] for i in range(0,x_test2.shape[0] - 30 + 1,30)]\n",
    "target2=np.stack(target2,axis=0)'''\n",
    "\n",
    "sc0,output0=lstmed.predict(x_train)\n",
    "sc1,output1=lstmed.predict(x_test1)\n",
    "sc2,output2=lstmed.predict(x_test2)\n",
    "output0=output0.reshape((-1,14));sc0=sc0.reshape((-1))\n",
    "output1=output1.reshape((-1,14));sc1=sc1.reshape((-1))\n",
    "output2=output2.reshape((-1,14));sc2=sc2.reshape((-1))\n",
    "print(np.mean(sc0)+np.std(sc0),np.mean(sc1)+np.std(sc1),np.mean(sc2)+np.std(sc2))\n",
    "\n",
    "'''idx=np.where(sc1>2225029)[0]\n",
    "len(idx)\n",
    "idx=np.where(sc2>2225029)[0]\n",
    "len(idx)'''\n",
    "\n",
    "fig=plt.figure(figsize=(20,3))\n",
    "plt.plot(sc0,'.',label='sc0',alpha=0.1)\n",
    "plt.plot(sc1,'.',label='sc1',alpha=0.1)\n",
    "plt.grid()\n",
    "\n",
    "fig=plt.figure(figsize=(20,3))\n",
    "plt.plot(sc2,'.',label='sc2',alpha=0.7)\n",
    "plt.grid()\n",
    "plt.legend()\n",
    "#plt.savefig('images/sjs13_2.png',bbox_inces='tight', pad_inches=0)\n",
    "plt.show()\n",
    "\n",
    "for i in range(14):\n",
    "    fig=plt.figure(figsize=(20,3))\n",
    "    plt.plot(x_train[:,i],label='target1',alpha=0.8)\n",
    "    plt.plot(output0[:,i],label='output1',alpha=0.8)\n",
    "    plt.grid()\n",
    "    plt.legend()\n",
    "    plt.savefig('images/results/'+str(i)+'.png',bbox_inces='tight', pad_inches=0)\n",
    "    plt.show()\n",
    "\n",
    "for i in range(14):\n",
    "    fig=plt.figure(figsize=(20,3))\n",
    "    plt.plot(x_test1[:,i],label='target1',alpha=0.8)\n",
    "    plt.plot(output1[:,i],label='output1',alpha=0.8)\n",
    "    plt.grid()\n",
    "    plt.legend()\n",
    "    plt.savefig('images/results/val/'+str(i)+'.png',bbox_inces='tight', pad_inches=0)\n",
    "    plt.show()\n",
    "\n",
    "for i in range(14):\n",
    "    fig=plt.figure(figsize=(20,3))\n",
    "    plt.plot(x_test2[:,i],label='target1',alpha=0.8)\n",
    "    plt.plot(output2[:,i],label='output1',alpha=0.8)\n",
    "    plt.grid()\n",
    "    plt.legend()\n",
    "    plt.savefig('images/results/test/'+str(i)+'.png',bbox_inces='tight', pad_inches=0)\n",
    "    plt.title(str(i))\n",
    "    plt.show()\n",
    "\n",
    "y_t=(x_test2*std_train)+m_train\n",
    "y_p=(output2*std_train)+m_train\n",
    "for i in range(14):\n",
    "    mape=np.mean(np.abs((y_t[:,i]-y_p[:,i])/y_t[:,i]))*100\n",
    "    print(i,mape)\n",
    "    \n",
    "fig=plt.figure(figsize=(20,3))\n",
    "for i in range(14):\n",
    "    plt.plot(np.abs((x_test2[:,i]-output2[:,i])/x_test2[:,i]),'.',label=str(i),alpha=0.8)\n",
    "plt.grid()\n",
    "plt.legend()\n",
    "#plt.savefig('images/sjs13_2.png',bbox_inces='tight', pad_inches=0)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
