{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using backend: pytorch\n",
      "/home/shs/anaconda3/lib/python3.7/site-packages/statsmodels/tools/_testing.py:19: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n",
      "  import pandas.util.testing as tm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "__main__:151: DeprecationWarning: The class is deprecated and will be removed from dgl in v0.5. Import MPNNPredictor from dgllife.model instead.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import BCEWithLogitsLoss\n",
    "from torch.utils.data import DataLoader\n",
    "from dgl import model_zoo,DGLGraph\n",
    "import math\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "from utils_merge import Meter, set_random_seed, collate, EarlyStopping,load_brl_dataset\n",
    "import argparse\n",
    "# from sklearn import svm, datasets\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# from sklearn.metrics import plot_confusion_matrix\n",
    "import seaborn as sn\n",
    "import pandas as pd\n",
    "from sklearn import metrics\n",
    "import collections\n",
    "import sys; sys.argv=['']; del sys\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from dgl.contrib.deprecation import deprecated\n",
    "from dgl.nn.pytorch import Set2Set, NNConv\n",
    "    \n",
    "class ImputationNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ImputationNet,self).__init__()\n",
    "        \n",
    "        self.fc1 = nn.Linear(6*7+7, 512)\n",
    "        self.fc2 = nn.Linear(512,512)\n",
    "        self.fc3 = nn.Linear(512,512)\n",
    "        self.fc_last = nn.Linear(512,359*7)\n",
    "        self.tanh=nn.Tanh()\n",
    "        \n",
    "    def forward(self,x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = self.tanh(self.fc_last(x))#self.fc_last(x)#\n",
    "        return x\n",
    "\n",
    "class MPNNModel(nn.Module):\n",
    "    @deprecated('Import MPNNPredictor from dgllife.model instead.', 'class')\n",
    "    def __init__(self,\n",
    "                 node_input_dim=15,\n",
    "                 edge_input_dim=5,\n",
    "                 #output_dim=12,\n",
    "                 node_hidden_dim=64,\n",
    "                 edge_hidden_dim=128,#64,#\n",
    "                 num_step_message_passing=6,#4,#\n",
    "                 num_step_set2set=6,#4,#\n",
    "                 num_layer_set2set=3):#2):#\n",
    "        super(MPNNModel, self).__init__()\n",
    "\n",
    "        self.num_step_message_passing = num_step_message_passing\n",
    "        self.lin0 = nn.Linear(node_input_dim, node_hidden_dim)\n",
    "        edge_network = nn.Sequential(\n",
    "            nn.Linear(edge_input_dim, edge_hidden_dim), nn.ReLU(),\n",
    "            nn.Linear(edge_hidden_dim, node_hidden_dim * node_hidden_dim))\n",
    "        self.conv = NNConv(in_feats=node_hidden_dim,\n",
    "                           out_feats=node_hidden_dim,\n",
    "                           edge_func=edge_network,\n",
    "                           aggregator_type='sum')\n",
    "        self.gru = nn.GRU(node_hidden_dim, node_hidden_dim)\n",
    "\n",
    "        self.set2set = Set2Set(node_hidden_dim, num_step_set2set, num_layer_set2set)\n",
    "        self.lin1 = nn.Linear(2 * node_hidden_dim, node_hidden_dim)\n",
    "        \n",
    "        self.lin2_1 = nn.Linear(node_hidden_dim, 121)\n",
    "        self.lin2_2 = nn.Linear(node_hidden_dim, 1)\n",
    "\n",
    "    def forward(self, g, n_feat, e_feat):\n",
    "        out = F.relu(self.lin0(n_feat))                 # (B1, H1)\n",
    "        h = out.unsqueeze(0)                            # (1, B1, H1)\n",
    "        \n",
    "        for i in range(self.num_step_message_passing):\n",
    "            m = F.relu(self.conv(g, out, e_feat))       # (B1, H1)\n",
    "            out, h = self.gru(m.unsqueeze(0), h)\n",
    "            out = out.squeeze(0)\n",
    "            \n",
    "        out = self.set2set(g, out)\n",
    "        out = F.relu(self.lin1(out))\n",
    "        \n",
    "        out1=self.lin2_1(out)#for classification\n",
    "        out2=self.lin2_2(out)#for regression\n",
    "        return [out1,out2]\n",
    "\n",
    "class ImPrNet(nn.Module):\n",
    "    def __init__(self, model_disp, model_multitask):\n",
    "        super(ImPrNet, self).__init__()\n",
    "        self.model_imputation = model_disp\n",
    "        self.model_multitask = model_multitask\n",
    "        \n",
    "    def forward(self, g,  x, e_feat):#x(n,6*7+7)-disp6,nf\n",
    "        disp_359 = self.model_imputation(x)#(n,359*7)-disp359\n",
    "        disp_359=disp_359.view(-1,359,7)\n",
    "        x=x[:,:6*7].view(-1,6,7)\n",
    "        \n",
    "        x=torch.cat((disp_359,x),axis=1)\n",
    "        x=x.view(len(x)*365,7)\n",
    "        #print('forwad',len(g),x.shape)\n",
    "        \n",
    "        #h = bg.ndata.pop('n_feat')\n",
    "        #e = bg.edata.pop('e_feat')\n",
    "        #h, e = h.to(args['device']), e.to(args['device'])\n",
    "        #for i in range(len(g)):g[i].ndata['n_feat'] =x[i]\n",
    "        g.ndata['n_feat'] =x\n",
    "        \n",
    "        output = self.model_multitask(g,  x, e_feat)\n",
    "        \n",
    "        return output\n",
    "    \n",
    "\n",
    "\n",
    "print(torch.cuda.current_device())\n",
    "torch.cuda.set_device(1)\n",
    "print(torch.cuda.current_device())\n",
    "\n",
    "\n",
    "parser = argparse.ArgumentParser(description='Molecule Regression')\n",
    "parser.add_argument('-m', '--model', type=str,default='MPNN',help='Model to use')#choices=['MPNN', 'SCHNET', 'MGCN', 'AttentiveFP'],\n",
    "#parser.add_argument('-d', '--dataset', type=str, default='bridge',help='Dataset to use')#choices=['Alchemy', 'Aromaticity'],                \n",
    "parser.add_argument('-p', '--pre-trained', action='store_true', default=False, help='Whether to skip training and use a pre-trained model')\n",
    "args = parser.parse_args().__dict__\n",
    "training_setting= {\n",
    "    'random_seed': 0,\n",
    "    'batch_size': 64,#64,#\n",
    "    'num_epochs': 2000,#900,\n",
    "    'node_in_feats': 7,\n",
    "    'edge_in_feats': 6,\n",
    "    #'output_dim': 120,\n",
    "    'lr': 0.00001,#0.001,#\n",
    "    'patience': 300,#20,\n",
    "    'metric_name': 'l1',#'roc_auc',#\n",
    "    'weight_decay': 0,\n",
    "    'n_task':41,\n",
    "}\n",
    "args.update(training_setting)\n",
    "args['device'] = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "args['data_path_x']='data/data_mlp/single/input6/'#'data/data_mpnn/'\n",
    "args['data_path_label']='data/label_multitask/'\n",
    "set_random_seed(args['random_seed'])\n",
    "\n",
    "\n",
    "train_loader,val_loader,test_loader=load_brl_dataset(args)#for batch_id, batch_data in enumerate(train_loader):bg, labels = batch_data;print(a)\n",
    "\n",
    "#model = load_model(args)\n",
    "model_i = ImputationNet()\n",
    "model_p = MPNNModel(node_input_dim=args['node_in_feats'],\n",
    "                  edge_input_dim=args['edge_in_feats'])\n",
    "\n",
    "# Load state dicts\n",
    "model_i.load_state_dict(torch.load('model_saved/mlp/single/imputation/last.pth')['model_state_dict'])\n",
    "model_p.load_state_dict(torch.load('model_saved/multitask/model2_5000/early_stop.pth')['model_state_dict'])\n",
    "\n",
    "model = ImPrNet(model_i, model_p)\n",
    "model.to(args['device'])\n",
    "\n",
    "loss_fn_cablenumber =nn.CrossEntropyLoss()\n",
    "loss_fn_area =nn.L1Loss(reduction='none')\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=args['lr'], weight_decay=args['weight_decay'])\n",
    "stopper = EarlyStopping(mode='lower', patience=args['patience'],\n",
    "                        filename='model_saved/finetuning/early_stop.pth')\n",
    "\n",
    "\n",
    "def run_a_train_epoch_cr(args, epoch, model, data_loader,\n",
    "                      loss_criterion_cablenumber,loss_criterion_area, optimizer):\n",
    "    model.train()\n",
    "    train_meter = Meter()\n",
    "    correct=0\n",
    "    total=0\n",
    "    for batch_id, batch_data in enumerate(data_loader):\n",
    "        bg, h, labels_cablenumber,labels_area = batch_data\n",
    "        labels_cablenumber = labels_cablenumber.to(args['device'])\n",
    "        labels_area = labels_area.to(args['device'])\n",
    "        labels=[labels_cablenumber,labels_area]\n",
    "        #prediction = regress(args, model, bg)\n",
    "        #h = bg.ndata.pop('n_feat')\n",
    "        e = bg.edata.pop('e_feat')\n",
    "        h, e = h.to(args['device']), e.to(args['device'])\n",
    "        prediction = model(bg, h, e)\n",
    "        \n",
    "        loss_cablenumber = (loss_criterion_cablenumber(prediction[0], labels_cablenumber).float()).mean()\n",
    "        loss_area = (loss_criterion_area(prediction[1],labels_area).float()).mean()\n",
    "        loss=(loss_cablenumber+loss_area)\n",
    "       \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_meter.update(prediction[1], labels_area)\n",
    "        \n",
    "        _, predicted = torch.max(prediction[0].data, 1)\n",
    "        total += labels_cablenumber.size(0)\n",
    "        correct += (predicted == labels_cablenumber).sum().item();del predicted\n",
    "        \n",
    "    total_score_acc=100-100 * correct / total\n",
    "    total_score = np.mean(train_meter.compute_metric(args['metric_name']))\n",
    "    print('epoch {:d}/{:d}, training {} {:.4f} / accuracy(%) {:.4f}'.format(\n",
    "        epoch + 1, args['num_epochs'], args['metric_name'], total_score, total_score_acc)) \n",
    "    #print('epoch {:d}/{:d}, training total_score_acc {:.4f}'.format(epoch + 1, args['num_epochs'], total_score_acc)) \n",
    "    \n",
    "def run_an_eval_epoch_cr(args, model, data_loader):\n",
    "    model.eval()\n",
    "    eval_meter = Meter()\n",
    "    correct=0\n",
    "    total=0\n",
    "    with torch.no_grad():\n",
    "        for batch_id, batch_data in enumerate(data_loader):\n",
    "            bg,h, labels_cablenumber,labels_area = batch_data\n",
    "            labels_cablenumber = labels_cablenumber.to(args['device'])\n",
    "            labels_area = labels_area.to(args['device'])\n",
    "            labels=[labels_cablenumber,labels_area]\n",
    "            #prediction = regress(args, model, bg)\n",
    "            #h = bg.ndata.pop('n_feat')\n",
    "            e = bg.edata.pop('e_feat')\n",
    "            h, e = h.to(args['device']), e.to(args['device'])\n",
    "            prediction = model(bg, h, e)\n",
    "            \n",
    "            \n",
    "            eval_meter.update(prediction[1], labels_area)\n",
    "            \n",
    "            _, predicted = torch.max(prediction[0].data, 1)\n",
    "            total += labels_cablenumber.size(0)\n",
    "            correct += (predicted == labels_cablenumber).sum().item();del predicted\n",
    "        total_score_acc=100-100 * correct / total\n",
    "        total_score = np.mean(eval_meter.compute_metric(args['metric_name']))\n",
    "    return total_score,total_score_acc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(5000):#250+100\n",
    "    st=time.time()\n",
    "    if epoch%1000==0 and epoch!=0:\n",
    "        print('save')\n",
    "        torch.save({'model_state_dict': model.state_dict(),\n",
    "            'optimizer': optimizer.state_dict()}, 'model_saved/finetuning/'+str(epoch)+'.pth')\n",
    "    # Train\n",
    "    run_a_train_epoch_cr(args, epoch, model, train_loader, loss_fn_cablenumber, loss_fn_area, optimizer)\n",
    "    # Validation and early stop\n",
    "    val_score,val_acc = run_an_eval_epoch_cr(args, model, val_loader)\n",
    "    early_stop = stopper.step(val_acc, model,optimizer)\n",
    "    print('epoch {:d}/{:d}, validation {} {:.4f}, accuracy(%) {:.4f}, best validation {} {:.4f}, time{:.1f}'.format(\n",
    "        epoch + 1, args['num_epochs'], args['metric_name'], val_score,val_acc,args['metric_name'], stopper.best_score,time.time()-st))\n",
    "    #if early_stop:\n",
    "    #    break\n",
    "torch.save({'model_state_dict': model.state_dict(),\n",
    "        'optimizer': optimizer.state_dict()}, 'model_saved/finetuning/last.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.save({'model_state_dict': model.state_dict(),\n",
    "#             'optimizer': optimizer.state_dict()}, 'model_saved/multitask/model2_5000/1.pth')\n",
    "state=torch.load('model_saved/finetuning/lr1/early_stop.pth') \n",
    "model.load_state_dict(state['model_state_dict'])\n",
    "optimizer.load_state_dict(state['optimizer'])\n",
    "for param_group in optimizer.param_groups: print(param_group['lr'])\n",
    "for param_group in optimizer.param_groups: param_group['lr'] = param_group['lr']*0.1\n",
    "for param_group in optimizer.param_groups: print(param_group['lr'])\n",
    " "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
