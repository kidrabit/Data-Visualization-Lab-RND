{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from dgl.contrib.deprecation import deprecated\n",
    "from dgl.nn.pytorch import Set2Set, NNConv\n",
    "\n",
    "class MPNNModel(nn.Module):\n",
    "    @deprecated('Import MPNNPredictor from dgllife.model instead.', 'class')\n",
    "    def __init__(self,\n",
    "                 node_input_dim=15,\n",
    "                 edge_input_dim=5,\n",
    "                 #output_dim=12,\n",
    "                 node_hidden_dim=64,\n",
    "                 edge_hidden_dim=128,#64,#\n",
    "                 num_step_message_passing=6,#4,#\n",
    "                 num_step_set2set=6,#4,#\n",
    "                 num_layer_set2set=3):#2):#\n",
    "        super(MPNNModel, self).__init__()\n",
    "\n",
    "        self.num_step_message_passing = num_step_message_passing\n",
    "        self.lin0 = nn.Linear(node_input_dim, node_hidden_dim)\n",
    "        edge_network = nn.Sequential(\n",
    "            nn.Linear(edge_input_dim, edge_hidden_dim), nn.ReLU(),\n",
    "            nn.Linear(edge_hidden_dim, node_hidden_dim * node_hidden_dim))\n",
    "        self.conv = NNConv(in_feats=node_hidden_dim,\n",
    "                           out_feats=node_hidden_dim,\n",
    "                           edge_func=edge_network,\n",
    "                           aggregator_type='sum')\n",
    "        self.gru = nn.GRU(node_hidden_dim, node_hidden_dim)\n",
    "\n",
    "        self.set2set = Set2Set(node_hidden_dim, num_step_set2set, num_layer_set2set)\n",
    "        self.lin1 = nn.Linear(2 * node_hidden_dim, node_hidden_dim)\n",
    "        #self.lin2 = nn.Linear(node_hidden_dim, output_dim)\n",
    "        ###self.lin2_1 = nn.Linear(node_hidden_dim, node_hidden_dim)\n",
    "        ###self.lin2_2 = nn.Linear(node_hidden_dim, node_hidden_dim)\n",
    "        \n",
    "        self.lin2_1 = nn.Linear(node_hidden_dim, 121)\n",
    "        self.lin2_2 = nn.Linear(node_hidden_dim, 1)\n",
    "\n",
    "    def forward(self, g, n_feat, e_feat):\n",
    "        out = F.relu(self.lin0(n_feat))                 # (B1, H1)\n",
    "        h = out.unsqueeze(0)                            # (1, B1, H1)\n",
    "        \n",
    "        for i in range(self.num_step_message_passing):\n",
    "            m = F.relu(self.conv(g, out, e_feat))       # (B1, H1)\n",
    "            out, h = self.gru(m.unsqueeze(0), h)\n",
    "            out = out.squeeze(0)\n",
    "            \n",
    "        out = self.set2set(g, out)\n",
    "        out = F.relu(self.lin1(out))\n",
    "        #out = self.lin2(out)\n",
    "        \n",
    "        ###out1 = F.relu(self.lin2_1(out))\n",
    "        ###out2 = F.relu(self.lin2_2(out))\n",
    "        \n",
    "        out1=self.lin2_1(out)#for classification\n",
    "        out2=self.lin2_2(out)#for regression\n",
    "        return [out1,out2]\n",
    "\n",
    "import numpy as np\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import BCEWithLogitsLoss\n",
    "from torch.utils.data import DataLoader\n",
    "from dgl import model_zoo,DGLGraph\n",
    "import math\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "from utils_mpnn_multitask import Meter, set_random_seed, collate, EarlyStopping,load_brl_dataset,regress\n",
    "import argparse\n",
    "# from sklearn import svm, datasets\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# from sklearn.metrics import plot_confusion_matrix\n",
    "import seaborn as sn\n",
    "import pandas as pd\n",
    "from sklearn import metrics\n",
    "import collections\n",
    "import sys; sys.argv=['']; del sys\n",
    "parser = argparse.ArgumentParser(description='Molecule Regression')\n",
    "parser.add_argument('-m', '--model', type=str,default='MPNN',help='Model to use')#choices=['MPNN', 'SCHNET', 'MGCN', 'AttentiveFP'],\n",
    "#parser.add_argument('-d', '--dataset', type=str, default='bridge',help='Dataset to use')#choices=['Alchemy', 'Aromaticity'],                \n",
    "parser.add_argument('-p', '--pre-trained', action='store_true', default=False, help='Whether to skip training and use a pre-trained model')\n",
    "args = parser.parse_args().__dict__\n",
    "training_setting= {\n",
    "    'random_seed': 0,\n",
    "    'batch_size': 64,#64,#\n",
    "    'num_epochs': 2000,#900,\n",
    "    'node_in_feats': 7,\n",
    "    'edge_in_feats': 6,\n",
    "    #'output_dim': 120,\n",
    "    'lr': 0.001,#0.001,#\n",
    "    'patience': 300,#20,\n",
    "    'metric_name': 'l1',#'roc_auc',#\n",
    "    'weight_decay': 0,\n",
    "    'n_task':41,\n",
    "}\n",
    "args.update(training_setting)\n",
    "args['device'] = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "args['data_path_x']='data/data_mpnn/'\n",
    "args['data_path_label']='data/label_multitask/'\n",
    "set_random_seed(args['random_seed'])\n",
    "\n",
    "print(torch.cuda.current_device())\n",
    "torch.cuda.set_device(1)\n",
    "print(torch.cuda.current_device())\n",
    "\n",
    "train_loader,val_loader,test_loader=load_brl_dataset(args)#for batch_id, batch_data in enumerate(train_loader):bg, labels = batch_data;print(a)\n",
    "if args['pre_trained']:\n",
    "    args['num_epochs'] = 0\n",
    "    model = model_zoo.chem.load_pretrained(args['exp'])\n",
    "else:\n",
    "    #model = load_model(args)\n",
    "    model = MPNNModel(node_input_dim=args['node_in_feats'],\n",
    "                      edge_input_dim=args['edge_in_feats'])\n",
    "    if args['model'] in ['SCHNET', 'MGCN']:\n",
    "        model.set_mean_std(train_set.mean, train_set.std, args['device'])\n",
    "    #loss_fn =BCEWithLogitsLoss()#nn.L1Loss(reduction='none')\n",
    "    loss_fn_cablenumber =nn.CrossEntropyLoss()\n",
    "    loss_fn_area =nn.L1Loss(reduction='none')\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=args['lr'], weight_decay=args['weight_decay'])\n",
    "    stopper = EarlyStopping(mode='lower', patience=args['patience'],\n",
    "                            filename='model_saved/multitask/model2_5000/early_stop.pth')\n",
    "\n",
    "model.to(args['device'])\n",
    "\n",
    "    \n",
    "def run_a_train_epoch_cr(args, epoch, model, data_loader,\n",
    "                      loss_criterion_cablenumber,loss_criterion_area, optimizer):\n",
    "    model.train()\n",
    "    train_meter = Meter()\n",
    "    correct=0\n",
    "    total=0\n",
    "    for batch_id, batch_data in enumerate(data_loader):\n",
    "        bg, labels_cablenumber,labels_area = batch_data\n",
    "        labels_cablenumber = labels_cablenumber.to(args['device'])\n",
    "        labels_area = labels_area.to(args['device'])\n",
    "        labels=[labels_cablenumber,labels_area]\n",
    "        prediction = regress(args, model, bg)\n",
    "        \n",
    "        loss_cablenumber = (loss_criterion_cablenumber(prediction[0], labels_cablenumber).float()).mean()\n",
    "        loss_area = (loss_criterion_area(prediction[1],labels_area).float()).mean()\n",
    "        loss=(loss_cablenumber+loss_area)\n",
    "       \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_meter.update(prediction[1], labels_area)\n",
    "        \n",
    "        _, predicted = torch.max(prediction[0].data, 1)\n",
    "        total += labels_cablenumber.size(0)\n",
    "        correct += (predicted == labels_cablenumber).sum().item();del predicted\n",
    "        \n",
    "    total_score_acc=100-100 * correct / total\n",
    "    total_score = np.mean(train_meter.compute_metric(args['metric_name']))\n",
    "    print('epoch {:d}/{:d}, training {} {:.4f} / accuracy(%) {:.4f}'.format(\n",
    "        epoch + 1, args['num_epochs'], args['metric_name'], total_score, total_score_acc)) \n",
    "    #print('epoch {:d}/{:d}, training total_score_acc {:.4f}'.format(epoch + 1, args['num_epochs'], total_score_acc)) \n",
    "    \n",
    "def run_an_eval_epoch_cr(args, model, data_loader):\n",
    "    model.eval()\n",
    "    eval_meter = Meter()\n",
    "    correct=0\n",
    "    total=0\n",
    "    with torch.no_grad():\n",
    "        for batch_id, batch_data in enumerate(data_loader):\n",
    "            bg, labels_cablenumber,labels_area = batch_data\n",
    "            labels_cablenumber = labels_cablenumber.to(args['device'])\n",
    "            labels_area = labels_area.to(args['device'])\n",
    "            labels=[labels_cablenumber,labels_area]\n",
    "            prediction = regress(args, model, bg)\n",
    "            \n",
    "            eval_meter.update(prediction[1], labels_area)\n",
    "            \n",
    "            _, predicted = torch.max(prediction[0].data, 1)\n",
    "            total += labels_cablenumber.size(0)\n",
    "            correct += (predicted == labels_cablenumber).sum().item();del predicted\n",
    "        total_score_acc=100-100 * correct / total\n",
    "        total_score = np.mean(eval_meter.compute_metric(args['metric_name']))\n",
    "    return total_score,total_score_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state=torch.load('model_saved/multitask/model2_5000/last_4000.pth') \n",
    "model.load_state_dict(state['model_state_dict'])\n",
    "optimizer.load_state_dict(state['optimizer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    \n",
    "for epoch in range(1000):#250+100\n",
    "    st=time.time()\n",
    "    if epoch%500==0 and epoch!=0:\n",
    "        print('save')\n",
    "        torch.save({'model_state_dict': model.state_dict(),\n",
    "            'optimizer': optimizer.state_dict()}, 'model_saved/multitask/'+str(epoch)+'.pth')\n",
    "    # Train\n",
    "    run_a_train_epoch_cr(args, epoch, model, train_loader, loss_fn_cablenumber, loss_fn_area, optimizer)\n",
    "    # Validation and early stop\n",
    "    val_score,val_acc = run_an_eval_epoch_cr(args, model, val_loader)\n",
    "    early_stop = stopper.step(val_acc, model,optimizer)\n",
    "    print('epoch {:d}/{:d}, validation {} {:.4f}, accuracy(%) {:.4f}, best validation {} {:.4f}, time{:.1f}'.format(\n",
    "        epoch + 1, args['num_epochs'], args['metric_name'], val_score,val_acc,args['metric_name'], stopper.best_score,time.time()-st))\n",
    "    #if early_stop:\n",
    "    #    break\n",
    "    torch.save({'model_state_dict': model.state_dict(),\n",
    "            'optimizer': optimizer.state_dict()}, 'model_saved/multitask/model2_5000/last.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.save({'model_state_dict': model.state_dict(),\n",
    "#             'optimizer': optimizer.state_dict()}, 'model_saved/multitask/model2_5000/1.pth')\n",
    "state=torch.load('model_saved/multitask/model2_5000/early_stop1.pth') \n",
    "model.load_state_dict(state['model_state_dict'])\n",
    "optimizer.load_state_dict(state['optimizer'])\n",
    "for param_group in optimizer.param_groups: print(param_group['lr'])\n",
    "for param_group in optimizer.param_groups: param_group['lr'] = param_group['lr']*0.1\n",
    "for param_group in optimizer.param_groups: print(param_group['lr'])\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    \n",
    "for epoch in range(5000):#250+100\n",
    "    st=time.time()\n",
    "    if epoch%500==0 and epoch!=0:\n",
    "        print('save')\n",
    "        torch.save({'model_state_dict': model.state_dict(),\n",
    "            'optimizer': optimizer.state_dict()}, 'model_saved/multitask/model2_5000/'+str(epoch)+'.pth')\n",
    "    # Train\n",
    "    run_a_train_epoch_cr(args, epoch, model, train_loader, loss_fn_cablenumber, loss_fn_area, optimizer)\n",
    "    # Validation and early stop\n",
    "    val_score,val_acc = run_an_eval_epoch_cr(args, model, val_loader)\n",
    "    early_stop = stopper.step(val_acc, model,optimizer)\n",
    "    print('epoch {:d}/{:d}, validation {} {:.4f}, accuracy(%) {:.4f}, best validation {} {:.4f}, time{:.1f}'.format(\n",
    "        epoch + 1, args['num_epochs'], args['metric_name'], val_score,val_acc,args['metric_name'], stopper.best_score,time.time()-st))\n",
    "    #if early_stop:\n",
    "    #    break\n",
    "    torch.save({'model_state_dict': model.state_dict(),\n",
    "            'optimizer': optimizer.state_dict()}, 'model_saved/multitask/model2_5000/last.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state=torch.load('model_saved/multitask/model2_5000/early_stop.pth') \n",
    "model.load_state_dict(state['model_state_dict'])\n",
    "optimizer.load_state_dict(state['optimizer'])\n",
    "\n",
    "model.eval()\n",
    "#eval_meter = Meter()\n",
    "cn_p=[];area_p=[];cn_t=[];area_t=[]\n",
    "with torch.no_grad():\n",
    "    for batch_id, batch_data in enumerate(test_loader):\n",
    "        \n",
    "        bg, labels_cablenumber,labels_area = batch_data\n",
    "        labels_cablenumber = labels_cablenumber.to(args['device'])\n",
    "        labels_area = labels_area.to(args['device'])\n",
    "        #labels=[labels_cablenumber,labels_area]\n",
    "        cn_t.append(labels_cablenumber.cpu().detach().numpy())\n",
    "        area_t.append(labels_area.cpu().detach().numpy())\n",
    "        \n",
    "        prediction = regress(args, model, bg)\n",
    "        cn_p.append(prediction[0].cpu().detach().numpy())\n",
    "        area_p.append(prediction[1].cpu().detach().numpy())\n",
    "\n",
    "cn_p=np.concatenate(cn_p,axis=0)\n",
    "cn_p=np.argmax(cn_p,axis=1)\n",
    "area_p=np.concatenate(area_p,axis=0)\n",
    "\n",
    "cn_t=np.concatenate(cn_t,axis=0)\n",
    "area_t=np.concatenate(area_t,axis=0)\n",
    "\n",
    "print(cn_p.\n",
    "      shape,area_p.shape,cn_t.shape,area_t.shape)\n",
    "\n",
    "acc=np.sum(cn_p==cn_t)/len(cn_p)\n",
    "print('acc: ',acc)\n",
    "# plt.plot(area_t,area_p,'.')\n",
    "\n",
    "w_idx=np.where(cn_p!=cn_t)\n",
    "c_idx=np.where(cn_p==cn_t)\n",
    "area=np.load('data/label_multitask/a_label_test.npz')['arr_0']\n",
    "print('acc mean',np.mean(area[w_idx]),np.mean(area[c_idx]))\n",
    "\n",
    "import sklearn.metrics as metrics\n",
    "t_area=area_t.copy();p_area=area_p.copy()\n",
    "#plt.plot(p_area.flatten(),t_area.flatten(),'.',color='darkblue',alpha=0.2)\n",
    "plt.plot(t_area[c_idx].flatten(),p_area[c_idx].flatten(),'.',color='deepskyblue',alpha=0.5)\n",
    "plt.plot(t_area[w_idx].flatten(),p_area[w_idx].flatten(),'.',color='crimson',alpha=0.3)\n",
    "#plt.ylim(np.min([a_p,a_t]), np.max([a_p,a_t]))\n",
    "#plt.xlim(np.min([a_p,a_t]), np.max([a_p,a_t]))\n",
    "plt.ylim(-0.1,1.1)\n",
    "plt.xlim(-0.1,1.1)\n",
    "plt.xlabel('actual')\n",
    "plt.ylabel('pred')\n",
    "plt.grid()\n",
    "plt.savefig('images/multitask/test_area.png', bbox_inches = 'tight')\n",
    "plt.show()\n",
    "\n",
    "y_t=area_t.copy();y_p=area_p.copy()\n",
    "print(y_t.shape,y_p.shape)#(\n",
    "import sklearn.metrics as metrics\n",
    "mae = metrics.mean_absolute_error(y_t, y_p)\n",
    "mse = metrics.mean_squared_error(y_t, y_p)\n",
    "rmse=np.sqrt(mse)\n",
    "print('mae', mae, '| rmse:',rmse)\n",
    "print('actual mean', np.mean(y_t),'| pred mean',np.mean(y_p))\n",
    "print(np.corrcoef(y_t.flatten(),y_p.flatten()))\n",
    "print('rmse/range',rmse/(np.max(y_t)-np.min(y_t)))\n",
    "print('mape',np.mean(np.abs(y_t-y_p)/y_t))\n",
    "iqr= np.subtract(*np.percentile(y_t, [75, 25]))\n",
    "print('rmse/iqr',rmse/iqr)\n",
    "print('rmse/mean',rmse/np.mean(y_t))\n",
    "print('actual min max', np.min(y_t),np.max(y_t))\n",
    "print('pred min max', np.min(y_p),np.max(y_p))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#validation set\n",
    "\n",
    "state=torch.load('model_saved/multitask/model2_5000/early_stop.pth') \n",
    "model.load_state_dict(state['model_state_dict'])\n",
    "optimizer.load_state_dict(state['optimizer'])\n",
    "\n",
    "model.eval()\n",
    "#eval_meter = Meter()\n",
    "cn_p=[];area_p=[];cn_t=[];area_t=[]\n",
    "with torch.no_grad():\n",
    "    for batch_id, batch_data in enumerate(val_loader):\n",
    "        \n",
    "        bg, labels_cablenumber,labels_area = batch_data\n",
    "        labels_cablenumber = labels_cablenumber.to(args['device'])\n",
    "        labels_area = labels_area.to(args['device'])\n",
    "        #labels=[labels_cablenumber,labels_area]\n",
    "        cn_t.append(labels_cablenumber.cpu().detach().numpy())\n",
    "        area_t.append(labels_area.cpu().detach().numpy())\n",
    "        \n",
    "        prediction = regress(args, model, bg)\n",
    "        cn_p.append(prediction[0].cpu().detach().numpy())\n",
    "        area_p.append(prediction[1].cpu().detach().numpy())\n",
    "\n",
    "cn_p=np.concatenate(cn_p,axis=0)\n",
    "cn_p=np.argmax(cn_p,axis=1)\n",
    "area_p=np.concatenate(area_p,axis=0)\n",
    "\n",
    "cn_t=np.concatenate(cn_t,axis=0)\n",
    "area_t=np.concatenate(area_t,axis=0)\n",
    "\n",
    "print(cn_p.\n",
    "      shape,area_p.shape,cn_t.shape,area_t.shape)\n",
    "\n",
    "acc=np.sum(cn_p==cn_t)/len(cn_p)\n",
    "print('acc: ',acc)\n",
    "# plt.plot(area_t,area_p,'.')\n",
    "\n",
    "w_idx=np.where(cn_p!=cn_t)\n",
    "c_idx=np.where(cn_p==cn_t)\n",
    "area=np.load('data/label_multitask/a_label_val.npz')['arr_0']\n",
    "print('acc mean',np.mean(area[w_idx]),np.mean(area[c_idx]))\n",
    "\n",
    "\n",
    "import sklearn.metrics as metrics\n",
    "t_area=area_t.copy();p_area=area_p.copy()\n",
    "#plt.plot(p_area.flatten(),t_area.flatten(),'.',color='darkblue',alpha=0.2)\n",
    "plt.plot(t_area[c_idx].flatten(),p_area[c_idx].flatten(),'.',color='deepskyblue',alpha=0.5)\n",
    "plt.plot(t_area[w_idx].flatten(),p_area[w_idx].flatten(),'.',color='crimson',alpha=0.3)\n",
    "#plt.ylim(np.min([a_p,a_t]), np.max([a_p,a_t]))\n",
    "#plt.xlim(np.min([a_p,a_t]), np.max([a_p,a_t]))\n",
    "plt.ylim(-0.1,1.1)\n",
    "plt.xlim(-0.1,1.1)\n",
    "plt.xlabel('actual')\n",
    "plt.ylabel('pred')\n",
    "plt.grid()\n",
    "plt.savefig('images/multitask/val_area.png', bbox_inches = 'tight')\n",
    "plt.show()\n",
    "\n",
    "y_t=area_t.copy();y_p=area_p.copy()\n",
    "print(y_t.shape,y_p.shape)#(\n",
    "import sklearn.metrics as metrics\n",
    "mae = metrics.mean_absolute_error(y_t, y_p)\n",
    "mse = metrics.mean_squared_error(y_t, y_p)\n",
    "rmse=np.sqrt(mse)\n",
    "print('mae', mae, '| rmse:',rmse)\n",
    "print('actual mean', np.mean(y_t),'| pred mean',np.mean(y_p))\n",
    "print(np.corrcoef(y_t.flatten(),y_p.flatten()))\n",
    "print('rmse/range',rmse/(np.max(y_t)-np.min(y_t)))\n",
    "print('mape',np.mean(np.abs(y_t-y_p)/y_t))\n",
    "iqr= np.subtract(*np.percentile(y_t, [75, 25]))\n",
    "print('rmse/iqr',rmse/iqr)\n",
    "print('rmse/mean',rmse/np.mean(y_t))\n",
    "print('actual min max', np.min(y_t),np.max(y_t))\n",
    "print('pred min max', np.min(y_p),np.max(y_p))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state=torch.load('model_saved/multitask/model2_5000/early_stop.pth') \n",
    "model.load_state_dict(state['model_state_dict'])\n",
    "optimizer.load_state_dict(state['optimizer'])\n",
    "\n",
    "model.eval()\n",
    "#eval_meter = Meter()\n",
    "cn_p=[];area_p=[];cn_t=[];area_t=[]\n",
    "with torch.no_grad():\n",
    "    for batch_id, batch_data in enumerate(train_loader):\n",
    "        \n",
    "        bg, labels_cablenumber,labels_area = batch_data\n",
    "        labels_cablenumber = labels_cablenumber.to(args['device'])\n",
    "        labels_area = labels_area.to(args['device'])\n",
    "        #labels=[labels_cablenumber,labels_area]\n",
    "        cn_t.append(labels_cablenumber.cpu().detach().numpy())\n",
    "        area_t.append(labels_area.cpu().detach().numpy())\n",
    "        \n",
    "        prediction = regress(args, model, bg)\n",
    "        cn_p.append(prediction[0].cpu().detach().numpy())\n",
    "        area_p.append(prediction[1].cpu().detach().numpy())\n",
    "\n",
    "cn_p=np.concatenate(cn_p,axis=0)\n",
    "cn_p=np.argmax(cn_p,axis=1)\n",
    "area_p=np.concatenate(area_p,axis=0)\n",
    "\n",
    "cn_t=np.concatenate(cn_t,axis=0)\n",
    "area_t=np.concatenate(area_t,axis=0)\n",
    "\n",
    "print(cn_p.\n",
    "      shape,area_p.shape,cn_t.shape,area_t.shape)\n",
    "\n",
    "acc=np.sum(cn_p==cn_t)/len(cn_p)\n",
    "print('acc: ',acc)\n",
    "# plt.plot(area_t,area_p,'.')\n",
    "\n",
    "w_idx=np.where(cn_p!=cn_t)\n",
    "c_idx=np.where(cn_p==cn_t)\n",
    "area=np.load('data/label_multitask/a_label_train.npz')['arr_0']\n",
    "print('acc mean',np.mean(area[w_idx]),np.mean(area[c_idx]))\n",
    "\n",
    "\n",
    "import sklearn.metrics as metrics\n",
    "t_area=area_t.copy();p_area=area_p.copy()\n",
    "#plt.plot(p_area.flatten(),t_area.flatten(),'.',color='darkblue',alpha=0.2)\n",
    "plt.plot(t_area[c_idx].flatten(),p_area[c_idx].flatten(),'.',color='deepskyblue',alpha=0.5)\n",
    "plt.plot(t_area[w_idx].flatten(),p_area[w_idx].flatten(),'.',color='crimson',alpha=0.3)\n",
    "#plt.ylim(np.min([a_p,a_t]), np.max([a_p,a_t]))\n",
    "#plt.xlim(np.min([a_p,a_t]), np.max([a_p,a_t]))\n",
    "plt.ylim(-0.1,1.1)\n",
    "plt.xlim(-0.1,1.1)\n",
    "plt.xlabel('actual')\n",
    "plt.ylabel('pred')\n",
    "plt.grid()\n",
    "plt.savefig('images/multitask/train_area.png', bbox_inches = 'tight')\n",
    "plt.show()\n",
    "\n",
    "y_t=area_t.copy();y_p=area_p.copy()\n",
    "print(y_t.shape,y_p.shape)#(\n",
    "import sklearn.metrics as metrics\n",
    "mae = metrics.mean_absolute_error(y_t, y_p)\n",
    "mse = metrics.mean_squared_error(y_t, y_p)\n",
    "rmse=np.sqrt(mse)\n",
    "print('mae', mae, '| rmse:',rmse)\n",
    "print('actual mean', np.mean(y_t),'| pred mean',np.mean(y_p))\n",
    "print(np.corrcoef(y_t.flatten(),y_p.flatten()))\n",
    "print('rmse/range',rmse/(np.max(y_t)-np.min(y_t)))\n",
    "print('mape',np.mean(np.abs(y_t-y_p)/y_t))\n",
    "iqr= np.subtract(*np.percentile(y_t, [75, 25]))\n",
    "print('rmse/iqr',rmse/iqr)\n",
    "print('rmse/mean',rmse/np.mean(y_t))\n",
    "print('actual min max', np.min(y_t),np.max(y_t))\n",
    "print('pred min max', np.min(y_p),np.max(y_p))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
