{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f5836f0a",
   "metadata": {},
   "source": [
    "# GNN 직접 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4c1cdd04",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Graph convolution model\n",
    "import torch_geometric.nn as pyg_nn\n",
    "# Graph utility function\n",
    "import torch_geometric.utils as pyg_utils\n",
    "\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "\n",
    "# 사용할 데이터셋\n",
    "from torch_geometric.datasets import TUDataset\n",
    "from torch_geometric.datasets import Planetoid\n",
    "from torch_geometric.data import DataLoader\n",
    "\n",
    "import torch_geometric.transforms as T\n",
    "\n",
    "from tensorboardX import SummaryWriter\n",
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1d1f4e98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GNNStack class 생성\n",
    "class GNNStack(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, task = 'node'):\n",
    "        super(GNNStack, self).__init__() \n",
    "        self.task = task\n",
    "        # task에 맞는 layer설정\n",
    "        self.convs = nn.ModuleList()\n",
    "        self.convs.append(self.build_conv_model(input_dim,hidden_dim))\n",
    "        # 정규화\n",
    "        self.lns = nn.ModuleList()\n",
    "        self.lns.append(nn.LayerNorm(hidden_dim))\n",
    "        \n",
    "        for l in range(2):\n",
    "            self.convs.append(self.build_conv_model(hidden_dim, hidden_dim))\n",
    "        \n",
    "        # MLP\n",
    "        self.post_mp = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, hidden_dim), nn.Dropout(0.25), \n",
    "            nn.Linear(hidden_dim, output_dim))\n",
    "        \n",
    "        if not (self.task == 'node' or self.task == 'graph'):\n",
    "            raise RuntimeError('Unknown task.')\n",
    "            \n",
    "        self.dropout = 0.25\n",
    "        self.num_layers = 3\n",
    "        \n",
    "        \n",
    "    def build_conv_model(self, input_dim, hidden_dim):\n",
    "    # refer to pytorch geometric nn module for different implementation of GNNs.\n",
    "    if self.task == 'node': # node classification\n",
    "        return CustomConv(input_dim, hidden_dim)\n",
    "        \n",
    "    def forward(self, data):\n",
    "        '''\n",
    "        x: feature matrix \\in R ^(# of nodes \\times d(embedding dimension))\n",
    "        edge_index : sparse adjacency list\n",
    "        ex) node1: [1,4,6]\n",
    "        batch: batch마다 node의 개수가 다름. => 매우 복잡\n",
    "        '''\n",
    "        x, edge_index, batch = data.x, data.edge_index, data.batch \n",
    "        \n",
    "        # feature가 없는 경우\n",
    "        if data.num_node_features == 0:\n",
    "            x = torch.ones(data.num_nodes, 1)\n",
    "            \n",
    "        for i in range(self.num_layers):\n",
    "            x = self.convs[i](x, edge_index) # Modulelist # convolution layer\n",
    "            emb = x\n",
    "            x = F.relu(x)\n",
    "            # testing에는 사용 x # nn.Dropout()을 사용하면 이럴 필요 없을텐데,, 굳이,,?\n",
    "            x = F.dropout(x, p=self.dropout, training=self.training) \n",
    "            if not i == self.num_layers - 1:\n",
    "                x = self.lns[i](x)\n",
    "                \n",
    "            # graph classification인 경우, pooling이 필요하므로 !!\n",
    "            if self.task == 'graph':\n",
    "                x = pyg_nn.global_mean_pool(x, batch) # max\n",
    "        \n",
    "        x = self.post_mp(x) # Sequential MLP\n",
    "        \n",
    "        return emb, F.log_softmax(x,dim = 1) # emd: to visualize that graph looks like # F.lof_softmax: CROSS-ENTROPY를 위해\n",
    "    \n",
    "    def loss(self, pred, label):\n",
    "        return F.null_loss(pred, label) # nn.CrossEntropyLoss는 nn.LogSoftmax()와 nn.NLLLoss() # label: one-hot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "865cc7ee",
   "metadata": {},
   "source": [
    "Here pyg_nn.GCNConv and pyg_nn.GINConv are instances of MessagePassing. They define a single layer of graph convolution, which can be decomposed into:\n",
    "* Message computation\n",
    "* Aggregation\n",
    "* Update\n",
    "* Pooling\n",
    "\n",
    "Here we give an example of how to subclass the pytorch geometric MessagePassing class to derive a new model (rather than using existing GCNConv and GINConv).\n",
    "\n",
    "We make use of `MessagePassing`'s key building blocks:\n",
    "- `aggr='add'`: The aggregation method to use (\"add\", \"mean\" or \"max\").\n",
    "- `propagate()`: The initial call to start propagating messages. Takes in the edge indices and any other data to pass along (e.g. to update node embeddings).\n",
    "- `message()`: Constructs messages to node i. Takes any argument which was initially passed to propagate().\n",
    "- `update()`: Updates node embeddings. Takes in the output of aggregation as first argument and any argument which was initially passed to propagate()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c4364d54",
   "metadata": {},
   "outputs": [],
   "source": [
    "?pyg_nn.MessagePassing.propagate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bf652264",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomConv(pyg_nn.MessagePassing):\n",
    "    def __init__(self,in_channels, out_channels):\n",
    "        super(CustomConv,self).__init__(aggr = 'add')\n",
    "        self.linear = nn.Linear(in_channels, out_channels)\n",
    "        self.linear_self = nn.Linear(in_channels, out_channels)\n",
    "        \n",
    "    def forward(self, x, edge_index):\n",
    "        \"\"\"\n",
    "        Convolution을 위해서는 2가지가 필수적임.\n",
    "        x has shape [N, in_channels] # feature matrix\n",
    "        edge_index has shape [2, E] ==> connectivity ==> 2: (u, v)\n",
    "        \n",
    "        \"\"\"\n",
    "        # Add self-loops to the adjacency matrix.\n",
    "        # A -> \\tilde{A}\n",
    "        # pyg_utils.add_self_loops(edge_index, num_nodes = x.size(0))  \n",
    "        # neighbor 정보뿐만 아니라, 내 정보까지 add해야하므로 self-loops 추가! \n",
    "        \n",
    "        # 지울수도 있다 !\n",
    "        edge_index, _ = pyg_utils.remove_self_loops(edge_index)\n",
    "\n",
    "        # Transform node feature matrix.\n",
    "        self_x = self.lin_self(x) # B\n",
    "        x = self.lin(x) # W    \n",
    "        \n",
    "        # self_x: skip connection #compute message for all the nodes\n",
    "        return self_x + self.propagate(edge_index, \n",
    "                                    size=(x.size(0), x.size(0)), x=x)\n",
    "    \n",
    "    \n",
    "    def message(self, x_i, x_j, edge_index, size):\n",
    "        # Compute messages\n",
    "        # x_i is self-embedding\n",
    "        # x_j has shape [E, out_channels]\n",
    "\n",
    "        row, col = edge_index\n",
    "        deg = pyg_utils.degree(row, size[0], dtype=x_j.dtype)\n",
    "        deg_inv_sqrt = deg.pow(-0.5)\n",
    "        norm = deg_inv_sqrt[row] * deg_inv_sqrt[col]\n",
    "\n",
    "        return x_j\n",
    "    \n",
    "    \n",
    "    def update(self, aggr_out):\n",
    "        # aggr_out has shape [N, out_channels]\n",
    "        F.normalize(aggr_out, p=2, dim=-1) # dim: 상황에 따라 알맞게 조정할 \n",
    "        return aggr_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "950ebfc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(dataset, task, writer):\n",
    "    if task == 'graph':\n",
    "        data_size = len(dataset)\n",
    "        loader = DataLoader(dataset[:int(data_size * 0.8)], batch_size=64, shuffle=True)\n",
    "        test_loader = DataLoader(dataset[int(data_size * 0.8):], batch_size=64, shuffle=True)\n",
    "    else:\n",
    "        test_loader = loader = DataLoader(dataset, batch_size=64, shuffle=True)\n",
    "\n",
    "    # build model\n",
    "    model = GNNStack(max(dataset.num_node_features, 1), 32, dataset.num_classes, task=task)\n",
    "    opt = optim.Adam(model.parameters(), lr=0.01)\n",
    "    \n",
    "    # train\n",
    "    for epoch in range(200):\n",
    "        total_loss = 0\n",
    "        model.train()\n",
    "        for batch in loader:\n",
    "            #print(batch.train_mask, '----')\n",
    "            opt.zero_grad()\n",
    "            embedding, pred = model(batch)\n",
    "            label = batch.y\n",
    "            if task == 'node':\n",
    "                pred = pred[batch.train_mask]\n",
    "                label = label[batch.train_mask]\n",
    "            loss = model.loss(pred, label)\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "            total_loss += loss.item() * batch.num_graphs\n",
    "        total_loss /= len(loader.dataset)\n",
    "        writer.add_scalar(\"loss\", total_loss, epoch)\n",
    "\n",
    "        if epoch % 10 == 0:\n",
    "            test_acc = test(test_loader, model)\n",
    "            print(\"Epoch {}. Loss: {:.4f}. Test accuracy: {:.4f}\".format(\n",
    "                epoch, total_loss, test_acc))\n",
    "            writer.add_scalar(\"test accuracy\", test_acc, epoch)\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "224caa57",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(loader, model, is_validation=False):\n",
    "    model.eval()\n",
    "\n",
    "    correct = 0\n",
    "    for data in loader:\n",
    "        with torch.no_grad():\n",
    "            emb, pred = model(data)\n",
    "            pred = pred.argmax(dim=1)\n",
    "            label = data.y\n",
    "\n",
    "        if model.task == 'node':\n",
    "            mask = data.val_mask if is_validation else data.test_mask\n",
    "            # node classification: only evaluate on nodes in test set\n",
    "            pred = pred[mask]\n",
    "            label = data.y[mask]\n",
    "            \n",
    "        correct += pred.eq(label).sum().item()\n",
    "    \n",
    "    if model.task == 'graph':\n",
    "        total = len(loader.dataset) \n",
    "    else:\n",
    "        total = 0\n",
    "        for data in loader.dataset:\n",
    "            total += torch.sum(data.test_mask).item()\n",
    "    return correct / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8da18313",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Python\n"
     ]
    }
   ],
   "source": [
    "get_ipython().system_raw(\n",
    "    'tensorboard --logdir {} --host 0.0.0.0 --port 6006 &'\n",
    "    .format(\"./log\")\n",
    ")\n",
    "get_ipython().system_raw('./ngrok http 6006 &')\n",
    "!curl -s http://localhost:4040/api/tunnels | python3 -c \\\n",
    "    \"import sys, json; print(json.load(sys.stdin)['tunnels'][0]['public_url'])\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f13546eb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "writer = SummaryWriter(\"./log/\" + datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\n",
    "\n",
    "dataset = TUDataset(root='/tmp/ENZYMES', name='ENZYMES')\n",
    "dataset = dataset.shuffle()\n",
    "task = 'graph'\n",
    "\n",
    "\n",
    "model = train(dataset, task, writer)\n",
    "writer = SummaryWriter(\"./log/\" + datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\n",
    "\n",
    "dataset = Planetoid(root='/tmp/cora', name='cora')\n",
    "task = 'node'\n",
    "\n",
    "model = train(dataset, task, writer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0823201f",
   "metadata": {},
   "outputs": [],
   "source": [
    "color_list = [\"red\", \"orange\", \"green\", \"blue\", \"purple\", \"brown\"]\n",
    "\n",
    "loader = DataLoader(dataset, batch_size=64, shuffle=True)\n",
    "embs = []\n",
    "colors = []\n",
    "for batch in loader:\n",
    "    emb, pred = model(batch)\n",
    "    embs.append(emb)\n",
    "    colors += [color_list[y] for y in batch.y]\n",
    "embs = torch.cat(embs, dim=0)\n",
    "\n",
    "xs, ys = zip(*TSNE().fit_transform(embs.detach().numpy()))\n",
    "plt.scatter(xs, ys, color=colors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11f37cea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93562a4a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "389e1526",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c169a2a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
