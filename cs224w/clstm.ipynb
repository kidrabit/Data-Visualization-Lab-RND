{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3e4e33df",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4a2a6ad8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1220029700</th>\n",
       "      <th>1220032100</th>\n",
       "      <th>1220033000</th>\n",
       "      <th>1220024600</th>\n",
       "      <th>1220028200</th>\n",
       "      <th>1210030000</th>\n",
       "      <th>1220025300</th>\n",
       "      <th>1220029200</th>\n",
       "      <th>1220034000</th>\n",
       "      <th>1220034300</th>\n",
       "      <th>...</th>\n",
       "      <th>1220026300</th>\n",
       "      <th>1220033400</th>\n",
       "      <th>1220025200</th>\n",
       "      <th>1220033100</th>\n",
       "      <th>1220031800</th>\n",
       "      <th>1220033300</th>\n",
       "      <th>1220031000</th>\n",
       "      <th>1220034600</th>\n",
       "      <th>1220032800</th>\n",
       "      <th>1220026800</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>25.19</td>\n",
       "      <td>29.65</td>\n",
       "      <td>28.08</td>\n",
       "      <td>25.14</td>\n",
       "      <td>20.30</td>\n",
       "      <td>36.73</td>\n",
       "      <td>28.74</td>\n",
       "      <td>31.59</td>\n",
       "      <td>32.78</td>\n",
       "      <td>27.48</td>\n",
       "      <td>...</td>\n",
       "      <td>31.11</td>\n",
       "      <td>31.87</td>\n",
       "      <td>25.61</td>\n",
       "      <td>35.04</td>\n",
       "      <td>36.00</td>\n",
       "      <td>31.82</td>\n",
       "      <td>25.18</td>\n",
       "      <td>24.80</td>\n",
       "      <td>17.00</td>\n",
       "      <td>29.09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>26.96</td>\n",
       "      <td>24.27</td>\n",
       "      <td>26.63</td>\n",
       "      <td>26.65</td>\n",
       "      <td>20.49</td>\n",
       "      <td>36.74</td>\n",
       "      <td>26.28</td>\n",
       "      <td>28.80</td>\n",
       "      <td>30.56</td>\n",
       "      <td>26.95</td>\n",
       "      <td>...</td>\n",
       "      <td>30.26</td>\n",
       "      <td>34.42</td>\n",
       "      <td>26.18</td>\n",
       "      <td>34.95</td>\n",
       "      <td>36.61</td>\n",
       "      <td>30.24</td>\n",
       "      <td>25.07</td>\n",
       "      <td>21.33</td>\n",
       "      <td>26.80</td>\n",
       "      <td>28.95</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>26.20</td>\n",
       "      <td>28.07</td>\n",
       "      <td>27.76</td>\n",
       "      <td>24.94</td>\n",
       "      <td>22.35</td>\n",
       "      <td>32.30</td>\n",
       "      <td>27.88</td>\n",
       "      <td>32.74</td>\n",
       "      <td>32.22</td>\n",
       "      <td>35.34</td>\n",
       "      <td>...</td>\n",
       "      <td>33.27</td>\n",
       "      <td>24.70</td>\n",
       "      <td>24.06</td>\n",
       "      <td>31.08</td>\n",
       "      <td>37.30</td>\n",
       "      <td>27.94</td>\n",
       "      <td>24.16</td>\n",
       "      <td>30.84</td>\n",
       "      <td>24.68</td>\n",
       "      <td>31.14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>26.72</td>\n",
       "      <td>23.98</td>\n",
       "      <td>26.34</td>\n",
       "      <td>27.54</td>\n",
       "      <td>24.68</td>\n",
       "      <td>40.67</td>\n",
       "      <td>29.85</td>\n",
       "      <td>27.55</td>\n",
       "      <td>28.83</td>\n",
       "      <td>31.98</td>\n",
       "      <td>...</td>\n",
       "      <td>32.80</td>\n",
       "      <td>30.08</td>\n",
       "      <td>26.93</td>\n",
       "      <td>35.77</td>\n",
       "      <td>34.74</td>\n",
       "      <td>27.34</td>\n",
       "      <td>25.15</td>\n",
       "      <td>27.20</td>\n",
       "      <td>24.02</td>\n",
       "      <td>33.40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>27.21</td>\n",
       "      <td>24.11</td>\n",
       "      <td>26.63</td>\n",
       "      <td>24.71</td>\n",
       "      <td>19.57</td>\n",
       "      <td>32.51</td>\n",
       "      <td>26.93</td>\n",
       "      <td>29.77</td>\n",
       "      <td>30.60</td>\n",
       "      <td>28.39</td>\n",
       "      <td>...</td>\n",
       "      <td>30.48</td>\n",
       "      <td>26.82</td>\n",
       "      <td>26.36</td>\n",
       "      <td>32.22</td>\n",
       "      <td>38.51</td>\n",
       "      <td>28.47</td>\n",
       "      <td>23.82</td>\n",
       "      <td>30.69</td>\n",
       "      <td>23.19</td>\n",
       "      <td>33.97</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 58 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   1220029700  1220032100  1220033000  1220024600  1220028200  1210030000  \\\n",
       "0       25.19       29.65       28.08       25.14       20.30       36.73   \n",
       "1       26.96       24.27       26.63       26.65       20.49       36.74   \n",
       "2       26.20       28.07       27.76       24.94       22.35       32.30   \n",
       "3       26.72       23.98       26.34       27.54       24.68       40.67   \n",
       "4       27.21       24.11       26.63       24.71       19.57       32.51   \n",
       "\n",
       "   1220025300  1220029200  1220034000  1220034300  ...  1220026300  \\\n",
       "0       28.74       31.59       32.78       27.48  ...       31.11   \n",
       "1       26.28       28.80       30.56       26.95  ...       30.26   \n",
       "2       27.88       32.74       32.22       35.34  ...       33.27   \n",
       "3       29.85       27.55       28.83       31.98  ...       32.80   \n",
       "4       26.93       29.77       30.60       28.39  ...       30.48   \n",
       "\n",
       "   1220033400  1220025200  1220033100  1220031800  1220033300  1220031000  \\\n",
       "0       31.87       25.61       35.04       36.00       31.82       25.18   \n",
       "1       34.42       26.18       34.95       36.61       30.24       25.07   \n",
       "2       24.70       24.06       31.08       37.30       27.94       24.16   \n",
       "3       30.08       26.93       35.77       34.74       27.34       25.15   \n",
       "4       26.82       26.36       32.22       38.51       28.47       23.82   \n",
       "\n",
       "   1220034600  1220032800  1220026800  \n",
       "0       24.80       17.00       29.09  \n",
       "1       21.33       26.80       28.95  \n",
       "2       30.84       24.68       31.14  \n",
       "3       27.20       24.02       33.40  \n",
       "4       30.69       23.19       33.97  \n",
       "\n",
       "[5 rows x 58 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('gang_csv.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "487a3b24",
   "metadata": {},
   "outputs": [],
   "source": [
    "adj_matrix = np.load('adj_matrix.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "aac2e177",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./edges_index.pickle', 'rb') as fr:\n",
    "    edge_index = pickle.load(fr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ecefa727",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from copy import deepcopy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "fd29ff05",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM(nn.Module):\n",
    "    def __init__(self, num_series, hidden):\n",
    "        '''\n",
    "        LSTM model with output layer to generate predictions.\n",
    "        Args:\n",
    "          num_series: number of input time series.\n",
    "          hidden: number of hidden units.\n",
    "        '''\n",
    "        super(LSTM, self).__init__()\n",
    "        self.p = num_series\n",
    "        self.hidden = hidden\n",
    "\n",
    "        # Set up network.\n",
    "        self.lstm = nn.LSTM(num_series, hidden, batch_first=True)\n",
    "        self.lstm.flatten_parameters()\n",
    "        self.linear = nn.Conv1d(hidden, 1, 1)\n",
    "\n",
    "    def init_hidden(self, batch):\n",
    "        '''Initialize hidden states for LSTM cell.'''\n",
    "        device = self.lstm.weight_ih_l0.device\n",
    "        return (torch.zeros(1, batch, self.hidden, device=device),\n",
    "                torch.zeros(1, batch, self.hidden, device=device))\n",
    "\n",
    "    def forward(self, X, hidden=None):\n",
    "        # Set up hidden state.\n",
    "        if hidden is None:\n",
    "            hidden = self.init_hidden(X.shape[0])\n",
    "\n",
    "        # Apply LSTM.\n",
    "        X, hidden = self.lstm(X, hidden)\n",
    "\n",
    "        # Calculate predictions using output layer.\n",
    "        X = X.transpose(2, 1)\n",
    "        X = self.linear(X)\n",
    "        return X.transpose(2, 1), hidden\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9684778f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class cLSTM(nn.Module):\n",
    "    \n",
    "    def __init__(self, num_series, hidden):\n",
    "        super(cLSTM, self).__init__()\n",
    "        self.p = num_series\n",
    "        self.hidden = hidden\n",
    "\n",
    "        # Set up networks.\n",
    "        self.networks = nn.ModuleList([\n",
    "            LSTM(num_series, hidden) for _ in range(num_series)])\n",
    "    \n",
    "    def forward(self, X, hidden=None):\n",
    "        '''\n",
    "        Perform forward pass.\n",
    "        Args:\n",
    "          X: torch tensor of shape (batch, T, p).\n",
    "          hidden: hidden states for LSTM cell.\n",
    "        '''\n",
    "        if hidden is None:\n",
    "            hidden = [None for _ in range(self.p)]\n",
    "        pred = [self.networks[i](X, hidden[i])\n",
    "                for i in range(self.p)]\n",
    "        pred, hidden = zip(*pred)\n",
    "        pred = torch.cat(pred, dim=2)\n",
    "        return pred, hidden\n",
    "\n",
    "    def GC(self, threshold=True):\n",
    "        '''\n",
    "        Extract learned Granger causality.\n",
    "        Args:\n",
    "          threshold: return norm of weights, or whether norm is nonzero.\n",
    "        Returns:\n",
    "          GC: (p x p) matrix. Entry (i, j) indicates whether variable j is\n",
    "            Granger causal of variable i.\n",
    "        '''\n",
    "        GC = [torch.norm(net.lstm.weight_ih_l0, dim=0)\n",
    "              for net in self.networks]\n",
    "        GC = torch.stack(GC)\n",
    "        if threshold:\n",
    "            return (GC > 0).int()\n",
    "        else:\n",
    "            return GC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "867f5007",
   "metadata": {},
   "outputs": [],
   "source": [
    "class cLSTMSparse(nn.Module):\n",
    "    def __init__(self, num_series, sparsity, hidden):\n",
    "        '''\n",
    "        cLSTM model that only uses specified interactions.\n",
    "        Args:\n",
    "          num_series: dimensionality of multivariate time series.\n",
    "          sparsity: torch byte tensor indicating Granger causality, with size\n",
    "            (num_series, num_series).\n",
    "          hidden: number of units in LSTM cell.\n",
    "        '''\n",
    "        super(cLSTMSparse, self).__init__()\n",
    "        self.p = num_series\n",
    "        self.hidden = hidden\n",
    "        self.sparsity = sparsity\n",
    "\n",
    "        # Set up networks.\n",
    "        self.networks = nn.ModuleList([\n",
    "            LSTM(int(torch.sum(sparsity[i].int())), hidden)\n",
    "            for i in range(num_series)])\n",
    "\n",
    "    def forward(self, X, hidden=None):\n",
    "        '''\n",
    "        Perform forward pass.\n",
    "        Args:\n",
    "          X: torch tensor of shape (batch, T, p).\n",
    "          hidden: hidden states for LSTM cell.\n",
    "        '''\n",
    "        if hidden is None:\n",
    "            hidden = [None for _ in range(self.p)]\n",
    "        pred = [self.networks[i](X[:, :, self.sparsity[i]], hidden[i])\n",
    "                for i in range(self.p)]\n",
    "        pred, hidden = zip(*pred)\n",
    "        pred = torch.cat(pred, dim=2)\n",
    "        return pred, hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "63532643",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prox_update(network, lam, lr):\n",
    "    '''Perform in place proximal update on first layer weight matrix.'''\n",
    "    W = network.lstm.weight_ih_l0\n",
    "    norm = torch.norm(W, dim=0, keepdim=True)\n",
    "    W.data = ((W / torch.clamp(norm, min=(lam * lr)))\n",
    "              * torch.clamp(norm - (lr * lam), min=0.0))\n",
    "    network.lstm.flatten_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6e3815c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def regularize(network, lam):\n",
    "    '''Calculate regularization term for first layer weight matrix.'''\n",
    "    W = network.lstm.weight_ih_l0\n",
    "    return lam * torch.sum(torch.norm(W, dim=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ed0714be",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ridge_regularize(network, lam):\n",
    "    '''Apply ridge penalty at linear layer and hidden-hidden weights.'''\n",
    "    return lam * (\n",
    "        torch.sum(network.linear.weight ** 2) +\n",
    "        torch.sum(network.lstm.weight_hh_l0 ** 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ba950915",
   "metadata": {},
   "outputs": [],
   "source": [
    "def restore_parameters(model, best_model):\n",
    "    '''Move parameter values from best_model to model.'''\n",
    "    for params, best_params in zip(model.parameters(), best_model.parameters()):\n",
    "        params.data = best_params\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "296d5762",
   "metadata": {},
   "outputs": [],
   "source": [
    "def arrange_input(data, context):\n",
    "    '''\n",
    "    Arrange a single time series into overlapping short sequences.\n",
    "    Args:\n",
    "      data: time series of shape (T, dim).\n",
    "      context: length of short sequences.\n",
    "    '''\n",
    "    assert context >= 1 and isinstance(context, int)\n",
    "    input = torch.zeros(len(data) - context, context, data.shape[1],\n",
    "                        dtype=torch.float32, device=data.device)\n",
    "    target = torch.zeros(len(data) - context, context, data.shape[1],\n",
    "                         dtype=torch.float32, device=data.device)\n",
    "    for i in range(context):\n",
    "        start = i\n",
    "        end = len(data) - context + i\n",
    "        input[:, i, :] = data[start:end]\n",
    "        target[:, i, :] = data[start+1:end+1]\n",
    "    return input.detach(), target.detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2479a477",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model_gista(clstm, X, context, lam, lam_ridge, lr, max_iter,\n",
    "                      check_every=50, r=0.8, lr_min=1e-8, sigma=0.5,\n",
    "                      monotone=False, m=10, lr_decay=0.5,\n",
    "                      begin_line_search=True, switch_tol=1e-3, verbose=1):\n",
    "    '''\n",
    "    Train cLSTM model with GISTA.\n",
    "    Args:\n",
    "      clstm: clstm model.\n",
    "      X: tensor of data, shape (batch, T, p).\n",
    "      context: length for short overlapping subsequences.\n",
    "      lam: parameter for nonsmooth regularization.\n",
    "      lam_ridge: parameter for ridge regularization on output layer.\n",
    "      lr: learning rate.\n",
    "      max_iter: max number of GISTA iterations.\n",
    "      check_every: how frequently to record loss.\n",
    "      r: for line search.\n",
    "      lr_min: for line search.\n",
    "      sigma: for line search.\n",
    "      monotone: for line search.\n",
    "      m: for line search.\n",
    "      lr_decay: for adjusting initial learning rate of line search.\n",
    "      begin_line_search: whether to begin with line search.\n",
    "      switch_tol: tolerance for switching to line search.\n",
    "      verbose: level of verbosity (0, 1, 2).\n",
    "    '''\n",
    "    p = clstm.p\n",
    "    clstm_copy = deepcopy(clstm)\n",
    "    loss_fn = nn.MSELoss(reduction='mean')\n",
    "    lr_list = [lr for _ in range(p)]\n",
    "\n",
    "    # Set up data.\n",
    "    X, Y = zip(*[arrange_input(x, context) for x in X])\n",
    "    X = torch.cat(X, dim=0)\n",
    "    Y = torch.cat(Y, dim=0)\n",
    "\n",
    "    # Calculate full loss.\n",
    "    mse_list = []\n",
    "    smooth_list = []\n",
    "    loss_list = []\n",
    "    for i in range(p):\n",
    "        net = clstm.networks[i]\n",
    "        pred, _ = net(X)\n",
    "        mse = loss_fn(pred[:, :, 0], Y[:, :, i])\n",
    "        ridge = ridge_regularize(net, lam_ridge)\n",
    "        smooth = mse + ridge\n",
    "        mse_list.append(mse)\n",
    "        smooth_list.append(smooth)\n",
    "        with torch.no_grad():\n",
    "            nonsmooth = regularize(net, lam)\n",
    "            loss = smooth + nonsmooth\n",
    "            loss_list.append(loss)\n",
    "\n",
    "    # Set up lists for loss and mse.\n",
    "    with torch.no_grad():\n",
    "        loss_mean = sum(loss_list) / p\n",
    "        mse_mean = sum(mse_list) / p\n",
    "    train_loss_list = [loss_mean]\n",
    "    train_mse_list = [mse_mean]\n",
    "\n",
    "    # For switching to line search.\n",
    "    line_search = begin_line_search\n",
    "\n",
    "    # For line search criterion.\n",
    "    done = [False for _ in range(p)]\n",
    "    assert 0 < sigma <= 1\n",
    "    assert m > 0\n",
    "    if not monotone:\n",
    "        last_losses = [[loss_list[i]] for i in range(p)]\n",
    "\n",
    "    for it in range(max_iter):\n",
    "        # Backpropagate errors.\n",
    "        sum([smooth_list[i] for i in range(p) if not done[i]]).backward()\n",
    "\n",
    "        # For next iteration.\n",
    "        new_mse_list = []\n",
    "        new_smooth_list = []\n",
    "        new_loss_list = []\n",
    "\n",
    "        # Perform GISTA step for each network.\n",
    "        for i in range(p):\n",
    "            # Skip if network converged.\n",
    "            if done[i]:\n",
    "                new_mse_list.append(mse_list[i])\n",
    "                new_smooth_list.append(smooth_list[i])\n",
    "                new_loss_list.append(loss_list[i])\n",
    "                continue\n",
    "\n",
    "            # Prepare for line search.\n",
    "            step = False\n",
    "            lr_it = lr_list[i]\n",
    "            net = clstm.networks[i]\n",
    "            net_copy = clstm_copy.networks[i]\n",
    "\n",
    "            while not step:\n",
    "                # Perform tentative ISTA step.\n",
    "                for param, temp_param in zip(net.parameters(),\n",
    "                                             net_copy.parameters()):\n",
    "                    temp_param.data = param - lr_it * param.grad\n",
    "\n",
    "                # Proximal update.\n",
    "                prox_update(net_copy, lam, lr_it)\n",
    "\n",
    "                # Check line search criterion.\n",
    "                pred, _ = net_copy(X)\n",
    "                mse = loss_fn(pred[:, :, 0], Y[:, :, i])\n",
    "                ridge = ridge_regularize(net_copy, lam_ridge)\n",
    "                smooth = mse + ridge\n",
    "                with torch.no_grad():\n",
    "                    nonsmooth = regularize(net_copy, lam)\n",
    "                    loss = smooth + nonsmooth\n",
    "                    tol = (0.5 * sigma / lr_it) * sum(\n",
    "                        [torch.sum((param - temp_param) ** 2)\n",
    "                         for param, temp_param in\n",
    "                         zip(net.parameters(), net_copy.parameters())])\n",
    "\n",
    "                comp = loss_list[i] if monotone else max(last_losses[i])\n",
    "                if not line_search or (comp - loss) > tol:\n",
    "                    step = True\n",
    "                    if verbose > 1:\n",
    "                        print('Taking step, network i = %d, lr = %f'\n",
    "                              % (i, lr_it))\n",
    "                        print('Gap = %f, tol = %f' % (comp - loss, tol))\n",
    "\n",
    "                    # For next iteration.\n",
    "                    new_mse_list.append(mse)\n",
    "                    new_smooth_list.append(smooth)\n",
    "                    new_loss_list.append(loss)\n",
    "\n",
    "                    # Adjust initial learning rate.\n",
    "                    lr_list[i] = (\n",
    "                        (lr_list[i] ** (1 - lr_decay)) * (lr_it ** lr_decay))\n",
    "\n",
    "                    if not monotone:\n",
    "                        if len(last_losses[i]) == m:\n",
    "                            last_losses[i].pop(0)\n",
    "                        last_losses[i].append(loss)\n",
    "                else:\n",
    "                    # Reduce learning rate.\n",
    "                    lr_it *= r\n",
    "                    if lr_it < lr_min:\n",
    "                        done[i] = True\n",
    "                        new_mse_list.append(mse_list[i])\n",
    "                        new_smooth_list.append(smooth_list[i])\n",
    "                        new_loss_list.append(loss_list[i])\n",
    "                        if verbose > 0:\n",
    "                            print('Network %d converged' % (i + 1))\n",
    "                        break\n",
    "\n",
    "            # Clean up.\n",
    "            net.zero_grad()\n",
    "\n",
    "            if step:\n",
    "                # Swap network parameters.\n",
    "                clstm.networks[i], clstm_copy.networks[i] = net_copy, net\n",
    "\n",
    "        # For next iteration.\n",
    "        mse_list = new_mse_list\n",
    "        smooth_list = new_smooth_list\n",
    "        loss_list = new_loss_list\n",
    "\n",
    "        # Check if all networks have converged.\n",
    "        if sum(done) == p:\n",
    "            if verbose > 0:\n",
    "                print('Done at iteration = %d' % (it + 1))\n",
    "            break\n",
    "\n",
    "        # Check progress\n",
    "        if (it + 1) % check_every == 0:\n",
    "            with torch.no_grad():\n",
    "                loss_mean = sum(loss_list) / p\n",
    "                mse_mean = sum(mse_list) / p\n",
    "                ridge_mean = (sum(smooth_list) - sum(mse_list)) / p\n",
    "                nonsmooth_mean = (sum(loss_list) - sum(smooth_list)) / p\n",
    "\n",
    "            train_loss_list.append(loss_mean)\n",
    "            train_mse_list.append(mse_mean)\n",
    "\n",
    "            if verbose > 0:\n",
    "                print(('-' * 10 + 'Iter = %d' + '-' * 10) % (it + 1))\n",
    "                print('Total loss = %f' % loss_mean)\n",
    "                print('MSE = %f, Ridge = %f, Nonsmooth = %f'\n",
    "                      % (mse_mean, ridge_mean, nonsmooth_mean))\n",
    "                print('Variable usage = %.2f%%'\n",
    "                      % (100 * torch.mean(clstm.GC().float())))\n",
    "\n",
    "            # Check whether loss has increased.\n",
    "            if not line_search:\n",
    "                if train_loss_list[-2] - train_loss_list[-1] < switch_tol:\n",
    "                    line_search = True\n",
    "                    if verbose > 0:\n",
    "                        print('Switching to line search')\n",
    "\n",
    "    return train_loss_list, train_mse_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "11d79b44",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model_adam(clstm, X, context, lr, max_iter, lam=0, lam_ridge=0,\n",
    "                     lookback=5, check_every=50, verbose=1):\n",
    "    '''Train model with Adam.'''\n",
    "    p = X.shape[-1]\n",
    "    loss_fn = nn.MSELoss(reduction='mean')\n",
    "    optimizer = torch.optim.Adam(clstm.parameters(), lr=lr)\n",
    "    train_loss_list = []\n",
    "\n",
    "    # Set up data.\n",
    "    X, Y = zip(*[arrange_input(x, context) for x in X])\n",
    "    X = torch.cat(X, dim=0)\n",
    "    Y = torch.cat(Y, dim=0)\n",
    "\n",
    "    # For early stopping.\n",
    "    best_it = None\n",
    "    best_loss = np.inf\n",
    "    best_model = None\n",
    "\n",
    "    for it in range(max_iter):\n",
    "        # Calculate loss.\n",
    "        pred = [clstm.networks[i](X)[0] for i in range(p)]\n",
    "        loss = sum([loss_fn(pred[i][:, :, 0], Y[:, :, i]) for i in range(p)])\n",
    "\n",
    "        # Add penalty term.\n",
    "        if lam > 0:\n",
    "            loss = loss + sum([regularize(net, lam) for net in clstm.networks])\n",
    "\n",
    "        if lam_ridge > 0:\n",
    "            loss = loss + sum([ridge_regularize(net, lam_ridge)\n",
    "                               for net in clstm.networks])\n",
    "\n",
    "        # Take gradient step.\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        clstm.zero_grad()\n",
    "\n",
    "        # Check progress.\n",
    "        if (it + 1) % check_every == 0:\n",
    "            mean_loss = loss / p\n",
    "            train_loss_list.append(mean_loss.detach())\n",
    "\n",
    "            if verbose > 0:\n",
    "                print(('-' * 10 + 'Iter = %d' + '-' * 10) % (it + 1))\n",
    "                print('Loss = %f' % mean_loss)\n",
    "\n",
    "            # Check for early stopping.\n",
    "            if mean_loss < best_loss:\n",
    "                best_loss = mean_loss\n",
    "                best_it = it\n",
    "                best_model = deepcopy(clstm)\n",
    "            elif (it - best_it) == lookback * check_every:\n",
    "                if verbose:\n",
    "                    print('Stopping early')\n",
    "                break\n",
    "\n",
    "    # Restore best model.\n",
    "    restore_parameters(clstm, best_model)\n",
    "\n",
    "    return train_loss_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "8467c381",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model_ista(clstm, X, context, lr, max_iter, lam=0, lam_ridge=0,\n",
    "                     lookback=5, check_every=50, verbose=1):\n",
    "    '''Train model with Adam.'''\n",
    "    p = X.shape[-1]\n",
    "    loss_fn = nn.MSELoss(reduction='mean')\n",
    "    train_loss_list = []\n",
    "\n",
    "    # Set up data.\n",
    "    X, Y = zip(*[arrange_input(x, context) for x in X])\n",
    "    X = torch.cat(X, dim=0)\n",
    "    Y = torch.cat(Y, dim=0)\n",
    "\n",
    "    # For early stopping.\n",
    "    best_it = None\n",
    "    best_loss = np.inf\n",
    "    best_model = None\n",
    "\n",
    "    # Calculate smooth error.\n",
    "    pred = [clstm.networks[i](X)[0] for i in range(p)]\n",
    "    loss = sum([loss_fn(pred[i][:, :, 0], Y[:, :, i]) for i in range(p)])\n",
    "    ridge = sum([ridge_regularize(net, lam_ridge) for net in clstm.networks])\n",
    "    smooth = loss + ridge\n",
    "\n",
    "    for it in range(max_iter):\n",
    "        # Take gradient step.\n",
    "        smooth.backward()\n",
    "        for param in clstm.parameters():\n",
    "            param.data -= lr * param.grad\n",
    "\n",
    "        # Take prox step.\n",
    "        if lam > 0:\n",
    "            for net in clstm.networks:\n",
    "                prox_update(net, lam, lr)\n",
    "\n",
    "        clstm.zero_grad()\n",
    "\n",
    "        # Calculate loss for next iteration.\n",
    "        pred = [clstm.networks[i](X)[0] for i in range(p)]\n",
    "        loss = sum([loss_fn(pred[i][:, :, 0], Y[:, :, i]) for i in range(p)])\n",
    "        ridge = sum([ridge_regularize(net, lam_ridge)\n",
    "                     for net in clstm.networks])\n",
    "        smooth = loss + ridge\n",
    "\n",
    "        # Check progress.\n",
    "        if (it + 1) % check_every == 0:\n",
    "            # Add nonsmooth penalty.\n",
    "            nonsmooth = sum([regularize(net, lam) for net in clstm.networks])\n",
    "            mean_loss = (smooth + nonsmooth) / p\n",
    "            train_loss_list.append(mean_loss.detach())\n",
    "\n",
    "            if verbose > 0:\n",
    "                print(('-' * 10 + 'Iter = %d' + '-' * 10) % (it + 1))\n",
    "                print('Loss = %f' % mean_loss)\n",
    "                print('Variable usage = %.2f%%'\n",
    "                      % (100 * torch.mean(clstm.GC().float())))\n",
    "\n",
    "            # Check for early stopping.\n",
    "            if mean_loss < best_loss:\n",
    "                best_loss = mean_loss\n",
    "                best_it = it\n",
    "                best_model = deepcopy(clstm)\n",
    "            elif (it - best_it) == lookback * check_every:\n",
    "                if verbose:\n",
    "                    print('Stopping early')\n",
    "                break\n",
    "\n",
    "    # Restore best model.\n",
    "    restore_parameters(clstm, best_model)\n",
    "\n",
    "    return train_loss_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "96be8176",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_unregularized(clstm, X, context, lr, max_iter, lookback=5,\n",
    "                        check_every=50, verbose=1):\n",
    "    '''Train model with Adam.'''\n",
    "    p = X.shape[-1]\n",
    "    loss_fn = nn.MSELoss(reduction='mean')\n",
    "    optimizer = torch.optim.Adam(clstm.parameters(), lr=lr)\n",
    "    train_loss_list = []\n",
    "\n",
    "    # Set up data.\n",
    "    X, Y = zip(*[arrange_input(x, context) for x in X])\n",
    "    X = torch.cat(X, dim=0)\n",
    "    Y = torch.cat(Y, dim=0)\n",
    "\n",
    "    # For early stopping.\n",
    "    best_it = None\n",
    "    best_loss = np.inf\n",
    "    best_model = None\n",
    "\n",
    "    for it in range(max_iter):\n",
    "        # Calculate loss.\n",
    "        pred, hidden = clstm(X)\n",
    "        loss = sum([loss_fn(pred[:, :, i], Y[:, :, i]) for i in range(p)])\n",
    "\n",
    "        # Take gradient step.\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        clstm.zero_grad()\n",
    "\n",
    "        # Check progress.\n",
    "        if (it + 1) % check_every == 0:\n",
    "            mean_loss = loss / p\n",
    "            train_loss_list.append(mean_loss.detach())\n",
    "\n",
    "            if verbose > 0:\n",
    "                print(('-' * 10 + 'Iter = %d' + '-' * 10) % (it + 1))\n",
    "                print('Loss = %f' % mean_loss)\n",
    "\n",
    "            # Check for early stopping.\n",
    "            if mean_loss < best_loss:\n",
    "                best_loss = mean_loss\n",
    "                best_it = it\n",
    "                best_model = deepcopy(clstm)\n",
    "            elif (it - best_it) == lookback * check_every:\n",
    "                if verbose:\n",
    "                    print('Stopping early')\n",
    "                break\n",
    "\n",
    "    # Restore best model.\n",
    "    restore_parameters(clstm, best_model)\n",
    "\n",
    "    return train_loss_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d538969b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "9ac7d81d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.integrate import odeint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "56c4a8df",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "device = torch.device('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "377db3d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 2.3833,  4.3567,  7.5971, -2.8582, -4.1893, -1.9402,  9.7620,  4.7616,\n",
       "        -4.7465,  2.9787], device='cuda:0')"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "64d95135",
   "metadata": {},
   "outputs": [],
   "source": [
    "GC = adj_matrix\n",
    "X_np = df.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "e0248202",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = torch.tensor(X_np[np.newaxis], dtype=torch.float32, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "62b023a0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "clstm = cLSTM(X.shape[-1], hidden=100).cuda(device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "5f65c75c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\chanyoung\\anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\nn\\modules\\rnn.py:661: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ..\\aten\\src\\ATen\\native\\cudnn\\RNN.cpp:915.)\n",
      "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------Iter = 100----------\n",
      "Loss = 65.181747\n",
      "----------Iter = 200----------\n",
      "Loss = 41.225868\n",
      "----------Iter = 300----------\n",
      "Loss = 30.051872\n",
      "----------Iter = 400----------\n",
      "Loss = 25.131838\n",
      "----------Iter = 500----------\n",
      "Loss = 24.186506\n",
      "----------Iter = 600----------\n",
      "Loss = 23.250666\n",
      "----------Iter = 700----------\n",
      "Loss = 23.382753\n",
      "----------Iter = 800----------\n",
      "Loss = 22.206722\n",
      "----------Iter = 900----------\n",
      "Loss = 23.124399\n",
      "----------Iter = 1000----------\n",
      "Loss = 22.762918\n",
      "----------Iter = 1100----------\n",
      "Loss = 21.270380\n",
      "----------Iter = 1200----------\n",
      "Loss = 20.816652\n",
      "----------Iter = 1300----------\n",
      "Loss = 22.242414\n",
      "----------Iter = 1400----------\n",
      "Loss = 22.193954\n",
      "----------Iter = 1500----------\n",
      "Loss = 19.514540\n",
      "----------Iter = 1600----------\n",
      "Loss = 22.673132\n",
      "----------Iter = 1700----------\n",
      "Loss = 18.854631\n",
      "----------Iter = 1800----------\n",
      "Loss = 21.596512\n",
      "----------Iter = 1900----------\n",
      "Loss = 20.608078\n",
      "----------Iter = 2000----------\n",
      "Loss = 22.501295\n",
      "----------Iter = 2100----------\n",
      "Loss = 20.717356\n",
      "----------Iter = 2200----------\n",
      "Loss = 20.352995\n",
      "Stopping early\n"
     ]
    }
   ],
   "source": [
    "train_loss_list = train_model_adam(\n",
    "    clstm, X, context=10, lam=10.0, lam_ridge=1e-2, lr=1e-3, max_iter=20000,\n",
    "    check_every=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "94a414f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAjgAAAFgCAYAAAC2QAPxAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8QVMy6AAAACXBIWXMAAAsTAAALEwEAmpwYAAA0CUlEQVR4nO3deXiddZ338fc3a9NsTdIkTVLatE260SWloYAssguitDKAK+KI4KOiziPCqPM8o8487ogjgzoiogiiAwxIRZR9F1paulFa2tCNbkmaLlma/XyfP86dEqBL0p6Ts+Tzuq5c55w7933ON7/2Sj/9LffP3B0RERGRZJIS6wJEREREIk0BR0RERJKOAo6IiIgkHQUcERERSToKOCIiIpJ0FHBEREQk6SjgiMiwYmbfMLPbIn2uiMQX031wRORQzOxTwGfc/bSDfO944CfAiYABbwD/FygAfhmclgpkAvv7rnP3HDPbBJQD5e6+q997LgdmAxPcfdNBPvNp4C53V+gQkcNSD46IHK0/A48BpUAJ8CWg2d1/7+457p4DXAhs73sdHOuzEfho3wszmwlkHUtBZpZ2LNeLSPJQwBERzOw4M7vfzBrNrMnMbjnC+aOBCcCv3L0r+HrB3Z8fxMfeCXyy3+srgd8d5jO/A5wO3GJmrX01mpmb2RfMbD2wPjj2UzN708yazWypmZ3e732+ZWZ3Bc8rg+uvNLMtZrbLzP7lKM/NMrM7zGyPma0xsxvMbOsg2kNEIkgBR2SYM7NU4CFgM1AJVAB/PMJlTUAdcJeZLTCz0qP46JeAPDObFtTwYeCuQ53s7v8CPAdcG/QGXdvv2wuAk4DpweuXgRqgELgbuNfMRhymltOAKcA5wL+a2bSjOPebhNtvInAe8InDvIeIRJkCjojMIzwf5np3b3P3jiP1xHh48t5ZwCbgx8AOM3vWzKoH+dl9vTjnAWuBbYMtPvA9d9/t7u1BfXe5e5O797j7jwnPA5pymOu/7e7t7r4CWEF4HtBgz70c+K6773H3rcDNR/mziEgEKOCIyHHAZnfvGcxF7r7V3a9190nAeKCNwwwxHcKdwMeATx3Ftf292f+FmV0XDBPtM7O9QD4w+jDX7+z3fD+Qc6gTD3Nu+TvqeFtNIjK0FHBE5E1g3LFM0HX3N4GfATMGed1mwpON3w/cP5BLjnQ8mG/zz4R7VArcfRSwj/BKr2jaAYzt9/q4KH+eiByGAo6ILCb8j/P3zSzbzEaY2an9vm/Bsf5fBWb2bTOrMrOUYNLxpwnPqxmsq4Cz3b1tAOfWE57jcji5QA/QCKSZ2b8CeUdR12DdA3w9aJsK4NojXSAi0aOAIzLMuXsv8EGgCtgCbCU84bfPe4D2d3yFCE+ofRxoBl4FOgkPNQ32899w9yUDPP2nwKXBSqVDzXF5BPgrsI7wxOkOhma46N8It91Gwu1yH+E2EZEY0I3+RESiwMw+B3zE3d8b61pEhiP14IiIRICZlZnZqcGQ3RTgOuCBWNclMlzprp8iIpGRQXiLignAXsL3Evp5LAsSGc40RCUiIiJJR0NUIiIiknQSYohq9OjRXllZGesyREREJM4sXbp0l7sXv/N4QgScyspKliwZ6CpSERERGS7MbPPBjmuISkRERJKOAo6IiIgkHQUcERERSToKOCIiIpJ0FHBEREQk6SjgiIiISNJRwBEREZGko4AjIiIiSUcBR0RERJKOAo6IiIgknWEdcNydhuYOWjt7Yl2KiIiIRNCwDjjrG1qZ990neGJNfaxLERERkQga1gFnfNFIUlOM9fWtsS5FREREImhYB5zMtFQqi0ayvqEl1qWIiIhIBA3rgANQXZLL+gb14IiIiCQTBZzSHDY37aezpzfWpYiIiEiEDPuAU1WSQ2/I2bRrf6xLERERkQgZ9gGnuiQXQPNwREREksiwDzgTi7NJMbSSSkREJIkM+4AzIj2VcYVaSSUiIpJMhn3AAagqyVUPjoiISBJRwAEml+awcVcb3b2hWJciIiIiEaCAQ3ipeE/I2dzUFutSREREJAIUcOi3kkrDVCIiIklBAQeYVJyDGbqjsYiISJJQwAGyMlIZW5ClgCMiIpIkFHAC1SW5rK/XUnEREZFkoIATqC7JYUNjGz1aSSUiIpLwFHACVSU5dPWG2LJbe1KJiIgkOgWcwOTSvj2pNA9HREQk0SngBCaV5ABQp4AjIiKS8BRwAjmZaVSMytJEYxERkSSggNNPVUmOhqhERESSgAJOP9UlOdQ1tNIb8liXIiIiIsdAAaef6tIcOntCbNvTHutSRERE5Bgo4PRTFexJtU7zcERERBKaAk4/VcFKKs3DERERSWwKOP3kZ6VTmpfJ+gb14IiIiCSyqAYcMxtlZveZ2VozW2Nmp5hZoZk9Zmbrg8eCaNYwWJNLc3UvHBERkQQX7R6cnwJ/c/epwGxgDfA14Al3rwaeCF7HjapgJVVIK6lEREQSVtQCjpnlAWcAvwZw9y533wvMB+4ITrsDWBCtGo5GdUku+7t62b5PK6lEREQSVTR7cCYCjcBvzGyZmd1mZtlAqbvvAAgeSw52sZldY2ZLzGxJY2NjFMt8u+pSTTQWERFJdNEMOGnACcAv3H0O0MYghqPc/VZ3r3X32uLi4mjV+C5VxcGeVPUKOCIiIokqmgFnK7DV3RcFr+8jHHjqzawMIHhsiGINg1aQncHoHK2kEhERSWRRCzjuvhN408ymBIfOAV4DFgJXBseuBB6MVg1Hq7okh3XqwREREUlYaVF+/y8CvzezDGAD8I+EQ9U9ZnYVsAW4LMo1DFp1aQ73v7INd8fMYl2OiIiIDFJUA467LwdqD/Ktc6L5ucequjSX1s4edjZ3UJafFetyREREZJB0J+ODqO7bskHDVCIiIglJAecgqrUnlYiISEJTwDmIopxMCrMzqNNKKhERkYSkgHMIVSU5GqISERFJUAo4h1BdksP6hlbctSeViIhIolHAOYTqkhz2tXfT2NIZ61JERERkkBRwDqG6NBfQRGMREZFEpIBzCG8tFddEYxERkUSjgHMIxbmZ5GelqwdHREQkASngHIKZHZhoLCIiIolFAecwqktzqFPAERERSTgKOIdRVZLL7rYumlq1kkpERCSRKOAchrZsEBERSUwKOIdRXaqAIyIikogUcA5jTN4IcjLTtFRcREQkwSjgHIaZaU8qERGRBKSAcwSTS7VUXEREJNEo4BxBdUkuu1o72dPWFetSREREZIAUcI6gKphoXNeoXhwREZFEoYBzBG/tSaWAIyIikigUcI6gPD+LkRmprG/QSioREZFEoYBzBCkp4ZVU2rJBREQkcSjgDEBVSQ7rdC8cERGRhKGAMwDVJbnUN3eyr7071qWIiIjIACjgDMDkvpVUGqYSERFJCAo4A1BdkgtAnSYai4iIJAQFnAGoKMhiRHqKloqLiIgkCAWcAUhNMSYVa8sGERGRRKGAM0DVWiouIiKSMBRwBqi6NJdte9tp7eyJdSkiIiJyBAo4A1QVbNnwhnpxRERE4p4CzgD17UmlG/6JiIjEPwWcARpXOJKM1BTNwxEREUkACjgDlJaawsTibK2kEhERSQAKOINQXZqrXcVFREQSgALOIFSX5LB1Tzv7u7SSSkREJJ4p4AxCdUkO7rChsS3WpYiIiMhhKOAMQnWw6aaGqUREROKbAs4gjC/KJi3FtCeViIhInFPAGYT01BQmjM5mnQKOiIhIXFPAGaTq0hzqNEQlIiIS1xRwBqm6JJctu/fT0d0b61JERETkEBRwBqm6NIeQVlKJiIjENQWcQaouyQW0kkpERCSeKeAMUuXokaSmmPakEhERiWMKOIOUmZbK+KKRWiouIiISxxRwjkJ1SY6GqEREROKYAs5RqC7JZVPTfrp6QrEuRURERA4iqgHHzDaZ2SozW25mS4JjhWb2mJmtDx4LollDNFSX5tAbcjbu0koqERGReDQUPThnuXuNu9cGr78GPOHu1cATweuEopVUIiIi8S0WQ1TzgTuC53cAC2JQwzGZWJxNiqGJxiIiInEq2gHHgUfNbKmZXRMcK3X3HQDBY8nBLjSza8xsiZktaWxsjHKZgzMiPZVxhSO1VFxERCROpUX5/U919+1mVgI8ZmZrB3qhu98K3ApQW1vr0SrwaFWV5GqISkREJE5FtQfH3bcHjw3AA8A8oN7MygCCx4Zo1hAt1aU5bNzVRnevVlKJiIjEm6gFHDPLNrPcvufA+cCrwELgyuC0K4EHo1VDNFWX5NDd62xu2h/rUkREROQdojlEVQo8YGZ9n3O3u//NzF4G7jGzq4AtwGVRrCFq+lZS1TW0UFWSE+NqREREpL+oBRx33wDMPsjxJuCcaH3uUJlUkg3AuvpWLpgR42JERETkbXQn46M0MiONsQVZrNdKKhERkbijgHMMJpfmsr5eK6lERETijQLOMaguyWHDrjZ6tJJKREQkrijgHIOqkhy6ekK8uac91qWIiIhIPwo4x6C6NNiTSsNUIiIicUUB5xj0LQ/XRGMREZH4ooBzDHIy0yjPH6E9qUREROKMAs4xqirVnlQiIiLxRgHnGFWX5FDX0EooFHf7gYqIiAxbCjjHaHJpDh3dIbZqJZWIiEjcUMA5RlXBnlQaphIREYkfCjjHSCupRERE4o8CzjHKz0qnNC+T9fUKOCIiIvFCAScCqktyqdMQlYiISNxQwImAqpIc1je04q6VVCIiIvFAAScCqktz2N/Vy/Z9HbEuRURERFDAiYjqYCXVOu1JJSIiEhcUcCKgOlhJVaeJxiIiInFBAScCCrIzGJ2TqXvhiIiIxAkFnAipDiYai4iISOwp4ERIdWkOdfVaSSUiIhIPFHAipLokh5bOHuqbO2NdioiIyLCngBMh2pNKREQkfijgREh1abAnlVZSiYiIxJwCToQUZWdQMDJdE41FRETigAJOhJgZ1SW5rNfN/kRERGJOASeCqku1J5WIiEg8UMCJoOqSHPa1d9PYqpVUIiIisaSAE0HVpeGVVNqyQUREJLYUcCKob08qTTQWERGJLQWcCCrOzSRvRJruhSMiIhJjCjgRZGZUl+bqXjgiIiIxNqCAY2bZZpYSPJ9sZhebWXp0S0tM1SU51GmISkREJKYG2oPzLDDCzCqAJ4B/BH4braISWVVJDk1tXTRpJZWIiEjMDDTgmLvvBy4B/tPdPwRMj15ZiWtyad+eVOrFERERiZUBBxwzOwX4OPCX4FhadEpKbAf2pFLAERERiZmBBpx/Ar4OPODuq81sIvBU1KpKYGPyRpCTmUadtmwQERGJmQH1wrj7M8AzAMFk413u/qVoFpaozIyqkhz14IiIiMTQQFdR3W1meWaWDbwGvG5m10e3tMRVrYAjIiISUwMdopru7s3AAuBhYBxwRbSKSnTVpTk0tnSyd39XrEsREREZlgYacNKD+94sAB50925AW2YfQnVJsCeVenFERERiYqAB55fAJiAbeNbMxgPN0Soq0VUFe1Kt0x2NRUREYmKgk4xvBm7ud2izmZ0VnZISX8WoLEZmpGpPKhERkRgZ6CTjfDO7ycyWBF8/JtybIweRkhJeSaUhKhERkdgY6BDV7UALcHnw1Qz8JlpFJYPqklxe295Mb0hTlURERIbaQAPOJHf/prtvCL6+DUyMZmGJ7qypxTS1dbFoY1OsSxERERl2Bhpw2s3stL4XZnYq0B6dkpLDudNKyc5IZeHy7bEuRUREZNgZaMD5X8DPzGyTmW0CbgE+G7WqksCI9FTeN2MMD6/aQWdPb6zLERERGVYGFHDcfYW7zwZmAbPcfQ5w9kCuNbNUM1tmZg8FrwvN7DEzWx88Fhx19XFufk0FzR09PP16Y6xLERERGVYG2oMDgLs3B3c0BvjKAC/7MrCm3+uvAU+4ezXwRPA6KZ06qYjRORkaphIRERligwo472BHPMFsLHARcFu/w/OBO4LndxC+O3JSSktN4QOzynl8TT0tHd2xLkdERGTYOJaAM5D1z/8B3ACE+h0rdfcdAMFjyTHUEPcurimnsyfEI6vrY12KiIjIsHHYgGNmLWbWfJCvFqD8CNd+AGhw96VHU5iZXdN3Y8HGxsSdwzLnuFGMKxzJg8u3xboUERGRYeOwAcfdc9097yBfue5+pG0eTgUuDlZd/RE428zuAurNrAwgeGw4xGff6u617l5bXFw86B8sXpgZ82vKeaFuF40tnbEuR0REZFg4liGqw3L3r7v7WHevBD4CPOnunwAWAlcGp10JPBitGuLF/JpyQg4PrdRkYxERkaEQtYBzGN8HzjOz9cB5weukVlWSy/SyPB7UaioREZEhMSQBx92fdvcPBM+b3P0cd68OHncPRQ2xtmBOOcvf3MvmprZYlyIiIpL0YtGDMyx9cHY5ZqgXR0REZAgo4AyRsvws5lUW8qfl23DXDuMiIiLRpIAzhBbMqWBDYxurtzcf+WQRERE5ago4Q+jCGWNITzXdE0dERCTKFHCG0KiRGbx3cgkLV2ynN6RhKhERkWhRwBliC+aUU9/cyeKNw2LxmIiISEwo4Ayxc6aWkp2RqmEqERGRKFLAGWJZGam87/gxPLxqB509vbEuR0REJCkp4MTAxTXlNHf08MzribuJqIiISDxTwImB06pGU5SdoZv+iYiIRIkCTgykpabwgVllPL6mnpaO7liXIyIiknQUcGLk4poKOntCPLq6PtaliIiIJB0FnBg5YdwojivM4sEVGqYSERGJNAWcGDEz5s+u4Pn1jTS2dMa6HBERkaSigBND82vKCTn8ZaV6cURERCJJASeGqktzmV6Wp2EqERGRCFPAibH5NeUs27KXzU1tsS5FREQkaSjgxNgHZ5cDsFD3xBEREYkYBZwYKx+VxbwJhfxp+TbctcO4iIhIJCjgxIEFNRW80djG6u3NsS5FREQkKSjgxIELZ4whPdVYqMnGIiIiEaGAEwcKsjN47+RiFi7fTiikYSoREZFjpYATJ+bXVLCzuYNFG3fHuhQREZGEp4ATJ86dVsrIjFQWrtgW61JEREQSngJOnMjKSOV9x4/h4VU76ezpjXU5IiIiCU0BJ47MrylnX3s3z7zeGOtSREREEpoCThw5tWo0RdkZ2rpBRETkGCngxJH01BQumlXG46/V09rZE+tyREREEpYCTpyZX1NOZ0+IR1fvjHUpIiIiCUsBJ86cMK6AsQVZ/El7U4mIiBw1BZw4Y2bMrynnhbpdNLZ0xrocERGRhKSAE4fm11TQG3IeXrUj1qWIiIgkJAWcODS5NJdpZXn8ablu+iciInI0FHDi1PyacpZt2cuWpv2xLkVERCThKODEqQ/OLgfQ1g0iIiJHQQEnTlWMymLehEL+tHw77tphXEREZDAUcOLY/Jpy6hpaeW1Hc6xLERERSSgKOHHs/TPKSEsxFuqeOCIiIoOigBPHCrIzeO/kYhau2E4opGEqERGRgVLAiXPz51SwY18HizftjnUpIiIiCUMBJ86dO62EkRmpPKhhKhERkQFTwIlzIzPSOH96KQ+v2kFXTyjW5YiIiCQEBZwEMH9OBfvau3lmXWOsSxEREUkICjgJ4LSq0RRmZ/Cgtm4QEREZEAWcBJCemsJFM8t4fE09rZ09sS5HREQk7ingJIgFc8rp6A7x6OqdsS5FREQk7ingJIgTxhUwtiBLq6lEREQGQAEnQZgZF88u5/m6Xexq7Yx1OSIiInFNASeBzK+poDfk/GXljliXIiIiEteiFnDMbISZLTazFWa22sy+HRwvNLPHzGx98FgQrRqSzZQxuUwdk6vVVCIiIkcQzR6cTuBsd58N1AAXmNnJwNeAJ9y9GngieC0DNL+mgle27GVL0/5YlyIiIhK3ohZwPKw1eJkefDkwH7gjOH4HsCBaNSSji2vKMYObHnsdd23AKSIicjBRnYNjZqlmthxoAB5z90VAqbvvAAgeSw5x7TVmtsTMljQ26g6+fSpGZfFP50zmT8u3c/sLm2JdjoiISFyKasBx9153rwHGAvPMbMYgrr3V3Wvdvba4uDhqNSaiL55dxfnTS/nuw2v4+xu7Yl2OiIhI3BmSVVTuvhd4GrgAqDezMoDgsWEoakgmKSnGjy+fzYTR2Vx79zK27tF8HBERkf6iuYqq2MxGBc+zgHOBtcBC4MrgtCuBB6NVQzLLHZHOrVfMpbs3xGfvXEp7V2+sSxIREYkb0ezBKQOeMrOVwMuE5+A8BHwfOM/M1gPnBa/lKEwszuGnH6nhtR3NfP3+lZp0LCIiEkiL1hu7+0pgzkGONwHnROtzh5uzp5bylXMn8+PH1jGjIp/PnD4x1iWJiIjEnO5knAS+cFYV7zu+lO/9dS1/r9OkYxEREQWcJBCedFzDxNHZfOHuV3hztyYdi4jI8KaAkyRyMtO49ZO19IRck45FRGTYU8BJIhNGZ3PzR+awZmczX9OkYxERGcYUcJLMWVNLuO68yTy4fDu/fn5jrMsRERGJCQWcJPSFs6q44PgxfPfhNbygScciIjIMKeAkITPjxstnU1WSw7WadCwiIsOQAk6SyslM49YraunVpGMRERmGFHCSWOXobG7+aHjS8T//jyYdi4jI8KGAk+TOnFLCV8+fwsIV27ntOU06FhGR4UEBZxj4/JmTeP/MMXzvr2t4fr0mHYuISPJTwBkGzIwfXTqb6pJcrv2DJh2LiEjyU8AZJrIz07j1k3MJhZyrf7eE/V09sS5JREQkahRwhpHxReFJx6/Xt3DDfZp0LCIiyUsBZ5g5c0oJ179vCg+t3MGtz26IdTkiIiJRoYAzDH3uvZO4aGYZP/jbWp5b3xjrckRERCJOAWcYMjN+eOksJpfmcu3dy9jSpEnHIiKSXBRwhqns4E7HANfcqUnHIiKSXBRwhrFxRSP5z4/OYV19C9dr0rGIiCQRBZxh7ozJxdxwwVT+snIHv9SkYxERSRIKOMJnz5jIB2aV8cO/reWZdZp0LCIiiS8t1gVI7PVNOq5raOVTv1nMtDF5zJtQyLwJhZxYWUhxbmasSxQRERkUS4R5F7W1tb5kyZJYl5H0Glo6uHvRFl7etJtXNu+lvbsXgImjszmxspATJxRy0oRCxhZkYWYxrlZERATMbKm7177ruAKOHEx3b4hXt+1j8cbdvLxpNy9v2sO+9m4AxuSNCPfuTChkXmUh1SU5pKQo8IiIyNBTwJFjEgo56xpaeHnjbhYFoae+uROAUSPTqR1fyLwJBcybUMTx5Xmkp2p6l4iIRN+hAo7m4MiApKQYU8fkMXVMHlecUom7s2X3/gM9PIs37ubxNfUAZKWncsL4UcyrLOLECQXMOa6ArIzUGP8EIiIynKgHRyKmobmDlzftYfHGJhZv2sPanc24Q3qqMbMi/8CQ1tzxBYwamRHrckVEJAloiEqG3L72bpZu3s3ijeHQs2rbPrp7w3/fppTmcuKEAk6sDK/WKsvPinG1IiKSiBRwJObau3pZ/uZelmzazeJNu3ll8x7ausIrtSpGZR1Ylj5vQgGTinO0UktERI5Ic3Ak5rIyUjllUhGnTCoCoKc3xJodLSzetJslm3bz3PpGHli2DYCCkenUVoaHtE6cUKiJyyIiMijqwZG44e5s3NV2YFn6y5t2sznY6TwrPZU540YdGNKaM24UIzOik897Q05nTy+d3SHystJJ1RJ4EZG4pSEqSUj1zR3hwLMxHHrWBBOX01KM4yvyOXF8AdPL8+gNOR09ITq7e+nsCdHR/7E7REfPux87ukMHgkxnv9d984Qg3JN01pQSzplWyumTR5M3Ij2GrSEiIu+kgCNJobmjm6Wb9wSBZzcr3txHV2/oXeeZQWZaCiPSUw88jkhLJTM95cBjZloqI/o99j83My2F9NQUVm3bx1OvN7B3fzdpKca8CYWcM62Uc6aWUDk6OwYtcHB72rpYunkPS7fsob2rl+LcTIpzMinOzWR08FiUk6FhPhFJOgo4kpQ6unvZuqedzLSUt4WWjNSUiE1S7g05r2zZwxNrGnhybT3r6lsBmFiczbnTSjl7agm14wtIG6Lw0HcPopc37WHp5nDPVl1DuKb0VCMzLZXWzp6DXlswMv1A4Hn3Y8aBYFSYnTFkP4+IyLFQwBGJkC1N+3lybT1PrG3gpQ1NdPc6eSPSOHNKCedMK+HMySXkj4zcUFZ3b4jXtjezZPMelgTzk3a1hu8inTsijdrxBdRWFlI7voDZx41iRHoq7V297GrtpLG1k8aWzvDztz120dgSft6351h/ZlA4MuNtAagsfwTvO34Ms8bma4WbvI276++ExIwCjkgUtHb28Pz6Rh5f08BTaxtoausiNcWYO76Ac6eVcPbUUiYVZw/ql39LRzevbNnL0iDMLH/zrY1PxxZkcWJws8QTI7QPWFtnz7sCUGMQgPofr2/uoLvXmVyaw2Vzj2PBnArtNC88ubae6+9dyaVzx/K1C6cq6MiQU8ARibJQyFm+dS9Prmng8TX1rN3ZAsD4opGcM7WUc6aVcGJlIRlpbx/62b63/UDvzJLgDtAhhxSD6eV51I4vpLaygNrxhYzJHxGLHw0I37jxoZXbuXfJVpa/uZfUFOOsKSVcVjuWs6aUvOvnkuQWCjk/e6qOmx5fR1F2Brtau7ji5PF8++LjtfluhO1r7+bOFzfx279voig7k6tOn8D8mnIy07QFDijgiAy5bXvbeXJtA0+sqefvbzTR1RMiNzONMyYXM2tsPq/taGbJpj1s29sOwMiM8FL42vHhGx7WjBtFTmZ83qqqrqGFe5du5f5XttHY0klRdgbzayq4rHYs08ryYl1eVO1p6+Ivq3bwRmMrqWakphgpKUaqvfWYmgKpKSmkpkBKcE5qir3t+Tuv7bsmMy2FkyYWxvU/Xq2dPVx3z3IeWV3PgppyvnfJLH7y+DpufXYDH649ju9eMlO3V4iAptZOfv38Ru58cTMtnT28d3Ix9c0drN3ZQnFuJp96TyWfOGl8RIfEE5ECjkgM7e/q4YW6Jp5YE56709jSSUluJidWvtU7M60sN+Em9vb0hnh2fSP3LtnK42vq6e51ZlTkcdnc45hfU540e4519vTy5JoGHli2jadeb6C718nJTMPd6XUnFIJed3pDkfl9OqU0lx9fPpsZFfkReb9I2rirjWt+t4QNu9r4+oVTueq0CZgZ7s5Nj63jP5+s40NzKvjRpbMS7u9zvNixr51bn93AHxZvobMnxIUzxvD5M6uYUZGPu/N83S5ufXYDz63fxciMVC6vPY6rTpvAcYUjY116TCjgiMSJUMjZvb+LouyMpJqvsLutiweXb+PeJVt5bUczGakpnDe9lEtrx3JGdXHC/Y8+FHKWbN7DA8u28ZeV22nu6KE4N5P5s8v50AkVTC/Le9efn7sT8vDKu1AQeHrd6e3tC0J+IAi9FYpC9IbeumZTUxv/9ufX2N3WxRfPrubzZ02Km+X9T61t4Et/XEZaivGzj53Ae6pGv+ucW55cz42PruOimWX8x0dq4qb2RLC5qY3/euYN7lu6lZDDgpoKPnfmJKpKcg56/podzfzquQ38ecV2ekPOhTPKuPqMidQcN2poC48xBRwRGTKrt+/j3iVbeXD5Nvbs76Y0L5NLThjLZXPHMrH44L+s48Ubja088Mo2/rR8G1v3tJOVnsoFM8bwoTkVvGdS0ZD0Suzd38U3F67mweXbmVmRz02Xz6a6NDfqn3so7s7Pn36DGx99nWlj8vjlFXMP21vwq2c38J2H13De9FJu+dicuB5uiwfr6lv4+VN1LFyxnbTUFC6vHctnz5g04B6Znfs6+O3fN/H7RZtp6ehhXmUhV58xkXOmlgyL+VAKOCIy5PqGdu5dupVn1jXSG3Lmji/gsrljuWhWGblxcmfoXa2d/HnFdh5Yto2VW/eRYnBq1WguOaGC86ePITtGc6EeXrWD//OnV2nt7OGr50/mqtMmDnlPWFtnD1+9dwV/fXUnF88u5wf/MIusjCMHlt+9uIl/fXA1751czC+vmMuI9PgJOWt2NPPvD71GWmoKJ08s5OSJRcysyB/y3qZVW/dxy1PreWR1PSMzUvn4SeO4+vSJlOQd3WKC1s4e/vvlN7n9+Y1s29vOxNHZfOb0iVxyQkVctX+kKeCISEw1NHfwwLJt3Lt0K3UNrWSlp3LhjDFcWjuWkycUDfn/NNu7enlsTT1/WrbtQPg6vjyPD82p4OLZ5Uf9j0ykNbZ08i8PrOLR1+qpHV/AjZfNHrK7aG/a1cY1dy6hrqGVr184jc+cPmFQw6p/XLyFrz+wilMmFnHblbVR2z9uoHpDzq+f38CNj6wjLyuNwuyMAzfuzM5IpbYyHHZOnljIzIr8qPXWLd64m1uequPZdY3kjUjjU++p5B9PnUBBdmTmrPX0hvjrqzu59dkNrNq2j6LsDD55SiVXnDKewgh9RjxRwBGRuODuLH9zL/cu3cqfl2+npbOHrPRUykaNoDw/i/JRIyjLz6JiVFb42KgsyvOzBtRrcCShkPPShibuX7aNv726k9bOHsryRzC/poJLTqhgcgyHgQ7H3Xlg2Ta+uXA1Pb3ON94/lY+fND6qofDp1xv40h+WkZJi3PLREzit+t3zbQbi/le28tV7VzB3fAG3f+rEmPXabd2zn+vuWcGijbt53/GlfPdDMynKyWRXayeLNuzmpQ1NvLShifUNbwWeEyf0BZ4iZpTnHVPgcXeeXb+Lnz1Zx+JNuynKzuCq0ydwxcnjo9Ym7s6ijbv51bMbeGJtA5lpKVw6dyxXnTYh7oeKB0MBR0TiTkd3L4+s3smqrfvYsa+DbXvb2bGvnYaWTt75q6lgZDpl+VnhwBMEn7L8IACNyqI0N/OQ/wC9vrOF+5dtZeHy7ezY10FOZhrvnzmGBXMqYtJ7dLR27GvnhvtW8tz6XZxWNZofXDqLilFZEf2M/vNtpo7J49YjzLcZiIdWbufLf1zOzIp87vj0PPKzhi7kuDv3v7KNby1cjQPf/OB0Lp079pA9UY0tnSzeuJsXN+zipQ27D2yDkpOZRm1lAacEgef4AQaeUMh59LV6fvZUHau27aMsfwTXnDGRj5w4LiKhfaDqGlq47bmN3P/KNrpDIc6bVso1Z0xk7viChF/soIAjIgmjqydEfXMH2/e2Hwg+fc+3B8+bO96+31aKQWneiLeFnqz0VB59rZ41O5pJSzHeO7mYBXMqOG96acLOSXB37l68he/8ZQ2pZvzrEf7BHoy2zh6uv28FD6/ayQdnl/ODf5gZsWGlR1bv5Nq7X2HKmFzu/PRJERuOOZzdbV38ywOr+OurO5lXWciPL5896LDW2NLJoo1NQQ/P2wPPiZUFnDyxiFMmFTG97O2Bp6c3xEMrd/Dzp+tYV99KZdFIPnfmJD40Z2xMb4rZ2NLJnS9u4ncvbWbv/m7mjBvFNadP5PzjxyTcSsc+CjgiklRaO3vYsbc96PXpCz59oaid7fs66OoJMfu4UVwyp4IPzCqjKCd5tpbY0rSfr967gsWbdnPutBK+e8lMSnKPft7Q5qY2rvndUtY3tPC1C6dy9ekTI/4/+6fWNvDZu5YycXQ2d151UlS3+njq9QZuuG8le/d3cd35U7j69MhM0G5o6XjbkNYbjW0A5GamBUNahWSlp/Kr5zayZfd+ppTm8vmzJnHRzLK4ui/Q/q4e/mfpVm57fiObm/ZTmpdJZVH22/af69t8t+95YXZGXC77V8ARkWHF3Wnr6o3bu0FHQijk3P7CRn74yOuMzEjl/y2YwQdmlQ/6fZ5Z18iX/rAMgFs+NofTq4sjXeoBL9Tt4qo7XqZiVBZ3X30ypRGezL2/q4fvPryGu17awpTSXH7y4Rqml0fv7tp9gefFIPBsCALP7LH5fOGsKs6dVhrXQ6C9Ieex13by0ModNDS/tf9cS2fPQc8vzM6gOCeT0bkZB8LP6Hc8FudmUjAyY8h6hIY84JjZccDvgDFACLjV3X9qZoXAfwOVwCbgcnffc7j3UsARETm0uoYWrrtnBSu27uMDs8r49/kzBjQE5O781zMb+NEja5lcmsutV9Qyrij6d8NdtKGJT//2ZUbnZnL31SdHbB7Rsi17+Mo9K9jU1MZnTpvAdedPGfKhyIbmDhpaOjm+/N03gkwkHd29wca7ne/aeLexpe95Fw0tHXR0h951fYpBUU64B+jMKcXccMHUqNUai4BTBpS5+ytmlgssBRYAnwJ2u/v3zexrQIG7//Ph3ksBR0Tk8Hp6Q/zXM2/w0yfWM2pkBt/70EzOnV56yPP3d/Vw/X0r+cvKHVw0q4wfXTprSJdxv7JlD1fevpj8rHT+cPXJxzSRubs3xC1P1nHLU3WU5mZy4+Wzec+ko1v1JYPT11O66yBhqO/59PJ8vnLe5KjVEPMhKjN7ELgl+DrT3XcEIehpd59yuGsVcEREBmb19n1cd88K1u5s4bK5Y/m/H5xO3juWIW9p2s81dy5hXX0LN1wwlc+eEfn5NgOxaus+rrh9ESPSUrn76pOOaunyG42tfOW/l7Ni6z4umVPBNy8+fkhXaUnsxTTgmFkl8CwwA9ji7qP6fW+Puxcc5JprgGsAxo0bN3fz5s1Rr1NEJBl09vRy8xPr+cXTbzAmbwQ/umw2pwb7Rj23vpFr716Gu/OfHzuB906O3nybgVizo5lP3LYIM+Puq08a8L2I3J27XtrMdx5ew4j0VL6zYCYXzSqLcrUSj2IWcMwsB3gG+I67329mewcScPpTD46IyOAt27KH6+5ZwYZdbXzylPGMyR/BjY+8TnVJLrd+ci7ji4bmjshHUtfQwsd+tYiekHPXVScdcVJwQ3MH19+3kmfWNXJ69WhuvGx2xCcrS+KIScAxs3TgIeARd78pOPY6GqISERkS7V29/PCRtfzmhU0AvH/mGH506eyY7a91KBt3tfHxX71EW1cvd141j1ljRx30vIdX7eAbD6yio7uXb7x/GlecPD6hJ/PKsYvFJGMD7iA8ofif+h3/EdDUb5JxobvfcLj3UsARETk2izfuZnNTW8RuChgNb+7ez8due4m9bd389tPzmDv+rc795o5uvvXgau5fto1ZY/O56fIaqkqSZ7sBOXqxCDinAc8BqwgvEwf4BrAIuAcYB2wBLnP33Yd7LwUcEZHhYfvedj5+2yLqmzu4/VMncvLEIl58o4mv3ruCnc0dfOGsKr54dlVc3nBOYiPmq6iOhQKOiMjw0dDcwcduW8TWPft5/8wyHli2jfGFI7npwzWcMO6wUzZlGDpUwFEEFhGRuFKSN4I/XnMylUXZ3P/KNj42bxwPf/l0hRsZlPiaZSYiIgKMzsnkvs+9hy1N+6O61YIkL/XgiIhIXMrJTFO4kaOmgCMiIiJJRwFHREREko4CjoiIiCQdBRwRERFJOgo4IiIiknQUcERERCTpKOCIiIhI0lHAERERkaSjgCMiIiJJRwFHREREko4CjoiIiCQdc/dY13BEZtYIbI7iR4wGdkXx/SVM7Tw01M5DQ+08NNTOQyOR23m8uxe/82BCBJxoM7Ml7l4b6zqSndp5aKidh4baeWionYdGMrazhqhEREQk6SjgiIiISNJRwAm7NdYFDBNq56Ghdh4aauehoXYeGknXzpqDIyIiIklHPTgiIiKSdBRwREREJOkM64BjZheY2etmVmdmX4t1PYnMzI4zs6fMbI2ZrTazLwfHC83sMTNbHzwW9Lvm60Hbv25m74td9YnHzFLNbJmZPRS8VjtHmJmNMrP7zGxt8Pf6FLVz5JnZ/w5+Z7xqZn8wsxFq58gws9vNrMHMXu13bNBta2ZzzWxV8L2bzcyG+mc5GsM24JhZKvAz4EJgOvBRM5se26oSWg9wnbtPA04GvhC059eAJ9y9GngieE3wvY8AxwMXAD8P/kxkYL4MrOn3Wu0ceT8F/ubuU4HZhNtb7RxBZlYBfAmodfcZQCrhdlQ7R8ZvCbdTf0fTtr8ArgGqg693vmdcGrYBB5gH1Ln7BnfvAv4IzI9xTQnL3Xe4+yvB8xbC/xhUEG7TO4LT7gAWBM/nA39090533wjUEf4zkSMws7HARcBt/Q6rnSPIzPKAM4BfA7h7l7vvRe0cDWlAlpmlASOB7aidI8LdnwV2v+PwoNrWzMqAPHd/0cOrkn7X75q4NpwDTgXwZr/XW4NjcozMrBKYAywCSt19B4RDEFASnKb2P3r/AdwAhPodUztH1kSgEfhNMBR4m5llo3aOKHffBtwIbAF2APvc/VHUztE02LatCJ6/83jcG84B52BjiFozf4zMLAf4H+Cf3L35cKce5Jja/wjM7ANAg7svHeglBzmmdj6yNOAE4BfuPgdoI+jKPwS181EI5n/MByYA5UC2mX3icJcc5JjaOTIO1bYJ2+bDOeBsBY7r93os4a5ROUpmlk443Pze3e8PDtcHXZwEjw3BcbX/0TkVuNjMNhEeVj3bzO5C7RxpW4Gt7r4oeH0f4cCjdo6sc4GN7t7o7t3A/cB7UDtH02Dbdmvw/J3H495wDjgvA9VmNsHMMghPrloY45oSVjCr/tfAGne/qd+3FgJXBs+vBB7sd/wjZpZpZhMIT1xbPFT1Jip3/7q7j3X3SsJ/Z59090+gdo4od98JvGlmU4JD5wCvoXaOtC3AyWY2Mvgdcg7h+Xtq5+gZVNsGw1gtZnZy8Gf0yX7XxLW0WBcQK+7eY2bXAo8Qnrl/u7uvjnFZiexU4ApglZktD459A/g+cI+ZXUX4l9llAO6+2szuIfyPRg/wBXfvHfKqk4faOfK+CPw++A/QBuAfCf+nUO0cIe6+yMzuA14h3G7LCG8ZkIPa+ZiZ2R+AM4HRZrYV+CZH97vic4RXZGUBfw2+4p62ahAREZGkM5yHqERERCRJKeCIiIhI0lHAERERkaSjgCMiIiJJRwFHREREko4CjogckZkVmdny4GunmW3r9zrjCNfWmtnNA/iMv0eu4ne99ygz+3y03l9E4o+WiYvIoJjZt4BWd7+x37E0d++JXVWHF+yP9lCwY7WIDAPqwRGRo2JmvzWzm8zsKeAHZjbPzP4ebE759767AJvZmWb2UPD8W2Z2u5k9bWYbzOxL/d6vtd/5T5vZfWa21sx+H9xBFTN7f3DseTO7ue9931HX8Wa2OOhdWmlm1YRvbjYpOPaj4Lzrzezl4JxvB8cqg/e/Izh+n5mNDL73fTN7LTh+4zs/V0Tiy7C9k7GIRMRk4Fx37zWzPOCM4C7h5wLfBf7hINdMBc4CcoHXzewXwT5E/c0Bjie8580LwKlmtgT4ZfAZG4O7tB7M/wJ+6u59dyFOJbxR5gx3rwEws/MJ34p+HuHNBBea2RmE7+w6BbjK3V8ws9uBzwePHwKmurub2ajBNpSIDC314IjIsbi33+3c84F7zexV4CeEA8rB/MXdO919F+GN/koPcs5id9/q7iFgOVBJOBhtcPeNwTmHCjgvAt8ws38Gxrt7+0HOOT/4WkZ4m4CphAMPwJvu/kLw/C7gNKAZ6ABuM7NLgP2H+GwRiRMKOCJyLNr6Pf934KlgnssHgRGHuKaz3/NeDt6TfLBzbCAFufvdwMVAO/CImZ19kNMM+J671wRfVe7+6763ePdbeg/h3p7/ARYAfxtILSISOwo4IhIp+cC24PmnovD+a4GJwYRhgA8f7CQzm0i4p+dmwjskzwJaCA+J9XkE+LSZ5QTXVJhZSfC9cWZ2SvD8o8DzwXn57v4w8E9ATaR+KBGJDs3BEZFI+SFwh5l9BXgy0m/u7u3BUu+/mdkuYPEhTv0w8Akz6wZ2Av/m7rvN7IVg+Oyv7n69mU0DXgzmL7cCnyDcW7QGuNLMfgmsB35BOLw9aGYjCPf+/O9I/3wiEllaJi4iCcPMcty9NVhV9TNgvbv/JILvX4mWk4skBQ1RiUgiudrMlgOrCfeq/DK25YhIvFIPjoiIiCQd9eCIiIhI0lHAERERkaSjgCMiIiJJRwFHREREko4CjoiIiCSd/w+ST4vb3+0LewAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 576x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "# Loss function plot\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(50 * np.arange(len(train_loss_list)), train_loss_list)\n",
    "plt.title('cLSTM training')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Training steps')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "2b3c1891",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True variable usage = 20.07%\n",
      "Estimated variable usage = 100.00%\n",
      "Accuracy = 20.07%\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x360 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Check learned Granger causality\n",
    "GC_est = clstm.GC().cpu().data.numpy()\n",
    "\n",
    "print('True variable usage = %.2f%%' % (100 * np.mean(GC)))\n",
    "print('Estimated variable usage = %.2f%%' % (100 * np.mean(GC_est)))\n",
    "print('Accuracy = %.2f%%' % (100 * np.mean(GC == GC_est)))\n",
    "\n",
    "# Make figures\n",
    "fig, axarr = plt.subplots(1, 2, figsize=(10, 5))\n",
    "axarr[0].imshow(GC, cmap='Blues')\n",
    "axarr[0].set_title('GC actual')\n",
    "axarr[0].set_ylabel('Affected series')\n",
    "axarr[0].set_xlabel('Causal series')\n",
    "axarr[0].set_xticks([])\n",
    "axarr[0].set_yticks([])\n",
    "\n",
    "axarr[1].imshow(GC_est, cmap='Blues', vmin=0, vmax=1, extent=(0, len(GC_est), len(GC_est), 0))\n",
    "axarr[1].set_ylabel('Affected series')\n",
    "axarr[1].set_xlabel('Causal series')\n",
    "axarr[1].set_xticks([])\n",
    "axarr[1].set_yticks([])\n",
    "\n",
    "# Mark disagreements\n",
    "for i in range(len(GC_est)):\n",
    "    for j in range(len(GC_est)):\n",
    "        if GC[i, j] != GC_est[i, j]:\n",
    "            rect = plt.Rectangle((j, i-0.05), 1, 1, facecolor='none', edgecolor='red', linewidth=1)\n",
    "            axarr[1].add_patch(rect)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c2db8df",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
