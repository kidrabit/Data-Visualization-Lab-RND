{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1922b38f",
   "metadata": {},
   "source": [
    "## Graph Representation Learning\n",
    " - intuition : graph에는 다양한 downstram Task가 존재한다.\n",
    " \n",
    " - 이때, Machine Learning Lifecycle과 같이 Feature Engineering이 필요하고 Task Independant한 Automatic Feature Learning이 필요.\n",
    " - node를 feature로 표현하는 방법이 필요 -> Graph를 embedding 하기로 결정\n",
    " - 즉, 각 node를 low-dimension space에 mapping을 해야된다.\n",
    " - ex) graph -> Adjacency Matrix -> Latent Matrix (인접행렬은 계산량이 많아 더 낮은 차원의 Latent Matrix로 만들었다.)\n",
    " \n",
    " \n",
    " ### 기존 Deep Learning 과의 차이점\n",
    "   - network는 유클리드 공간에 있지 않아 좌표계로 표현할수 없다.(CNN과 같은 filter를 이동하면서 학습 불가)\n",
    "   - network는 고정된 형태가 아니다. -> 순서가 뒤바뀌어도 network에 영향이 없다.(즉, 데이터로 표현하기 힘듬.)\n",
    "  \n",
    "  \n",
    " ### Embedding Nodes\n",
    "   - graph G (V is the vertex set, A is the adjacency matrix(assume binary), no node features or extra information is used)\n",
    "   - Goal is to encode nodes so that similarity on the embedding space(e.g. dot product) approximates similarity in the original network ($similarity(u,v) = z_v^Tz_u$ )\n",
    "   \n",
    "   - 1) encoder 정의 ($ENC(v) = z_v$)\n",
    "   - 2) Similarity Function을 정의 (ex. 코사인 유사도)\n",
    "   - 3) encoder의 params optimizer\n",
    "   \n",
    " #### shallow encoding\n",
    "   - 가장 간단한 encoding 방식으로 look-up table을 사용.\n",
    "   - 각 node는 one-hot encoding으로 표현하여 look-up table의 representation을 가리키는 indicatior로 작동\n",
    "      - each node is assigned to a unique embeddong vector\n",
    "   \n",
    "   - 이런 방식을 차용하여 다양한 Embedding 방법론들이 연구되었다. (DeepWalk, node2vec, TransE) 어떤 방법론을 선택할지는 어떤 Node Similarity를 사용하느냐에 따라 다르다.\n",
    "      \n",
    "#### Random walk Approaches to Node Embeddings\n",
    "\n",
    "   - random walk : 특정 시작 node에서 랜덤으로 이웃을 선택하여 이동, 방향은 자유로우며 설정 횟수 만큼 이동(같은 노드도 여러번 방문 간가능) -> 이동하면서 방문한 node($z_v$)은 시작 node의 이웃으로 간주하고 repersentation을 학습\n",
    "   - $z_v^Tz_u$ : probability that u and v co-occur on a randeom warlk over the network\n",
    "   \n",
    "   - random walk를 사용하는 이유\n",
    "      - Expressivity : 확률정의에 따른 인접 이웃 정보를 유연하게 표현가능\n",
    "      - Eifficiency : 임베딩 과정에서 모든 node pair를 고려할 필요가 없어 계산량이 줄음\n",
    "   \n",
    "   - Random walk optimization\n",
    "      - 1) strategy 을 사용하여 고정된 횟수만큼 무작위로 이동한다\n",
    "      - 2) 매 Node마다 이웃 Node의 multiset $N_R(u)$를 저장한다.(by stratege R)\n",
    "      - 3) 목적함수에 따라 가 주어졌을 때 $N_R(u)$를 예측하는 Embedding을 학습한다. 즉 Loss를 최소화하는 $z-u$를 학습하는 것이다.\n",
    "      \n",
    "#### Negative Sampling\n",
    "  - Random walk를 통한 Graph embedding에는 계산량이 많음(줄여도 많다.)\n",
    "  - 따라서 Word2Vec에서 쓰이는 Negative Sampling을 Graph Embedding에 적용\n",
    "  - 다만, Word2Vec에 쓰이는 단어 빈도에 따라 Negative Sampling될 확률에 가중치를 준다.\n",
    "  \n",
    "  - Sample k negative nodes proportional to degree\n",
    "  - k(negative samples의 개수)의 두가지 고려 사향\n",
    "    - 1) 높은 k는 높은 robust estimates를 준다.\n",
    "    - 2) 높은 k는 학습이 negative events에 편향된다.\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10a821da",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
