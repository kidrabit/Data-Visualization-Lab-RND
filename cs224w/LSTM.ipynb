{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2baae5c5",
   "metadata": {},
   "source": [
    "### vanilla Rnn의 문제점\n",
    "\n",
    " - 긴 sequence를 가지면 vanishing gradient 발생\n",
    " - Back-prooagation 이 제대로 이루어지지 않는다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd7572e6",
   "metadata": {},
   "source": [
    "## LSTM(Long Short Term Memory Network)\n",
    " - 기본 RNN에 구조에 cell state를 추가\n",
    " - cell state : 중요한 정보를 남기거나 추가하는 기능\n",
    " - cell state에는 현재의 정보와 다음에 넘겨줄 정보가 같이 있으니, hidden state($h_t$)에 현재의 정보를 가공해서 추출\n",
    " - $C_{t-1}$(cell state)의 정보들의 gate coefficient(중요도,$G$)를 구한다.\n",
    "    -> using small non-linear layer \n",
    "\n",
    "- 그 후 $C_{t-1}*G $를 구하여 남길 정보를 추출\n",
    "![lstm](./lstm.png)\n",
    "\n",
    "- 과정\n",
    "\n",
    " 1) $ C_{t-1}$에서 불필요한 정보제거 ($f_t = \\sigma (W_f[h_{t-1},x_t] +b_f)$(forget gate))\n",
    "\n",
    " 2) $ C_{t-1}$에 새로운 input $x_t$와 $h_{t-1}$을 보고 중요한 정보를 넣는다.(($i_t = \\sigma (W_i[h_{t-1},x_t] +b_i)$(input gate), $C_t' = tanh(W_c[h_{t-1},x_t] + b_c$)\n",
    "\n",
    " 3) 위과정을 통해 $C_t$를 만든다. ($C_t = f_t*C_{t-1} + i_t * C_t'$)\n",
    "\n",
    " 4) $C_t$를 적절히 가공해 해당 t에서 $h_t$를 만든다.($h_t = o_t *tanh(C_t)$,$o_t = \\sigma (W_o[h_{t-1},x_t] + b_o)$)\n",
    "\n",
    "### vanishing gradient 가 발생하지 않는이유\n",
    " - cell state에 정보가 기록이 되고 나중에 지워지지 않는 다면,\n",
    " - non-linear activation function을 거치지 않고 t+n 시점까지 흘러오기 때문에 상당부분 해결가능\n",
    "\n",
    "\n",
    "## GRU(Gated Recurrent Unit)\n",
    " - Lstm의 gate가 많은 문제를 해결하려고 했다.\n",
    " \n",
    " 1) reset gate를 계산하여 임시 $h_t$를 만든다. ($r_t = \\sigma(h_{t-1},x_t)$, $h_t' = tanh(W[r_t * h_{t-1},x_t])$)\n",
    " \n",
    " 2) update gate를 통해 $h_{t-1}과 h_t'$간의 비중을 결정($z_t = \\sigma(W_z[h_{t-1},x_t])$)\n",
    "\n",
    " 3) $z_t$를 이용해 $h_t$를 계산한다.($h_t = (1-z_t) * h_{t-1} + z_t * h_t'$) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d431ba37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.8.1\n",
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import datetime\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import argparse\n",
    "from copy import deepcopy # Add Deepcopy for args\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "import seaborn as sns \n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(torch.__version__)\n",
    "%matplotlib inline\n",
    "%pylab inline\n",
    "pylab.rcParams['figure.figsize'] = (15, 9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0a67604d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM(nn.Module):\n",
    "    def __init__(self, in_dim, hid_dim, out_dim, num_layer, batch_size, dropout,use_bn):\n",
    "        \n",
    "        self.in_dim = in_dim\n",
    "        self.hid_dim = hid_dim\n",
    "        self.out_dim = out_dim\n",
    "        self.num_layer = num_layer\n",
    "        \n",
    "        self.batch_size = batch_size\n",
    "        self.dropout = dropout\n",
    "        self.use_bn = use_bn\n",
    "        \n",
    "        self.lstm = nn.LSTM(self.in_dim, self.hid_dim, self.num_layer)\n",
    "        self.hidden = self.init_hidden()\n",
    "        self.regressor = self.make_regressor()\n",
    "        \n",
    "        \n",
    "    def init_hidden(self):\n",
    "        return (torch.zeros(self.num_layers, self.batch_size, self.hid_dim),\n",
    "                torch.zeros(self.num_layers, self.batch_size, self.hid_dim))\n",
    "    \n",
    "    # 간단한 MLP model로 생각하면 됨.\n",
    "    def make_regressor(self):\n",
    "        layers = []\n",
    "        if self.use_bn:\n",
    "            layers.append(nn.BatchNorm1d(self.hid_dim))\n",
    "        layers.append(nn.Dropout(self.dropout))\n",
    "        \n",
    "        layers.append(nn.Linear(self.hid_dim, self.hid_dim // 2))\n",
    "        layers.append(nn.ReLU())\n",
    "        layers.append(nn.Linear(self.hid_dim // 2, self.hid_dim))\n",
    "        regressor = nn.Sequential(*layers)\n",
    "        return regressor\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \n",
    "        # lstm_out :마지막 레이어의 h(??), self.hidden : 모든 layer의 hidden_state\n",
    "        lstm_out, self.hidden = self.lstm(x, self.hidden) \n",
    "        y_pred = self.regressor(lstm_out[-1].view(self.batch_size, -1))\n",
    "        return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "369f64ed",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
