{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3b70a9b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "88c5f7cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from copy import deepcopy\n",
    "\n",
    "\n",
    "\n",
    "class RBFlayer(nn.Module):\n",
    "    def __init__(self, timelag):\n",
    "        super(RBFlayer, self).__init__()\n",
    "\n",
    "        self.timelag = timelag\n",
    "\n",
    "        device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        torch.cuda.manual_seed(0)\n",
    "\n",
    "        self.init_weight_cause = nn.Parameter(torch.rand(self.timelag, device=device))\n",
    "        self.init_weight_target = nn.Parameter(torch.rand(self.timelag, device=device))\n",
    "        self.cause_clt  = nn.Parameter(torch.rand(1,device=device))\n",
    "        self.cause_std = nn.Parameter(torch.rand(1,device=device))\n",
    "        self.target_clt = nn.Parameter(torch.rand(1,device=device))\n",
    "        self.target_std = nn.Parameter(torch.rand(1, device=device))\n",
    "\n",
    "    def init_clt(self):\n",
    "        return nn.Parameter(torch.rand(1,device=device))\n",
    "\n",
    "    def init_std(self):\n",
    "        return nn.Parameter(torch.rand(1, device=device))\n",
    "\n",
    "    def rbf(self, x, cluster, std):\n",
    "        return (x - cluster) * (x - cluster) / (std * std)\n",
    "\n",
    "    def forward(self, cause, target):\n",
    "        cause_c, cause_s = self.cause_clt[0], self.cause_std[0]\n",
    "        target_c, target_s = self.target_clt[0], self.target_std[0]\n",
    "        \n",
    "        cause = self.init_weight_cause * torch.tensor(\n",
    "            [self.rbf(cause[i], cause_c, cause_s) for i in range(len(cause))], device = device)\n",
    "        target = self.init_weight_target * torch.tensor(\n",
    "            [self.rbf(target[j], target_c, target_s) for j in range(len(target))], device = device)\n",
    "\n",
    "        return cause, target\n",
    "\n",
    "\n",
    "class RBFnet(nn.Module):\n",
    "    def __init__(self, input_size , output_size, timelag):\n",
    "        super(RBFnet,self).__init__()\n",
    "    \n",
    "        self.input_size = input_size      # number of data\n",
    "        self.output_size = output_size\n",
    "        self.timelag = timelag\n",
    "\n",
    "        self.linear = nn.ModuleList([nn.Linear(self.timelag*2,1) for _ in range(self.input_size)])\n",
    "        self.relu = nn.ReLU()\n",
    "        self.networks = nn.ModuleList([RBFlayer(self.timelag) for _ in range(self.input_size)])\n",
    "\n",
    "    def cause_target(self, cause, target):\n",
    "        x = torch.cat((cause, target), 0)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def GC(self, threshold=True):\n",
    "        '''\n",
    "        Extract learned Granger causality.\n",
    "        Args:\n",
    "          threshold: return norm of weights, or whether norm is nonzero.\n",
    "        Returns:\n",
    "          GC: (p x p) matrix. Entry (i, j) indicates whether variable j is\n",
    "            Granger causal of variable i.\n",
    "        '''\n",
    "        GC = [torch.norm(net.init_weight_cause, dim=0)\n",
    "              for net in self.networks]\n",
    "        GC = torch.stack(GC)\n",
    "        if threshold:\n",
    "            return (GC > 0).int()\n",
    "        else:\n",
    "            return GC\n",
    "\n",
    "\n",
    "    def forward(self, causes, targets):\n",
    "        out_list = []\n",
    "        for i in range(self.input_size):\n",
    "            cause, target = self.networks[i](causes[i], targets[i])\n",
    "            cause, target = self.relu(cause), self.relu(target)\n",
    "            pred = torch.cat((cause, target),0)\n",
    "            pred = self.linear[i](pred)\n",
    "            out_list.append(pred)\n",
    "\n",
    "        return out_list\n",
    "\n",
    "\n",
    "def restore_parameters(model, best_model):\n",
    "    '''Move parameter values from best_model to model.'''\n",
    "    for params, best_params in zip(model.parameters(), best_model.parameters()):\n",
    "        params.data = best_params\n",
    "\n",
    "\n",
    "def train_rbf(model, input_causes, input_targets, Y, lr, epochs, lookback=5,device = device):\n",
    "    # input_causes, input_targets : X\n",
    "    # Y : Y\n",
    "    model.to(device)\n",
    "    loss_fn = nn.MSELoss(reduction='mean')\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    train_loss_list = []\n",
    "\n",
    "    best_it = None\n",
    "    best_model = None\n",
    "    best_loss = np.inf\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        pred = model(input_causes, input_targets)\n",
    "        loss = sum([loss_fn(pred[i], Y[i]) for i in range(len(Y))])\n",
    "        print(\"epoch {} loss {} :\".format(epoch, loss / len(Y)))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        model.zero_grad()\n",
    "\n",
    "        mean_loss = loss / len(Y)\n",
    "\n",
    "        if mean_loss < best_loss:\n",
    "            best_loss = mean_loss\n",
    "            best_it = epoch\n",
    "            best_model = deepcopy(model)\n",
    "        elif (epoch - best_it) == lookback:\n",
    "            if verbose:\n",
    "                print('Stopping early')\n",
    "            break\n",
    "\n",
    "    restore_parameters(model, best_model)\n",
    "\n",
    "    return train_loss_list\n",
    "\n",
    "\n",
    "\n",
    "def data_split(X, cause, target, timelag, device = device):\n",
    "    input_cause = []\n",
    "    input_target = []\n",
    "    Y = []\n",
    "\n",
    "    for i in range(len(X) - (timelag + 1)):\n",
    "        input_cause.append(X[cause].values[i: i + timelag])\n",
    "        input_target.append(X[target].values[i: i + timelag])\n",
    "        Y.append([X[target][i + timelag + 1]])\n",
    "\n",
    "    return torch.tensor(input_cause, device=device).float(), torch.tensor(input_target,device=device).float(), torch.tensor(Y, device=device).float()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "1f3997a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('C:/Users/chanyoung/Desktop/Neural-GC-master/lorenz_96_10_10_1000.csv')\n",
    "X2d = df[['a','b']]\n",
    "torch.manual_seed(1234)\n",
    "input_cause, input_target, Y = data_split(X2d, 'a', 'b', 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "d101f6b2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0 loss 1984.463623046875 :\n",
      "epoch 1 loss 1305.23583984375 :\n",
      "epoch 2 loss 831.1533203125 :\n",
      "epoch 3 loss 528.258056640625 :\n",
      "epoch 4 loss 344.78375244140625 :\n",
      "epoch 5 loss 236.38897705078125 :\n",
      "epoch 6 loss 176.8124542236328 :\n",
      "epoch 7 loss 149.3320770263672 :\n",
      "epoch 8 loss 141.1910858154297 :\n",
      "epoch 9 loss 141.9173126220703 :\n",
      "epoch 10 loss 144.8268585205078 :\n",
      "epoch 11 loss 146.95358276367188 :\n",
      "epoch 12 loss 147.05538940429688 :\n",
      "epoch 13 loss 143.9816436767578 :\n",
      "epoch 14 loss 137.0172882080078 :\n",
      "epoch 15 loss 126.33810424804688 :\n",
      "epoch 16 loss 113.26525115966797 :\n",
      "epoch 17 loss 99.3742904663086 :\n",
      "epoch 18 loss 85.97523498535156 :\n",
      "epoch 19 loss 73.65156555175781 :\n",
      "epoch 20 loss 62.84405517578125 :\n",
      "epoch 21 loss 53.8270263671875 :\n",
      "epoch 22 loss 46.70965576171875 :\n",
      "epoch 23 loss 41.34923553466797 :\n",
      "epoch 24 loss 37.36167907714844 :\n",
      "epoch 25 loss 34.29926681518555 :\n",
      "epoch 26 loss 31.757936477661133 :\n",
      "epoch 27 loss 29.37957763671875 :\n",
      "epoch 28 loss 26.96114158630371 :\n",
      "epoch 29 loss 24.47947883605957 :\n",
      "epoch 30 loss 22.005586624145508 :\n",
      "epoch 31 loss 19.67422103881836 :\n",
      "epoch 32 loss 17.581829071044922 :\n",
      "epoch 33 loss 15.84903621673584 :\n",
      "epoch 34 loss 14.514267921447754 :\n",
      "epoch 35 loss 13.515190124511719 :\n",
      "epoch 36 loss 12.662792205810547 :\n",
      "epoch 37 loss 11.763315200805664 :\n",
      "epoch 38 loss 10.702648162841797 :\n",
      "epoch 39 loss 9.514517784118652 :\n",
      "epoch 40 loss 8.274007797241211 :\n",
      "epoch 41 loss 7.096916198730469 :\n",
      "epoch 42 loss 6.077938556671143 :\n",
      "epoch 43 loss 5.280074119567871 :\n",
      "epoch 44 loss 4.689595699310303 :\n",
      "epoch 45 loss 4.247767448425293 :\n",
      "epoch 46 loss 3.9015402793884277 :\n",
      "epoch 47 loss 3.610609531402588 :\n",
      "epoch 48 loss 3.3680081367492676 :\n",
      "epoch 49 loss 3.155805826187134 :\n",
      "epoch 50 loss 2.958777904510498 :\n",
      "epoch 51 loss 2.7770168781280518 :\n",
      "epoch 52 loss 2.6027169227600098 :\n",
      "epoch 53 loss 2.421560049057007 :\n",
      "epoch 54 loss 2.2134628295898438 :\n",
      "epoch 55 loss 1.9840110540390015 :\n",
      "epoch 56 loss 1.7403515577316284 :\n",
      "epoch 57 loss 1.5031882524490356 :\n",
      "epoch 58 loss 1.287392497062683 :\n",
      "epoch 59 loss 1.1112310886383057 :\n",
      "epoch 60 loss 0.9714033603668213 :\n",
      "epoch 61 loss 0.8652105331420898 :\n",
      "epoch 62 loss 0.7880785465240479 :\n",
      "epoch 63 loss 0.7250789999961853 :\n",
      "epoch 64 loss 0.670403778553009 :\n",
      "epoch 65 loss 0.6152195930480957 :\n",
      "epoch 66 loss 0.557044506072998 :\n",
      "epoch 67 loss 0.5056043267250061 :\n",
      "epoch 68 loss 0.45698264241218567 :\n",
      "epoch 69 loss 0.4117809236049652 :\n",
      "epoch 70 loss 0.3739341199398041 :\n",
      "epoch 71 loss 0.3447556793689728 :\n",
      "epoch 72 loss 0.3231585621833801 :\n",
      "epoch 73 loss 0.3031870424747467 :\n",
      "epoch 74 loss 0.27969422936439514 :\n",
      "epoch 75 loss 0.2515239417552948 :\n",
      "epoch 76 loss 0.22051621973514557 :\n",
      "epoch 77 loss 0.19187387824058533 :\n",
      "epoch 78 loss 0.16581371426582336 :\n",
      "epoch 79 loss 0.14385254681110382 :\n",
      "epoch 80 loss 0.12709945440292358 :\n",
      "epoch 81 loss 0.11500944942235947 :\n",
      "epoch 82 loss 0.10608146339654922 :\n",
      "epoch 83 loss 0.09875898063182831 :\n",
      "epoch 84 loss 0.09019184112548828 :\n",
      "epoch 85 loss 0.08341586589813232 :\n",
      "epoch 86 loss 0.07768543064594269 :\n",
      "epoch 87 loss 0.0717969611287117 :\n",
      "epoch 88 loss 0.06544111669063568 :\n",
      "epoch 89 loss 0.0586204007267952 :\n",
      "epoch 90 loss 0.052149448543787 :\n",
      "epoch 91 loss 0.04570998623967171 :\n",
      "epoch 92 loss 0.039359305053949356 :\n",
      "epoch 93 loss 0.03362741321325302 :\n",
      "epoch 94 loss 0.029187994077801704 :\n",
      "epoch 95 loss 0.026364710181951523 :\n",
      "epoch 96 loss 0.02436469867825508 :\n",
      "epoch 97 loss 0.022841349244117737 :\n",
      "epoch 98 loss 0.021970141679048538 :\n",
      "epoch 99 loss 0.021044712513685226 :\n",
      "epoch 100 loss 0.019806303083896637 :\n",
      "epoch 101 loss 0.018455935642123222 :\n",
      "epoch 102 loss 0.016687879338860512 :\n",
      "epoch 103 loss 0.01432585809379816 :\n",
      "epoch 104 loss 0.01252833567559719 :\n",
      "epoch 105 loss 0.010992005467414856 :\n",
      "epoch 106 loss 0.009606067091226578 :\n",
      "epoch 107 loss 0.008498069830238819 :\n",
      "epoch 108 loss 0.007344195619225502 :\n",
      "epoch 109 loss 0.006695879157632589 :\n",
      "epoch 110 loss 0.006088078022003174 :\n",
      "epoch 111 loss 0.0056477622129023075 :\n",
      "epoch 112 loss 0.005207793787121773 :\n",
      "epoch 113 loss 0.004717753268778324 :\n",
      "epoch 114 loss 0.004324363078922033 :\n",
      "epoch 115 loss 0.0038173184730112553 :\n",
      "epoch 116 loss 0.003636891720816493 :\n",
      "epoch 117 loss 0.0034220474772155285 :\n",
      "epoch 118 loss 0.003144544083625078 :\n",
      "epoch 119 loss 0.002994902664795518 :\n",
      "epoch 120 loss 0.002729474101215601 :\n",
      "epoch 121 loss 0.0023848852142691612 :\n",
      "epoch 122 loss 0.0020811280701309443 :\n",
      "epoch 123 loss 0.0018868364859372377 :\n",
      "epoch 124 loss 0.0016012551495805383 :\n",
      "epoch 125 loss 0.0013720298884436488 :\n",
      "epoch 126 loss 0.0012466112384572625 :\n",
      "epoch 127 loss 0.0012156550073996186 :\n",
      "epoch 128 loss 0.0011574234813451767 :\n",
      "epoch 129 loss 0.0010797777213156223 :\n",
      "epoch 130 loss 0.0009532766998745501 :\n",
      "epoch 131 loss 0.000894675962626934 :\n",
      "epoch 132 loss 0.0008403713000006974 :\n",
      "epoch 133 loss 0.0007260024431161582 :\n",
      "epoch 134 loss 0.0006807849858887494 :\n",
      "epoch 135 loss 0.0006139802862890065 :\n",
      "epoch 136 loss 0.0005625179037451744 :\n",
      "epoch 137 loss 0.0004976291093043983 :\n",
      "epoch 138 loss 0.0004625531146302819 :\n",
      "epoch 139 loss 0.00048432411858811975 :\n",
      "epoch 140 loss 0.00045417703222483397 :\n",
      "epoch 141 loss 0.0004154120688326657 :\n",
      "epoch 142 loss 0.00038136812509037554 :\n",
      "epoch 143 loss 0.00036458956310525537 :\n",
      "epoch 144 loss 0.00033004203578457236 :\n",
      "epoch 145 loss 0.0003331769839860499 :\n",
      "epoch 146 loss 0.000292563927359879 :\n",
      "epoch 147 loss 0.00024168103118427098 :\n",
      "epoch 148 loss 0.0002495096414349973 :\n",
      "epoch 149 loss 0.00022169254953041673 :\n",
      "epoch 150 loss 0.00019279925618320704 :\n",
      "epoch 151 loss 0.00019597882055677474 :\n",
      "epoch 152 loss 0.00018014815577771515 :\n",
      "epoch 153 loss 0.00020231495727784932 :\n",
      "epoch 154 loss 0.0001901877549244091 :\n",
      "epoch 155 loss 0.00017509228200651705 :\n",
      "epoch 156 loss 0.00017559528350830078 :\n",
      "epoch 157 loss 0.00017257036233786494 :\n",
      "epoch 158 loss 0.00016141566447913647 :\n",
      "epoch 159 loss 0.00014786038082093 :\n",
      "epoch 160 loss 0.00015067434287630022 :\n",
      "epoch 161 loss 0.00012782365956809372 :\n",
      "epoch 162 loss 0.00012833738583140075 :\n",
      "epoch 163 loss 0.00014320877380669117 :\n",
      "epoch 164 loss 0.00011474006169009954 :\n",
      "epoch 165 loss 0.00013604428386315703 :\n",
      "epoch 166 loss 0.00012426167086232454 :\n",
      "epoch 167 loss 0.00010419667523819953 :\n",
      "epoch 168 loss 0.00012099921877961606 :\n",
      "epoch 169 loss 0.00011867113062180579 :\n",
      "epoch 170 loss 9.171625424642116e-05 :\n",
      "epoch 171 loss 0.00011186949996044859 :\n",
      "epoch 172 loss 0.00010890024714171886 :\n",
      "epoch 173 loss 0.00010429896792629734 :\n",
      "epoch 174 loss 0.00012250874715391546 :\n",
      "epoch 175 loss 9.208628034684807e-05 :\n",
      "epoch 176 loss 0.000102236881502904 :\n",
      "epoch 177 loss 0.00010175608622375876 :\n",
      "epoch 178 loss 0.00010063124500447884 :\n",
      "epoch 179 loss 0.00010730827489169315 :\n",
      "epoch 180 loss 0.00010760612349258736 :\n",
      "epoch 181 loss 0.00010831718100234866 :\n",
      "epoch 182 loss 9.328427404398099e-05 :\n",
      "epoch 183 loss 0.00010041247878689319 :\n",
      "epoch 184 loss 0.00010821654723258689 :\n",
      "epoch 185 loss 9.570379188517109e-05 :\n",
      "epoch 186 loss 8.443113620160148e-05 :\n",
      "epoch 187 loss 9.372758358949795e-05 :\n",
      "epoch 188 loss 8.541580609744415e-05 :\n",
      "epoch 189 loss 8.826285920804366e-05 :\n",
      "epoch 190 loss 9.072005195776e-05 :\n",
      "epoch 191 loss 0.00010342209134250879 :\n",
      "epoch 192 loss 8.807833364699036e-05 :\n",
      "epoch 193 loss 0.0001141873435699381 :\n",
      "epoch 194 loss 8.41560322442092e-05 :\n",
      "epoch 195 loss 8.604994945926592e-05 :\n",
      "epoch 196 loss 8.033704943954945e-05 :\n",
      "epoch 197 loss 0.00010092571028508246 :\n",
      "epoch 198 loss 0.00010729139467002824 :\n",
      "epoch 199 loss 8.875227649696171e-05 :\n",
      "epoch 200 loss 9.143759962171316e-05 :\n",
      "epoch 201 loss 0.00010677347017917782 :\n",
      "epoch 202 loss 7.64262440497987e-05 :\n",
      "epoch 203 loss 8.757073373999447e-05 :\n",
      "epoch 204 loss 0.00010434559953864664 :\n",
      "epoch 205 loss 8.707701636012644e-05 :\n",
      "epoch 206 loss 8.009922748897225e-05 :\n",
      "epoch 207 loss 9.549959213472903e-05 :\n",
      "epoch 208 loss 7.818702579243109e-05 :\n",
      "epoch 209 loss 8.36261679069139e-05 :\n",
      "epoch 210 loss 9.38592929742299e-05 :\n",
      "epoch 211 loss 9.503015462541953e-05 :\n",
      "epoch 212 loss 8.556238026358187e-05 :\n",
      "epoch 213 loss 8.508848986821249e-05 :\n",
      "epoch 214 loss 8.242265175795183e-05 :\n",
      "epoch 215 loss 8.208068902604282e-05 :\n",
      "epoch 216 loss 8.696586155565456e-05 :\n",
      "epoch 217 loss 0.00010580582602415234 :\n",
      "epoch 218 loss 8.295285806525499e-05 :\n",
      "epoch 219 loss 7.648146129213274e-05 :\n",
      "epoch 220 loss 0.00010976097837556154 :\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 221 loss 9.047792264027521e-05 :\n",
      "epoch 222 loss 7.948128768475726e-05 :\n",
      "epoch 223 loss 7.677499525016174e-05 :\n",
      "epoch 224 loss 8.86929192347452e-05 :\n",
      "epoch 225 loss 9.160939953289926e-05 :\n",
      "epoch 226 loss 7.673873915337026e-05 :\n",
      "epoch 227 loss 8.428399451076984e-05 :\n",
      "epoch 228 loss 8.960432751337066e-05 :\n",
      "epoch 229 loss 0.000101084602647461 :\n",
      "epoch 230 loss 9.046193008543923e-05 :\n",
      "epoch 231 loss 9.382428834214807e-05 :\n",
      "epoch 232 loss 9.94754591374658e-05 :\n",
      "epoch 233 loss 8.243831689469516e-05 :\n",
      "epoch 234 loss 8.581011206842959e-05 :\n",
      "epoch 235 loss 8.24955859570764e-05 :\n",
      "epoch 236 loss 8.266515214927495e-05 :\n",
      "epoch 237 loss 9.991026308853179e-05 :\n",
      "epoch 238 loss 9.121901530306786e-05 :\n",
      "epoch 239 loss 9.145677177002653e-05 :\n",
      "epoch 240 loss 9.478822175879031e-05 :\n",
      "epoch 241 loss 0.0001143175977631472 :\n",
      "epoch 242 loss 9.322172263637185e-05 :\n",
      "epoch 243 loss 9.216415492119268e-05 :\n",
      "epoch 244 loss 0.00011507054296089336 :\n",
      "epoch 245 loss 0.00010681723506422713 :\n",
      "epoch 246 loss 0.00010314165410818532 :\n",
      "epoch 247 loss 9.856148972176015e-05 :\n",
      "epoch 248 loss 9.420332207810134e-05 :\n",
      "epoch 249 loss 0.00012192253780085593 :\n",
      "epoch 250 loss 8.968495239969343e-05 :\n",
      "epoch 251 loss 9.607306856196374e-05 :\n",
      "epoch 252 loss 0.00011087123129982501 :\n",
      "epoch 253 loss 8.514908404322341e-05 :\n",
      "epoch 254 loss 9.947897342499346e-05 :\n",
      "epoch 255 loss 0.00011205680493731052 :\n",
      "epoch 256 loss 0.00010490259592188522 :\n",
      "epoch 257 loss 9.511606185697019e-05 :\n",
      "epoch 258 loss 9.626738028600812e-05 :\n",
      "epoch 259 loss 0.00011254980927333236 :\n",
      "epoch 260 loss 0.0001027024190989323 :\n",
      "epoch 261 loss 0.00011221842578379437 :\n",
      "epoch 262 loss 9.875667456071824e-05 :\n",
      "epoch 263 loss 8.168083149939775e-05 :\n",
      "epoch 264 loss 0.00010975128680001944 :\n",
      "epoch 265 loss 8.810732833808288e-05 :\n",
      "epoch 266 loss 0.00011169515346409753 :\n",
      "epoch 267 loss 9.82563360594213e-05 :\n",
      "epoch 268 loss 9.211397991748527e-05 :\n",
      "epoch 269 loss 0.00012195829913252965 :\n",
      "epoch 270 loss 0.00011308555258437991 :\n",
      "epoch 271 loss 8.994506788440049e-05 :\n",
      "epoch 272 loss 0.000101579847978428 :\n",
      "epoch 273 loss 0.00011725001968443394 :\n",
      "epoch 274 loss 9.313468035543337e-05 :\n",
      "epoch 275 loss 0.00012254471948836 :\n",
      "epoch 276 loss 0.00010369245865149423 :\n",
      "epoch 277 loss 0.00013803956971969455 :\n",
      "epoch 278 loss 9.857164695858955e-05 :\n",
      "epoch 279 loss 0.0001292811648454517 :\n",
      "epoch 280 loss 0.00012644840171560645 :\n",
      "epoch 281 loss 0.00011690740939229727 :\n",
      "epoch 282 loss 0.00013762502931058407 :\n",
      "epoch 283 loss 0.00010511543950997293 :\n",
      "epoch 284 loss 0.00013844484055880457 :\n",
      "epoch 285 loss 0.00014049626770429313 :\n",
      "epoch 286 loss 0.00012649496784433722 :\n",
      "epoch 287 loss 0.00011735099542420357 :\n",
      "epoch 288 loss 0.00010100191866513342 :\n",
      "epoch 289 loss 0.00010926436516456306 :\n",
      "epoch 290 loss 0.0001349973026663065 :\n",
      "epoch 291 loss 0.00015290586452465504 :\n",
      "epoch 292 loss 0.00011835718032671139 :\n",
      "epoch 293 loss 0.00011151736543979496 :\n",
      "epoch 294 loss 0.0001236399548361078 :\n",
      "epoch 295 loss 0.00010250003833789378 :\n",
      "epoch 296 loss 0.00011627504864009097 :\n",
      "epoch 297 loss 0.00012756104115396738 :\n",
      "epoch 298 loss 0.00011019985686289147 :\n",
      "epoch 299 loss 0.00011225664638914168 :\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model1 = RBFnet(input_cause.size()[0], 1, input_cause.size()[1])\n",
    "train_rbf(model1, input_cause, input_target, Y, 0.01, 300, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "48b93715",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.5548, device='cuda:0', grad_fn=<MeanBackward0>)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.mean(model1.networks[1].init_weight_cause)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "add15474",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
