{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1403279f",
   "metadata": {},
   "source": [
    "# advanced CNN\n",
    "\n",
    "# AlexNet\n",
    " - first use Relu\n",
    " - norm layers\n",
    " - drop out\n",
    " - SGD\n",
    " - 7 cnn -> ensemble\n",
    "\n",
    "# VGGNet\n",
    "  - filter를 더 작게 하여 여러번 해준다.\n",
    "  - ex) 7 * 7을 한번하는 것 보다 3 * 3을 여러번 한다.\n",
    "     - 같은 receptive filter를 추출해도 param의 개수가 적다\n",
    "\n",
    "\n",
    "# googlenet\n",
    "\n",
    " - inception module 을 사용\n",
    "   - 한 layer에 동시에 여러 filter를 사용\n",
    "   - 그후 모든 결과를 concatenate한다.\n",
    "   - Bottle neck layer를 이용하여 input의 dense를 줄어 연산량을 줄인다.\n",
    "      - 인위적으로 1 * 1 conv fiter 32개를 씌어 dense를 줄임\n",
    "\n",
    " - no fc layers : computational efficieny를 높임 마지막에서만 사용 \n",
    "\n",
    "\n",
    " # ResNet\n",
    "  - very deep network using residual connections\n",
    "  - 깊은 뉴런이 overfitting 문제는 있어도 train에 대해서는 더욱 잘 맞추어야 하지만, 그렇지 못하는 문제가 발생했다.\n",
    "  - otimization의 문제로 확인됨.\n",
    "  - deep network -> problem -> residual connections\n",
    "  - residual connections\n",
    "    - input x의 값을 output f(x)에 값해 더해준다.\n",
    "    - 즉, 현재의 정보도 기억하면서 학습을 진행하면, layer가 깊어져도 학습이 잘진해이 될것이라고 판단한다.\n",
    "    - googlenet과 동일하게 bottle nect을 이용하여 연산량을 줄여준다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dad3bf49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ResNet "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "606be6c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import argparse\n",
    "import numpy as np\n",
    "import time\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from copy import deepcopy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1164cb0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv3x3(in_planes,out_planes, stride =1):\n",
    "    return nn.Conv2d(inplanes, out_plances, kernel_size = 3, stride = stride,\n",
    "                   padding = 1, bias = False)\n",
    "  \n",
    "def conv1x1(in_planes,out_planes, stride =1):\n",
    "    return nn.Conv2d(inplanes, out_plances, kernel_size = 1, stride = stride,\n",
    "                   padding = 1, bias = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d8cf0da9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 50개 이상의 layer를 가진 ResNet architecture에서 computational efficiency를 증가시키기 위해,\n",
    "# 3x3 convolution layer 앞뒤로 1x1 convolution layer를 추가한 Bottleneck module을 구현\n",
    "class BasicBlock(nn.Module):\n",
    "    expansion = 1\n",
    "\n",
    "    def __init__(self, in_planes, planes, stride = 1, downsample = None):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        self.conv1 = conv3x3(in_planes, planes, stride)\n",
    "        self.bn = nn.BatchNorm2d(planes)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.downsample = downsample # 차원이 바뀔때만 사용 마지막 input x+ out일 때\n",
    "        self.conv2 = conv3x3(planes, planes)\n",
    "        self.stride = stride\n",
    "\n",
    "    def forward(self,x):\n",
    "        identity = x\n",
    "\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn(out)\n",
    "    \n",
    "        if self.downsample is not None:\n",
    "            identity = self.downsample(x)\n",
    "\n",
    "        out += identity\n",
    "        out = self.relu(out)\n",
    "        return out\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4e56f219",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Bottleneck(nn.Module):\n",
    "    expansion = 4\n",
    "\n",
    "    def __init__(self, inplanes, planes, stride=1, downsample=None):\n",
    "        super(Bottleneck, self).__init__()\n",
    "        self.conv1 = conv1x1(inplanes, planes)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        self.conv2 = conv3x3(planes, planes, stride)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "        self.conv3 = conv1x1(planes, planes * self.expansion)\n",
    "        self.bn3 = nn.BatchNorm2d(planes * self.expansion)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.downsample = downsample\n",
    "        self.stride = stride\n",
    "\n",
    "    def forward(self, x):\n",
    "        identity = x\n",
    "\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv3(out)\n",
    "        out = self.bn3(out)\n",
    "\n",
    "        if self.downsample is not None:\n",
    "            identity = self.downsample(x)\n",
    "\n",
    "        out += identity\n",
    "        out = self.relu(out)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1a3ca8db",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNet(nn.Module):\n",
    "\n",
    "    def __init__(self, block, layers, num_classes=1000, zero_init_residual=False):\n",
    "        super(ResNet, self).__init__()\n",
    "        self.inplanes = 64\n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3,\n",
    "                               bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "        self.layer1 = self._make_layer(block, 64, layers[0])\n",
    "        self.layer2 = self._make_layer(block, 128, layers[1], stride=2)\n",
    "        self.layer3 = self._make_layer(block, 256, layers[2], stride=2)\n",
    "        self.layer4 = self._make_layer(block, 512, layers[3], stride=2)\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.fc = nn.Linear(512 * block.expansion, num_classes)\n",
    "\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "\n",
    "        # Zero-initialize the last BN in each residual branch,\n",
    "        # so that the residual branch starts with zeros, and each residual block behaves like an identity.\n",
    "        # This improves the model by 0.2~0.3% according to https://arxiv.org/abs/1706.02677\n",
    "        if zero_init_residual:\n",
    "            for m in self.modules():\n",
    "                if isinstance(m, Bottleneck):\n",
    "                    nn.init.constant_(m.bn3.weight, 0)\n",
    "                elif isinstance(m, BasicBlock):\n",
    "                    nn.init.constant_(m.bn2.weight, 0)\n",
    "\n",
    "    def _make_layer(self, block, planes, blocks, stride=1):\n",
    "        downsample = None\n",
    "        if stride != 1 or self.inplanes != planes * block.expansion:\n",
    "            downsample = nn.Sequential(\n",
    "                conv1x1(self.inplanes, planes * block.expansion, stride),\n",
    "                nn.BatchNorm2d(planes * block.expansion),\n",
    "            )\n",
    "\n",
    "        layers = []\n",
    "        layers.append(block(self.inplanes, planes, stride, downsample))\n",
    "        self.inplanes = planes * block.expansion\n",
    "        for _ in range(1, blocks):\n",
    "            layers.append(block(self.inplanes, planes))\n",
    "\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.maxpool(x)\n",
    "\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "\n",
    "        x = self.avgpool(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62fab651",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
