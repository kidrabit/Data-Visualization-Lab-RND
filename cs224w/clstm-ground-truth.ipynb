{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "00aa5201",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.integrate import odeint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "17e61327",
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulate_lorenz_96(p, T, F=10.0, delta_t=0.1, sd=0.1, burn_in=1000,\n",
    "                       seed=0):\n",
    "    if seed is not None:\n",
    "        np.random.seed(seed)\n",
    "\n",
    "    # Use scipy to solve ODE.\n",
    "    x0 = np.random.normal(scale=0.01, size=p)\n",
    "    t = np.linspace(0, (T + burn_in) * delta_t, T + burn_in)\n",
    "    X = odeint(lorenz, x0, t, args=(F,))\n",
    "    X += np.random.normal(scale=sd, size=(T + burn_in, p))\n",
    "\n",
    "    # Set up Granger causality ground truth.\n",
    "    GC = np.zeros((p, p), dtype=int)\n",
    "    for i in range(p):\n",
    "        GC[i, i] = 1\n",
    "        GC[i, (i + 1) % p] = 1\n",
    "        GC[i, (i - 1) % p] = 1\n",
    "        GC[i, (i - 2) % p] = 1\n",
    "\n",
    "    return X[burn_in:], GC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a096cc0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "  \n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from copy import deepcopy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "14f4ebf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model_ista(clstm, X, context, lr, max_iter, lam=0, lam_ridge=0,\n",
    "                     lookback=5, check_every=50, verbose=1):\n",
    "    '''Train model with Adam.'''\n",
    "    p = X.shape[-1]\n",
    "    loss_fn = nn.MSELoss(reduction='mean')\n",
    "    train_loss_list = []\n",
    "\n",
    "    # Set up data.\n",
    "    X, Y = zip(*[arrange_input(x, context) for x in X])\n",
    "    X = torch.cat(X, dim=0)\n",
    "    Y = torch.cat(Y, dim=0)\n",
    "\n",
    "    # For early stopping.\n",
    "    best_it = None\n",
    "    best_loss = np.inf\n",
    "    best_model = None\n",
    "\n",
    "    # Calculate smooth error.\n",
    "    pred = [clstm.networks[i](X)[0] for i in range(p)]\n",
    "    loss = sum([loss_fn(pred[i][:, :, 0], Y[:, :, i]) for i in range(p)])\n",
    "    ridge = sum([ridge_regularize(net, lam_ridge) for net in clstm.networks])\n",
    "    smooth = loss + ridge\n",
    "\n",
    "    for it in range(max_iter):\n",
    "        # Take gradient step.\n",
    "        smooth.backward()\n",
    "        for param in clstm.parameters():\n",
    "            param.data -= lr * param.grad\n",
    "\n",
    "        # Take prox step.\n",
    "        if lam > 0:\n",
    "            for net in clstm.networks:\n",
    "                prox_update(net, lam, lr)\n",
    "\n",
    "        clstm.zero_grad()\n",
    "\n",
    "        # Calculate loss for next iteration.\n",
    "        pred = [clstm.networks[i](X)[0] for i in range(p)]\n",
    "        loss = sum([loss_fn(pred[i][:, :, 0], Y[:, :, i]) for i in range(p)])\n",
    "        ridge = sum([ridge_regularize(net, lam_ridge)\n",
    "                     for net in clstm.networks])\n",
    "        smooth = loss + ridge\n",
    "\n",
    "        # Check progress.\n",
    "        if (it + 1) % check_every == 0:\n",
    "            # Add nonsmooth penalty.\n",
    "            nonsmooth = sum([regularize(net, lam) for net in clstm.networks])\n",
    "            mean_loss = (smooth + nonsmooth) / p\n",
    "            train_loss_list.append(mean_loss.detach())\n",
    "\n",
    "            if verbose > 0:\n",
    "                print(('-' * 10 + 'Iter = %d' + '-' * 10) % (it + 1))\n",
    "                print('Loss = %f' % mean_loss)\n",
    "                print('Variable usage = %.2f%%'\n",
    "                      % (100 * torch.mean(clstm.GC().float())))\n",
    "\n",
    "            # Check for early stopping.\n",
    "            if mean_loss < best_loss:\n",
    "                best_loss = mean_loss\n",
    "                best_it = it\n",
    "                best_model = deepcopy(clstm)\n",
    "            elif (it - best_it) == lookback * check_every:\n",
    "                if verbose:\n",
    "                    print('Stopping early')\n",
    "                break\n",
    "\n",
    "    # Restore best model.\n",
    "    restore_parameters(clstm, best_model)\n",
    "\n",
    "    return train_loss_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0fb02376",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lorenz(x, t, F):\n",
    "    '''Partial derivatives for Lorenz-96 ODE.'''\n",
    "    p = len(x)\n",
    "    dxdt = np.zeros(p)\n",
    "    for i in range(p):\n",
    "        dxdt[i] = (x[(i+1) % p] - x[(i-2) % p]) * x[(i-1) % p] - x[i] + F\n",
    "\n",
    "    return dxdt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "91f3a98c",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda')\n",
    "X_np, GC = simulate_lorenz_96(p=10, F=10, T=1000)\n",
    "X = torch.tensor(X_np[np.newaxis], dtype=torch.float32, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4bbcf9ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.09901663  2.90255933  7.90956947 ...  7.58412067 -0.87003867\n",
      "   0.61771009]\n",
      " [ 2.38325721  4.35670602  7.5971257  ...  4.76156927 -4.74652638\n",
      "   2.97870363]\n",
      " [ 6.12371096  5.58656138  3.61239589 ...  2.99557554 -5.32498916\n",
      "   3.21535708]\n",
      " ...\n",
      " [-0.80381534  1.65664602  1.62185873 ... 10.65667465  2.78380341\n",
      "  -3.29308802]\n",
      " [ 0.19956412  2.45640257  3.63145308 ... 10.67337297 -0.5973428\n",
      "  -3.51307331]\n",
      " [ 0.20966461  3.01812589  5.86000539 ... 10.10419812 -3.20311821\n",
      "  -0.30507416]]\n",
      "[[1 1 0 0 0 0 0 0 1 1]\n",
      " [1 1 1 0 0 0 0 0 0 1]\n",
      " [1 1 1 1 0 0 0 0 0 0]\n",
      " [0 1 1 1 1 0 0 0 0 0]\n",
      " [0 0 1 1 1 1 0 0 0 0]\n",
      " [0 0 0 1 1 1 1 0 0 0]\n",
      " [0 0 0 0 1 1 1 1 0 0]\n",
      " [0 0 0 0 0 1 1 1 1 0]\n",
      " [0 0 0 0 0 0 1 1 1 1]\n",
      " [1 0 0 0 0 0 0 1 1 1]]\n"
     ]
    }
   ],
   "source": [
    "print(X_np)\n",
    "print(GC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e381ce38",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM(nn.Module):\n",
    "    def __init__(self, num_series, hidden):\n",
    "        '''\n",
    "        LSTM model with output layer to generate predictions.\n",
    "        Args:\n",
    "          num_series: number of input time series.\n",
    "          hidden: number of hidden units.\n",
    "        '''\n",
    "        super(LSTM, self).__init__()\n",
    "        self.p = num_series\n",
    "        self.hidden = hidden\n",
    "\n",
    "        # Set up network.\n",
    "        self.lstm = nn.LSTM(num_series, hidden, batch_first=True)\n",
    "        self.lstm.flatten_parameters()\n",
    "        self.linear = nn.Conv1d(hidden, 1, 1)\n",
    "\n",
    "    def init_hidden(self, batch):\n",
    "        '''Initialize hidden states for LSTM cell.'''\n",
    "        device = self.lstm.weight_ih_l0.device\n",
    "        return (torch.zeros(1, batch, self.hidden, device=device),\n",
    "                torch.zeros(1, batch, self.hidden, device=device))\n",
    "\n",
    "    def forward(self, X, hidden=None):\n",
    "        # Set up hidden state.\n",
    "        if hidden is None:\n",
    "            hidden = self.init_hidden(X.shape[0])\n",
    "\n",
    "        # Apply LSTM.\n",
    "        X, hidden = self.lstm(X, hidden)\n",
    "\n",
    "        # Calculate predictions using output layer.\n",
    "        X = X.transpose(2, 1)\n",
    "        X = self.linear(X)\n",
    "        return X.transpose(2, 1), hidden\n",
    "\n",
    "\n",
    "class cLSTM(nn.Module):\n",
    "    def __init__(self, num_series, hidden):\n",
    "        '''\n",
    "        cLSTM model with one LSTM per time series.\n",
    "        Args:\n",
    "          num_series: dimensionality of multivariate time series.\n",
    "          hidden: number of units in LSTM cell.\n",
    "        '''\n",
    "        super(cLSTM, self).__init__()\n",
    "        self.p = num_series\n",
    "        self.hidden = hidden\n",
    "\n",
    "        # Set up networks.\n",
    "        self.networks = nn.ModuleList([\n",
    "            LSTM(num_series, hidden) for _ in range(num_series)])\n",
    "\n",
    "    def forward(self, X, hidden=None):\n",
    "        '''\n",
    "        Perform forward pass.\n",
    "        Args:\n",
    "          X: torch tensor of shape (batch, T, p).\n",
    "          hidden: hidden states for LSTM cell.\n",
    "        '''\n",
    "        if hidden is None:\n",
    "            hidden = [None for _ in range(self.p)]\n",
    "        pred = [self.networks[i](X, hidden[i])\n",
    "                for i in range(self.p)]\n",
    "        pred, hidden = zip(*pred)\n",
    "        pred = torch.cat(pred, dim=2)\n",
    "        return pred, hidden\n",
    "\n",
    "    def GC(self, threshold=True):\n",
    "        '''\n",
    "        Extract learned Granger causality.\n",
    "        Args:\n",
    "          threshold: return norm of weights, or whether norm is nonzero.\n",
    "        Returns:\n",
    "          GC: (p x p) matrix. Entry (i, j) indicates whether variable j is\n",
    "            Granger causal of variable i.\n",
    "        '''\n",
    "        GC = [torch.norm(net.lstm.weight_ih_l0, dim=0)\n",
    "              for net in self.networks]\n",
    "        GC = torch.stack(GC)\n",
    "        if threshold:\n",
    "            return (GC > 0).int()\n",
    "        else:\n",
    "            return GC\n",
    "\n",
    "\n",
    "class cLSTMSparse(nn.Module):\n",
    "    def __init__(self, num_series, sparsity, hidden):\n",
    "        '''\n",
    "        cLSTM model that only uses specified interactions.\n",
    "        Args:\n",
    "          num_series: dimensionality of multivariate time series.\n",
    "          sparsity: torch byte tensor indicating Granger causality, with size\n",
    "            (num_series, num_series).\n",
    "          hidden: number of units in LSTM cell.\n",
    "        '''\n",
    "        super(cLSTMSparse, self).__init__()\n",
    "        self.p = num_series\n",
    "        self.hidden = hidden\n",
    "        self.sparsity = sparsity\n",
    "\n",
    "        # Set up networks.\n",
    "        self.networks = nn.ModuleList([\n",
    "            LSTM(int(torch.sum(sparsity[i].int())), hidden)\n",
    "            for i in range(num_series)])\n",
    "\n",
    "    def forward(self, X, hidden=None):\n",
    "        '''\n",
    "        Perform forward pass.\n",
    "        Args:\n",
    "          X: torch tensor of shape (batch, T, p).\n",
    "          hidden: hidden states for LSTM cell.\n",
    "        '''\n",
    "        if hidden is None:\n",
    "            hidden = [None for _ in range(self.p)]\n",
    "        pred = [self.networks[i](X[:, :, self.sparsity[i]], hidden[i])\n",
    "                for i in range(self.p)]\n",
    "        pred, hidden = zip(*pred)\n",
    "        pred = torch.cat(pred, dim=2)\n",
    "        return pred, hidden\n",
    "\n",
    "\n",
    "def prox_update(network, lam, lr):\n",
    "    '''Perform in place proximal update on first layer weight matrix.'''\n",
    "    W = network.lstm.weight_ih_l0\n",
    "    norm = torch.norm(W, dim=0, keepdim=True)\n",
    "    W.data = ((W / torch.clamp(norm, min=(lam * lr)))\n",
    "              * torch.clamp(norm - (lr * lam), min=0.0))\n",
    "    network.lstm.flatten_parameters()\n",
    "\n",
    "\n",
    "def regularize(network, lam):\n",
    "    '''Calculate regularization term for first layer weight matrix.'''\n",
    "    W = network.lstm.weight_ih_l0\n",
    "    return lam * torch.sum(torch.norm(W, dim=0))\n",
    "\n",
    "\n",
    "def ridge_regularize(network, lam):\n",
    "    '''Apply ridge penalty at linear layer and hidden-hidden weights.'''\n",
    "    return lam * (\n",
    "        torch.sum(network.linear.weight ** 2) +\n",
    "        torch.sum(network.lstm.weight_hh_l0 ** 2))\n",
    "\n",
    "\n",
    "def restore_parameters(model, best_model):\n",
    "    '''Move parameter values from best_model to model.'''\n",
    "    for params, best_params in zip(model.parameters(), best_model.parameters()):\n",
    "        params.data = best_params\n",
    "\n",
    "\n",
    "def arrange_input(data, context):\n",
    "    '''\n",
    "    Arrange a single time series into overlapping short sequences.\n",
    "    Args:\n",
    "      data: time series of shape (T, dim).\n",
    "      context: length of short sequences.\n",
    "    '''\n",
    "    assert context >= 1 and isinstance(context, int)\n",
    "    input = torch.zeros(len(data) - context, context, data.shape[1],\n",
    "                        dtype=torch.float32, device=data.device)\n",
    "    target = torch.zeros(len(data) - context, context, data.shape[1],\n",
    "                         dtype=torch.float32, device=data.device)\n",
    "    for i in range(context):\n",
    "        start = i\n",
    "        end = len(data) - context + i\n",
    "        input[:, i, :] = data[start:end]\n",
    "        target[:, i, :] = data[start+1:end+1]\n",
    "    return input.detach(), target.detach()\n",
    "\n",
    "\n",
    "def train_model_gista(clstm, X, context, lam, lam_ridge, lr, max_iter,\n",
    "                      check_every=50, r=0.8, lr_min=1e-8, sigma=0.5,\n",
    "                      monotone=False, m=10, lr_decay=0.5,\n",
    "                      begin_line_search=True, switch_tol=1e-3, verbose=1):\n",
    "    '''\n",
    "    Train cLSTM model with GISTA.\n",
    "    Args:\n",
    "      clstm: clstm model.\n",
    "      X: tensor of data, shape (batch, T, p).\n",
    "      context: length for short overlapping subsequences.\n",
    "      lam: parameter for nonsmooth regularization.\n",
    "      lam_ridge: parameter for ridge regularization on output layer.\n",
    "      lr: learning rate.\n",
    "      max_iter: max number of GISTA iterations.\n",
    "      check_every: how frequently to record loss.\n",
    "      r: for line search.\n",
    "      lr_min: for line search.\n",
    "      sigma: for line search.\n",
    "      monotone: for line search.\n",
    "      m: for line search.\n",
    "      lr_decay: for adjusting initial learning rate of line search.\n",
    "      begin_line_search: whether to begin with line search.\n",
    "      switch_tol: tolerance for switching to line search.\n",
    "      verbose: level of verbosity (0, 1, 2).\n",
    "    '''\n",
    "    p = clstm.p\n",
    "    clstm_copy = deepcopy(clstm)\n",
    "    loss_fn = nn.MSELoss(reduction='mean')\n",
    "    lr_list = [lr for _ in range(p)]\n",
    "\n",
    "    # Set up data.\n",
    "    X, Y = zip(*[arrange_input(x, context) for x in X])\n",
    "    X = torch.cat(X, dim=0)\n",
    "    Y = torch.cat(Y, dim=0)\n",
    "\n",
    "    # Calculate full loss.\n",
    "    mse_list = []\n",
    "    smooth_list = []\n",
    "    loss_list = []\n",
    "    for i in range(p):\n",
    "        net = clstm.networks[i]\n",
    "        pred, _ = net(X)\n",
    "        mse = loss_fn(pred[:, :, 0], Y[:, :, i])\n",
    "        ridge = ridge_regularize(net, lam_ridge)\n",
    "        smooth = mse + ridge\n",
    "        mse_list.append(mse)\n",
    "        smooth_list.append(smooth)\n",
    "        with torch.no_grad():\n",
    "            nonsmooth = regularize(net, lam)\n",
    "            loss = smooth + nonsmooth\n",
    "            loss_list.append(loss)\n",
    "\n",
    "    # Set up lists for loss and mse.\n",
    "    with torch.no_grad():\n",
    "        loss_mean = sum(loss_list) / p\n",
    "        mse_mean = sum(mse_list) / p\n",
    "    train_loss_list = [loss_mean]\n",
    "    train_mse_list = [mse_mean]\n",
    "\n",
    "    # For switching to line search.\n",
    "    line_search = begin_line_search\n",
    "\n",
    "    # For line search criterion.\n",
    "    done = [False for _ in range(p)]\n",
    "    assert 0 < sigma <= 1\n",
    "    assert m > 0\n",
    "    if not monotone:\n",
    "        last_losses = [[loss_list[i]] for i in range(p)]\n",
    "\n",
    "    for it in range(max_iter):\n",
    "        # Backpropagate errors.\n",
    "        sum([smooth_list[i] for i in range(p) if not done[i]]).backward()\n",
    "\n",
    "        # For next iteration.\n",
    "        new_mse_list = []\n",
    "        new_smooth_list = []\n",
    "        new_loss_list = []\n",
    "\n",
    "        # Perform GISTA step for each network.\n",
    "        for i in range(p):\n",
    "            # Skip if network converged.\n",
    "            if done[i]:\n",
    "                new_mse_list.append(mse_list[i])\n",
    "                new_smooth_list.append(smooth_list[i])\n",
    "                new_loss_list.append(loss_list[i])\n",
    "                continue\n",
    "\n",
    "            # Prepare for line search.\n",
    "            step = False\n",
    "            lr_it = lr_list[i]\n",
    "            net = clstm.networks[i]\n",
    "            net_copy = clstm_copy.networks[i]\n",
    "\n",
    "            while not step:\n",
    "                # Perform tentative ISTA step.\n",
    "                for param, temp_param in zip(net.parameters(),\n",
    "                                             net_copy.parameters()):\n",
    "                    temp_param.data = param - lr_it * param.grad\n",
    "\n",
    "                # Proximal update.\n",
    "                prox_update(net_copy, lam, lr_it)\n",
    "\n",
    "                # Check line search criterion.\n",
    "                pred, _ = net_copy(X)\n",
    "                mse = loss_fn(pred[:, :, 0], Y[:, :, i])\n",
    "                ridge = ridge_regularize(net_copy, lam_ridge)\n",
    "                smooth = mse + ridge\n",
    "                with torch.no_grad():\n",
    "                    nonsmooth = regularize(net_copy, lam)\n",
    "                    loss = smooth + nonsmooth\n",
    "                    tol = (0.5 * sigma / lr_it) * sum(\n",
    "                        [torch.sum((param - temp_param) ** 2)\n",
    "                         for param, temp_param in\n",
    "                         zip(net.parameters(), net_copy.parameters())])\n",
    "\n",
    "                comp = loss_list[i] if monotone else max(last_losses[i])\n",
    "                if not line_search or (comp - loss) > tol:\n",
    "                    step = True\n",
    "                    if verbose > 1:\n",
    "                        print('Taking step, network i = %d, lr = %f'\n",
    "                              % (i, lr_it))\n",
    "                        print('Gap = %f, tol = %f' % (comp - loss, tol))\n",
    "\n",
    "                    # For next iteration.\n",
    "                    new_mse_list.append(mse)\n",
    "                    new_smooth_list.append(smooth)\n",
    "                    new_loss_list.append(loss)\n",
    "\n",
    "                    # Adjust initial learning rate.\n",
    "                    lr_list[i] = (\n",
    "                        (lr_list[i] ** (1 - lr_decay)) * (lr_it ** lr_decay))\n",
    "\n",
    "                    if not monotone:\n",
    "                        if len(last_losses[i]) == m:\n",
    "                            last_losses[i].pop(0)\n",
    "                        last_losses[i].append(loss)\n",
    "                else:\n",
    "                    # Reduce learning rate.\n",
    "                    lr_it *= r\n",
    "                    if lr_it < lr_min:\n",
    "                        done[i] = True\n",
    "                        new_mse_list.append(mse_list[i])\n",
    "                        new_smooth_list.append(smooth_list[i])\n",
    "                        new_loss_list.append(loss_list[i])\n",
    "                        if verbose > 0:\n",
    "                            print('Network %d converged' % (i + 1))\n",
    "                        break\n",
    "\n",
    "            # Clean up.\n",
    "            net.zero_grad()\n",
    "\n",
    "            if step:\n",
    "                # Swap network parameters.\n",
    "                clstm.networks[i], clstm_copy.networks[i] = net_copy, net\n",
    "\n",
    "        # For next iteration.\n",
    "        mse_list = new_mse_list\n",
    "        smooth_list = new_smooth_list\n",
    "        loss_list = new_loss_list\n",
    "\n",
    "        # Check if all networks have converged.\n",
    "        if sum(done) == p:\n",
    "            if verbose > 0:\n",
    "                print('Done at iteration = %d' % (it + 1))\n",
    "            break\n",
    "\n",
    "        # Check progress\n",
    "        if (it + 1) % check_every == 0:\n",
    "            with torch.no_grad():\n",
    "                loss_mean = sum(loss_list) / p\n",
    "                mse_mean = sum(mse_list) / p\n",
    "                ridge_mean = (sum(smooth_list) - sum(mse_list)) / p\n",
    "                nonsmooth_mean = (sum(loss_list) - sum(smooth_list)) / p\n",
    "\n",
    "            train_loss_list.append(loss_mean)\n",
    "            train_mse_list.append(mse_mean)\n",
    "\n",
    "            if verbose > 0:\n",
    "                print(('-' * 10 + 'Iter = %d' + '-' * 10) % (it + 1))\n",
    "                print('Total loss = %f' % loss_mean)\n",
    "                print('MSE = %f, Ridge = %f, Nonsmooth = %f'\n",
    "                      % (mse_mean, ridge_mean, nonsmooth_mean))\n",
    "                print('Variable usage = %.2f%%'\n",
    "                      % (100 * torch.mean(clstm.GC().float())))\n",
    "\n",
    "            # Check whether loss has increased.\n",
    "            if not line_search:\n",
    "                if train_loss_list[-2] - train_loss_list[-1] < switch_tol:\n",
    "                    line_search = True\n",
    "                    if verbose > 0:\n",
    "                        print('Switching to line search')\n",
    "\n",
    "    return train_loss_list, train_mse_list\n",
    "\n",
    "\n",
    "def train_model_adam(clstm, X, context, lr, max_iter, lam=0, lam_ridge=0,\n",
    "                     lookback=5, check_every=50, verbose=1):\n",
    "    '''Train model with Adam.'''\n",
    "    p = X.shape[-1]\n",
    "    loss_fn = nn.MSELoss(reduction='mean')\n",
    "    optimizer = torch.optim.Adam(clstm.parameters(), lr=lr)\n",
    "    train_loss_list = []\n",
    "\n",
    "    # Set up data.\n",
    "    X, Y = zip(*[arrange_input(x, context) for x in X])\n",
    "    X = torch.cat(X, dim=0)\n",
    "    Y = torch.cat(Y, dim=0)\n",
    "\n",
    "    # For early stopping.\n",
    "    best_it = None\n",
    "    best_loss = np.inf\n",
    "    best_model = None\n",
    "\n",
    "    for it in range(max_iter):\n",
    "        # Calculate loss.\n",
    "        pred = [clstm.networks[i](X)[0] for i in range(p)]\n",
    "        loss = sum([loss_fn(pred[i][:, :, 0], Y[:, :, i]) for i in range(p)])\n",
    "\n",
    "        # Add penalty term.\n",
    "        if lam > 0:\n",
    "            loss = loss + sum([regularize(net, lam) for net in clstm.networks])\n",
    "\n",
    "        if lam_ridge > 0:\n",
    "            loss = loss + sum([ridge_regularize(net, lam_ridge)\n",
    "                               for net in clstm.networks])\n",
    "\n",
    "        # Take gradient step.\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        clstm.zero_grad()\n",
    "\n",
    "        # Check progress.\n",
    "        if (it + 1) % check_every == 0:\n",
    "            mean_loss = loss / p\n",
    "            train_loss_list.append(mean_loss.detach())\n",
    "\n",
    "            if verbose > 0:\n",
    "                print(('-' * 10 + 'Iter = %d' + '-' * 10) % (it + 1))\n",
    "                print('Loss = %f' % mean_loss)\n",
    "\n",
    "            # Check for early stopping.\n",
    "            if mean_loss < best_loss:\n",
    "                best_loss = mean_loss\n",
    "                best_it = it\n",
    "                best_model = deepcopy(clstm)\n",
    "            elif (it - best_it) == lookback * check_every:\n",
    "                if verbose:\n",
    "                    print('Stopping early')\n",
    "                break\n",
    "\n",
    "    # Restore best model.\n",
    "    restore_parameters(clstm, best_model)\n",
    "\n",
    "    return train_loss_list\n",
    "\n",
    "\n",
    "def train_model_ista(clstm, X, context, lr, max_iter, lam=0, lam_ridge=0,\n",
    "                     lookback=5, check_every=50, verbose=1):\n",
    "    '''Train model with Adam.'''\n",
    "    p = X.shape[-1]\n",
    "    loss_fn = nn.MSELoss(reduction='mean')\n",
    "    train_loss_list = []\n",
    "\n",
    "    # Set up data.\n",
    "    X, Y = zip(*[arrange_input(x, context) for x in X])\n",
    "    X = torch.cat(X, dim=0)\n",
    "    Y = torch.cat(Y, dim=0)\n",
    "\n",
    "    # For early stopping.\n",
    "    best_it = None\n",
    "    best_loss = np.inf\n",
    "    best_model = None\n",
    "\n",
    "    # Calculate smooth error.\n",
    "    pred = [clstm.networks[i](X)[0] for i in range(p)]\n",
    "    loss = sum([loss_fn(pred[i][:, :, 0], Y[:, :, i]) for i in range(p)])\n",
    "    ridge = sum([ridge_regularize(net, lam_ridge) for net in clstm.networks])\n",
    "    smooth = loss + ridge\n",
    "\n",
    "    for it in range(max_iter):\n",
    "        # Take gradient step.\n",
    "        smooth.backward()\n",
    "        for param in clstm.parameters():\n",
    "            param.data -= lr * param.grad\n",
    "\n",
    "        # Take prox step.\n",
    "        if lam > 0:\n",
    "            for net in clstm.networks:\n",
    "                prox_update(net, lam, lr)\n",
    "\n",
    "        clstm.zero_grad()\n",
    "\n",
    "        # Calculate loss for next iteration.\n",
    "        pred = [clstm.networks[i](X)[0] for i in range(p)]\n",
    "        loss = sum([loss_fn(pred[i][:, :, 0], Y[:, :, i]) for i in range(p)])\n",
    "        ridge = sum([ridge_regularize(net, lam_ridge)\n",
    "                     for net in clstm.networks])\n",
    "        smooth = loss + ridge\n",
    "\n",
    "        # Check progress.\n",
    "        if (it + 1) % check_every == 0:\n",
    "            # Add nonsmooth penalty.\n",
    "            nonsmooth = sum([regularize(net, lam) for net in clstm.networks])\n",
    "            mean_loss = (smooth + nonsmooth) / p\n",
    "            train_loss_list.append(mean_loss.detach())\n",
    "\n",
    "            if verbose > 0:\n",
    "                print(('-' * 10 + 'Iter = %d' + '-' * 10) % (it + 1))\n",
    "                print('Loss = %f' % mean_loss)\n",
    "                print('Variable usage = %.2f%%'\n",
    "                      % (100 * torch.mean(clstm.GC().float())))\n",
    "\n",
    "            # Check for early stopping.\n",
    "            if mean_loss < best_loss:\n",
    "                best_loss = mean_loss\n",
    "                best_it = it\n",
    "                best_model = deepcopy(clstm)\n",
    "            elif (it - best_it) == lookback * check_every:\n",
    "                if verbose:\n",
    "                    print('Stopping early')\n",
    "                break\n",
    "\n",
    "    # Restore best model.\n",
    "    restore_parameters(clstm, best_model)\n",
    "\n",
    "    return train_loss_list\n",
    "\n",
    "\n",
    "def train_unregularized(clstm, X, context, lr, max_iter, lookback=5,\n",
    "                        check_every=50, verbose=1):\n",
    "    '''Train model with Adam.'''\n",
    "    p = X.shape[-1]\n",
    "    loss_fn = nn.MSELoss(reduction='mean')\n",
    "    optimizer = torch.optim.Adam(clstm.parameters(), lr=lr)\n",
    "    train_loss_list = []\n",
    "\n",
    "    # Set up data.\n",
    "    X, Y = zip(*[arrange_input(x, context) for x in X])\n",
    "    X = torch.cat(X, dim=0)\n",
    "    Y = torch.cat(Y, dim=0)\n",
    "\n",
    "    # For early stopping.\n",
    "    best_it = None\n",
    "    best_loss = np.inf\n",
    "    best_model = None\n",
    "\n",
    "    for it in range(max_iter):\n",
    "        # Calculate loss.\n",
    "        pred, hidden = clstm(X)\n",
    "        loss = sum([loss_fn(pred[:, :, i], Y[:, :, i]) for i in range(p)])\n",
    "\n",
    "        # Take gradient step.\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        clstm.zero_grad()\n",
    "\n",
    "        # Check progress.\n",
    "        if (it + 1) % check_every == 0:\n",
    "            mean_loss = loss / p\n",
    "            train_loss_list.append(mean_loss.detach())\n",
    "\n",
    "            if verbose > 0:\n",
    "                print(('-' * 10 + 'Iter = %d' + '-' * 10) % (it + 1))\n",
    "                print('Loss = %f' % mean_loss)\n",
    "\n",
    "            # Check for early stopping.\n",
    "            if mean_loss < best_loss:\n",
    "                best_loss = mean_loss\n",
    "                best_it = it\n",
    "                best_model = deepcopy(clstm)\n",
    "            elif (it - best_it) == lookback * check_every:\n",
    "                if verbose:\n",
    "                    print('Stopping early')\n",
    "                break\n",
    "\n",
    "    # Restore best model.\n",
    "    restore_parameters(clstm, best_model)\n",
    "\n",
    "    return train_loss_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ca244d98",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------Iter = 50----------\n",
      "Loss = 86.845192\n",
      "Variable usage = 100.00%\n",
      "----------Iter = 100----------\n",
      "Loss = 38.276245\n",
      "Variable usage = 100.00%\n",
      "----------Iter = 150----------\n",
      "Loss = 21.874128\n",
      "Variable usage = 22.00%\n",
      "----------Iter = 200----------\n",
      "Loss = 20.476919\n",
      "Variable usage = 17.00%\n",
      "----------Iter = 250----------\n",
      "Loss = 19.647699\n",
      "Variable usage = 15.00%\n",
      "----------Iter = 300----------\n",
      "Loss = 19.091482\n",
      "Variable usage = 15.00%\n",
      "----------Iter = 350----------\n",
      "Loss = 18.683994\n",
      "Variable usage = 15.00%\n",
      "----------Iter = 400----------\n",
      "Loss = 18.359785\n",
      "Variable usage = 17.00%\n",
      "----------Iter = 450----------\n",
      "Loss = 18.079021\n",
      "Variable usage = 20.00%\n",
      "----------Iter = 500----------\n",
      "Loss = 17.819624\n",
      "Variable usage = 25.00%\n",
      "----------Iter = 550----------\n",
      "Loss = 17.571495\n",
      "Variable usage = 27.00%\n",
      "----------Iter = 600----------\n",
      "Loss = 17.330166\n",
      "Variable usage = 28.00%\n",
      "----------Iter = 650----------\n",
      "Loss = 17.094746\n",
      "Variable usage = 28.00%\n",
      "----------Iter = 700----------\n",
      "Loss = 16.865398\n",
      "Variable usage = 31.00%\n",
      "----------Iter = 750----------\n",
      "Loss = 16.642033\n",
      "Variable usage = 31.00%\n",
      "----------Iter = 800----------\n",
      "Loss = 16.424753\n",
      "Variable usage = 34.00%\n",
      "----------Iter = 850----------\n",
      "Loss = 16.213369\n",
      "Variable usage = 35.00%\n",
      "----------Iter = 900----------\n",
      "Loss = 16.007740\n",
      "Variable usage = 35.00%\n",
      "----------Iter = 950----------\n",
      "Loss = 15.807860\n",
      "Variable usage = 35.00%\n",
      "----------Iter = 1000----------\n",
      "Loss = 15.613675\n",
      "Variable usage = 35.00%\n",
      "----------Iter = 1050----------\n",
      "Loss = 15.425130\n",
      "Variable usage = 36.00%\n",
      "----------Iter = 1100----------\n",
      "Loss = 15.242129\n",
      "Variable usage = 36.00%\n",
      "----------Iter = 1150----------\n",
      "Loss = 15.064572\n",
      "Variable usage = 36.00%\n",
      "----------Iter = 1200----------\n",
      "Loss = 14.892395\n",
      "Variable usage = 36.00%\n",
      "----------Iter = 1250----------\n",
      "Loss = 14.725501\n",
      "Variable usage = 36.00%\n",
      "----------Iter = 1300----------\n",
      "Loss = 14.563785\n",
      "Variable usage = 36.00%\n",
      "----------Iter = 1350----------\n",
      "Loss = 14.407178\n",
      "Variable usage = 36.00%\n",
      "----------Iter = 1400----------\n",
      "Loss = 14.255561\n",
      "Variable usage = 36.00%\n",
      "----------Iter = 1450----------\n",
      "Loss = 14.108841\n",
      "Variable usage = 36.00%\n",
      "----------Iter = 1500----------\n",
      "Loss = 13.966907\n",
      "Variable usage = 36.00%\n",
      "----------Iter = 1550----------\n",
      "Loss = 13.829631\n",
      "Variable usage = 36.00%\n",
      "----------Iter = 1600----------\n",
      "Loss = 13.696898\n",
      "Variable usage = 36.00%\n",
      "----------Iter = 1650----------\n",
      "Loss = 13.568549\n",
      "Variable usage = 36.00%\n",
      "----------Iter = 1700----------\n",
      "Loss = 13.444455\n",
      "Variable usage = 36.00%\n",
      "----------Iter = 1750----------\n",
      "Loss = 13.324468\n",
      "Variable usage = 36.00%\n",
      "----------Iter = 1800----------\n",
      "Loss = 13.208426\n",
      "Variable usage = 36.00%\n",
      "----------Iter = 1850----------\n",
      "Loss = 13.096168\n",
      "Variable usage = 36.00%\n",
      "----------Iter = 1900----------\n",
      "Loss = 12.987540\n",
      "Variable usage = 36.00%\n",
      "----------Iter = 1950----------\n",
      "Loss = 12.882380\n",
      "Variable usage = 36.00%\n",
      "----------Iter = 2000----------\n",
      "Loss = 12.780525\n",
      "Variable usage = 36.00%\n",
      "----------Iter = 2050----------\n",
      "Loss = 12.681821\n",
      "Variable usage = 36.00%\n",
      "----------Iter = 2100----------\n",
      "Loss = 12.586107\n",
      "Variable usage = 36.00%\n",
      "----------Iter = 2150----------\n",
      "Loss = 12.493248\n",
      "Variable usage = 36.00%\n",
      "----------Iter = 2200----------\n",
      "Loss = 12.403096\n",
      "Variable usage = 36.00%\n",
      "----------Iter = 2250----------\n",
      "Loss = 12.315501\n",
      "Variable usage = 36.00%\n",
      "----------Iter = 2300----------\n",
      "Loss = 12.230342\n",
      "Variable usage = 36.00%\n",
      "----------Iter = 2350----------\n",
      "Loss = 12.147501\n",
      "Variable usage = 36.00%\n",
      "----------Iter = 2400----------\n",
      "Loss = 12.066834\n",
      "Variable usage = 36.00%\n",
      "----------Iter = 2450----------\n",
      "Loss = 11.988251\n",
      "Variable usage = 37.00%\n",
      "----------Iter = 2500----------\n",
      "Loss = 11.911644\n",
      "Variable usage = 37.00%\n",
      "----------Iter = 2550----------\n",
      "Loss = 11.836910\n",
      "Variable usage = 37.00%\n",
      "----------Iter = 2600----------\n",
      "Loss = 11.763949\n",
      "Variable usage = 37.00%\n",
      "----------Iter = 2650----------\n",
      "Loss = 11.692691\n",
      "Variable usage = 37.00%\n",
      "----------Iter = 2700----------\n",
      "Loss = 11.623034\n",
      "Variable usage = 37.00%\n",
      "----------Iter = 2750----------\n",
      "Loss = 11.554919\n",
      "Variable usage = 37.00%\n",
      "----------Iter = 2800----------\n",
      "Loss = 11.488266\n",
      "Variable usage = 37.00%\n",
      "----------Iter = 2850----------\n",
      "Loss = 11.422998\n",
      "Variable usage = 37.00%\n",
      "----------Iter = 2900----------\n",
      "Loss = 11.359083\n",
      "Variable usage = 37.00%\n",
      "----------Iter = 2950----------\n",
      "Loss = 11.296432\n",
      "Variable usage = 37.00%\n",
      "----------Iter = 3000----------\n",
      "Loss = 11.235015\n",
      "Variable usage = 37.00%\n",
      "----------Iter = 3050----------\n",
      "Loss = 11.174771\n",
      "Variable usage = 37.00%\n",
      "----------Iter = 3100----------\n",
      "Loss = 11.115641\n",
      "Variable usage = 37.00%\n",
      "----------Iter = 3150----------\n",
      "Loss = 11.057604\n",
      "Variable usage = 37.00%\n",
      "----------Iter = 3200----------\n",
      "Loss = 11.000602\n",
      "Variable usage = 37.00%\n",
      "----------Iter = 3250----------\n",
      "Loss = 10.944615\n",
      "Variable usage = 37.00%\n",
      "----------Iter = 3300----------\n",
      "Loss = 10.889590\n",
      "Variable usage = 37.00%\n",
      "----------Iter = 3350----------\n",
      "Loss = 10.835505\n",
      "Variable usage = 37.00%\n",
      "----------Iter = 3400----------\n",
      "Loss = 10.782332\n",
      "Variable usage = 37.00%\n",
      "----------Iter = 3450----------\n",
      "Loss = 10.730017\n",
      "Variable usage = 37.00%\n",
      "----------Iter = 3500----------\n",
      "Loss = 10.678566\n",
      "Variable usage = 36.00%\n",
      "----------Iter = 3550----------\n",
      "Loss = 10.627937\n",
      "Variable usage = 36.00%\n",
      "----------Iter = 3600----------\n",
      "Loss = 10.578102\n",
      "Variable usage = 36.00%\n",
      "----------Iter = 3650----------\n",
      "Loss = 10.529053\n",
      "Variable usage = 36.00%\n",
      "----------Iter = 3700----------\n",
      "Loss = 10.480756\n",
      "Variable usage = 36.00%\n",
      "----------Iter = 3750----------\n",
      "Loss = 10.433200\n",
      "Variable usage = 36.00%\n",
      "----------Iter = 3800----------\n",
      "Loss = 10.386363\n",
      "Variable usage = 36.00%\n",
      "----------Iter = 3850----------\n",
      "Loss = 10.340220\n",
      "Variable usage = 36.00%\n",
      "----------Iter = 3900----------\n",
      "Loss = 10.294762\n",
      "Variable usage = 36.00%\n",
      "----------Iter = 3950----------\n",
      "Loss = 10.249976\n",
      "Variable usage = 36.00%\n",
      "----------Iter = 4000----------\n",
      "Loss = 10.205846\n",
      "Variable usage = 36.00%\n",
      "----------Iter = 4050----------\n",
      "Loss = 10.162351\n",
      "Variable usage = 36.00%\n",
      "----------Iter = 4100----------\n",
      "Loss = 10.119475\n",
      "Variable usage = 36.00%\n",
      "----------Iter = 4150----------\n",
      "Loss = 10.077219\n",
      "Variable usage = 36.00%\n",
      "----------Iter = 4200----------\n",
      "Loss = 10.035556\n",
      "Variable usage = 36.00%\n",
      "----------Iter = 4250----------\n",
      "Loss = 9.994479\n",
      "Variable usage = 36.00%\n",
      "----------Iter = 4300----------\n",
      "Loss = 9.953981\n",
      "Variable usage = 36.00%\n",
      "----------Iter = 4350----------\n",
      "Loss = 9.914042\n",
      "Variable usage = 36.00%\n",
      "----------Iter = 4400----------\n",
      "Loss = 9.874656\n",
      "Variable usage = 36.00%\n",
      "----------Iter = 4450----------\n",
      "Loss = 9.835814\n",
      "Variable usage = 36.00%\n",
      "----------Iter = 4500----------\n",
      "Loss = 9.797507\n",
      "Variable usage = 36.00%\n",
      "----------Iter = 4550----------\n",
      "Loss = 9.759719\n",
      "Variable usage = 36.00%\n",
      "----------Iter = 4600----------\n",
      "Loss = 9.722441\n",
      "Variable usage = 36.00%\n",
      "----------Iter = 4650----------\n",
      "Loss = 9.685675\n",
      "Variable usage = 36.00%\n",
      "----------Iter = 4700----------\n",
      "Loss = 9.649397\n",
      "Variable usage = 36.00%\n",
      "----------Iter = 4750----------\n",
      "Loss = 9.613604\n",
      "Variable usage = 36.00%\n",
      "----------Iter = 4800----------\n",
      "Loss = 9.578290\n",
      "Variable usage = 36.00%\n",
      "----------Iter = 4850----------\n",
      "Loss = 9.543441\n",
      "Variable usage = 36.00%\n",
      "----------Iter = 4900----------\n",
      "Loss = 9.509067\n",
      "Variable usage = 36.00%\n",
      "----------Iter = 4950----------\n",
      "Loss = 9.475123\n",
      "Variable usage = 36.00%\n",
      "----------Iter = 5000----------\n",
      "Loss = 9.441634\n",
      "Variable usage = 36.00%\n",
      "----------Iter = 5050----------\n",
      "Loss = 9.408579\n",
      "Variable usage = 36.00%\n",
      "----------Iter = 5100----------\n",
      "Loss = 9.375956\n",
      "Variable usage = 36.00%\n",
      "----------Iter = 5150----------\n",
      "Loss = 9.343758\n",
      "Variable usage = 36.00%\n",
      "----------Iter = 5200----------\n",
      "Loss = 9.311969\n",
      "Variable usage = 36.00%\n",
      "----------Iter = 5250----------\n",
      "Loss = 9.280583\n",
      "Variable usage = 36.00%\n",
      "----------Iter = 5300----------\n",
      "Loss = 9.249608\n",
      "Variable usage = 36.00%\n",
      "----------Iter = 5350----------\n",
      "Loss = 9.219022\n",
      "Variable usage = 36.00%\n",
      "----------Iter = 5400----------\n",
      "Loss = 9.188825\n",
      "Variable usage = 36.00%\n",
      "----------Iter = 5450----------\n",
      "Loss = 9.159003\n",
      "Variable usage = 36.00%\n",
      "----------Iter = 5500----------\n",
      "Loss = 9.129558\n",
      "Variable usage = 36.00%\n",
      "----------Iter = 5550----------\n",
      "Loss = 9.100482\n",
      "Variable usage = 36.00%\n",
      "----------Iter = 5600----------\n",
      "Loss = 9.071767\n",
      "Variable usage = 36.00%\n",
      "----------Iter = 5650----------\n",
      "Loss = 9.043408\n",
      "Variable usage = 36.00%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------Iter = 5700----------\n",
      "Loss = 9.015396\n",
      "Variable usage = 36.00%\n",
      "----------Iter = 5750----------\n",
      "Loss = 8.987741\n",
      "Variable usage = 36.00%\n",
      "----------Iter = 5800----------\n",
      "Loss = 8.960411\n",
      "Variable usage = 36.00%\n",
      "----------Iter = 5850----------\n",
      "Loss = 8.933415\n",
      "Variable usage = 37.00%\n",
      "----------Iter = 5900----------\n",
      "Loss = 8.906749\n",
      "Variable usage = 37.00%\n",
      "----------Iter = 5950----------\n",
      "Loss = 8.880415\n",
      "Variable usage = 37.00%\n",
      "----------Iter = 6000----------\n",
      "Loss = 8.854391\n",
      "Variable usage = 37.00%\n",
      "----------Iter = 6050----------\n",
      "Loss = 8.828679\n",
      "Variable usage = 37.00%\n",
      "----------Iter = 6100----------\n",
      "Loss = 8.803278\n",
      "Variable usage = 37.00%\n",
      "----------Iter = 6150----------\n",
      "Loss = 8.778180\n",
      "Variable usage = 37.00%\n",
      "----------Iter = 6200----------\n",
      "Loss = 8.753374\n",
      "Variable usage = 37.00%\n",
      "----------Iter = 6250----------\n",
      "Loss = 8.728863\n",
      "Variable usage = 37.00%\n",
      "----------Iter = 6300----------\n",
      "Loss = 8.704654\n",
      "Variable usage = 37.00%\n",
      "----------Iter = 6350----------\n",
      "Loss = 8.680720\n",
      "Variable usage = 37.00%\n",
      "----------Iter = 6400----------\n",
      "Loss = 8.657079\n",
      "Variable usage = 37.00%\n",
      "----------Iter = 6450----------\n",
      "Loss = 8.633710\n",
      "Variable usage = 37.00%\n",
      "----------Iter = 6500----------\n",
      "Loss = 8.610609\n",
      "Variable usage = 38.00%\n",
      "----------Iter = 6550----------\n",
      "Loss = 8.587791\n",
      "Variable usage = 38.00%\n",
      "----------Iter = 6600----------\n",
      "Loss = 8.565228\n",
      "Variable usage = 38.00%\n",
      "----------Iter = 6650----------\n",
      "Loss = 8.542979\n",
      "Variable usage = 39.00%\n",
      "----------Iter = 6700----------\n",
      "Loss = 8.646133\n",
      "Variable usage = 42.00%\n",
      "----------Iter = 6750----------\n",
      "Loss = 8.503791\n",
      "Variable usage = 38.00%\n",
      "----------Iter = 6800----------\n",
      "Loss = 8.481951\n",
      "Variable usage = 38.00%\n",
      "----------Iter = 6850----------\n",
      "Loss = 8.460586\n",
      "Variable usage = 38.00%\n",
      "----------Iter = 6900----------\n",
      "Loss = 8.439479\n",
      "Variable usage = 38.00%\n",
      "----------Iter = 6950----------\n",
      "Loss = 8.418628\n",
      "Variable usage = 38.00%\n",
      "----------Iter = 7000----------\n",
      "Loss = 8.398024\n",
      "Variable usage = 38.00%\n",
      "----------Iter = 7050----------\n",
      "Loss = 8.377985\n",
      "Variable usage = 38.00%\n",
      "----------Iter = 7100----------\n",
      "Loss = 8.361215\n",
      "Variable usage = 38.00%\n",
      "----------Iter = 7150----------\n",
      "Loss = 8.339177\n",
      "Variable usage = 38.00%\n",
      "----------Iter = 7200----------\n",
      "Loss = 8.319476\n",
      "Variable usage = 38.00%\n",
      "----------Iter = 7250----------\n",
      "Loss = 8.300033\n",
      "Variable usage = 38.00%\n",
      "----------Iter = 7300----------\n",
      "Loss = 8.280804\n",
      "Variable usage = 38.00%\n",
      "----------Iter = 7350----------\n",
      "Loss = 8.261821\n",
      "Variable usage = 38.00%\n",
      "----------Iter = 7400----------\n",
      "Loss = 8.672060\n",
      "Variable usage = 44.00%\n",
      "----------Iter = 7450----------\n",
      "Loss = 8.226294\n",
      "Variable usage = 39.00%\n",
      "----------Iter = 7500----------\n",
      "Loss = 8.207776\n",
      "Variable usage = 40.00%\n",
      "----------Iter = 7550----------\n",
      "Loss = 8.189617\n",
      "Variable usage = 40.00%\n",
      "----------Iter = 7600----------\n",
      "Loss = 8.171694\n",
      "Variable usage = 40.00%\n",
      "----------Iter = 7650----------\n",
      "Loss = 8.154083\n",
      "Variable usage = 40.00%\n",
      "----------Iter = 7700----------\n",
      "Loss = 8.257492\n",
      "Variable usage = 45.00%\n",
      "----------Iter = 7750----------\n",
      "Loss = 8.134055\n",
      "Variable usage = 41.00%\n",
      "----------Iter = 7800----------\n",
      "Loss = 8.109728\n",
      "Variable usage = 40.00%\n",
      "----------Iter = 7850----------\n",
      "Loss = 8.091919\n",
      "Variable usage = 39.00%\n",
      "----------Iter = 7900----------\n",
      "Loss = 8.075006\n",
      "Variable usage = 39.00%\n",
      "----------Iter = 7950----------\n",
      "Loss = 8.058314\n",
      "Variable usage = 40.00%\n",
      "----------Iter = 8000----------\n",
      "Loss = 8.041909\n",
      "Variable usage = 40.00%\n",
      "----------Iter = 8050----------\n",
      "Loss = 8.083285\n",
      "Variable usage = 42.00%\n",
      "----------Iter = 8100----------\n",
      "Loss = 8.011199\n",
      "Variable usage = 40.00%\n",
      "----------Iter = 8150----------\n",
      "Loss = 7.995181\n",
      "Variable usage = 40.00%\n",
      "----------Iter = 8200----------\n",
      "Loss = 7.979408\n",
      "Variable usage = 41.00%\n",
      "----------Iter = 8250----------\n",
      "Loss = 7.963816\n",
      "Variable usage = 41.00%\n",
      "----------Iter = 8300----------\n",
      "Loss = 7.948410\n",
      "Variable usage = 41.00%\n",
      "----------Iter = 8350----------\n",
      "Loss = 8.223729\n",
      "Variable usage = 47.00%\n",
      "----------Iter = 8400----------\n",
      "Loss = 7.924736\n",
      "Variable usage = 42.00%\n",
      "----------Iter = 8450----------\n",
      "Loss = 9.595018\n",
      "Variable usage = 47.00%\n",
      "----------Iter = 8500----------\n",
      "Loss = 7.959695\n",
      "Variable usage = 46.00%\n",
      "----------Iter = 8550----------\n",
      "Loss = 7.886331\n",
      "Variable usage = 40.00%\n",
      "----------Iter = 8600----------\n",
      "Loss = 7.871097\n",
      "Variable usage = 40.00%\n",
      "----------Iter = 8650----------\n",
      "Loss = 8.015584\n",
      "Variable usage = 45.00%\n",
      "----------Iter = 8700----------\n",
      "Loss = 7.844107\n",
      "Variable usage = 41.00%\n",
      "----------Iter = 8750----------\n",
      "Loss = 7.829817\n",
      "Variable usage = 41.00%\n",
      "----------Iter = 8800----------\n",
      "Loss = 7.815798\n",
      "Variable usage = 41.00%\n",
      "----------Iter = 8850----------\n",
      "Loss = 7.801956\n",
      "Variable usage = 41.00%\n",
      "----------Iter = 8900----------\n",
      "Loss = 7.788332\n",
      "Variable usage = 41.00%\n",
      "----------Iter = 8950----------\n",
      "Loss = 8.445338\n",
      "Variable usage = 46.00%\n",
      "----------Iter = 9000----------\n",
      "Loss = 7.763449\n",
      "Variable usage = 42.00%\n",
      "----------Iter = 9050----------\n",
      "Loss = 7.753382\n",
      "Variable usage = 42.00%\n",
      "----------Iter = 9100----------\n",
      "Loss = 7.738596\n",
      "Variable usage = 40.00%\n",
      "----------Iter = 9150----------\n",
      "Loss = 8.803683\n",
      "Variable usage = 47.00%\n",
      "----------Iter = 9200----------\n",
      "Loss = 7.714635\n",
      "Variable usage = 41.00%\n",
      "----------Iter = 9250----------\n",
      "Loss = 7.711653\n",
      "Variable usage = 41.00%\n",
      "----------Iter = 9300----------\n",
      "Loss = 7.695776\n",
      "Variable usage = 41.00%\n",
      "----------Iter = 9350----------\n",
      "Loss = 7.681178\n",
      "Variable usage = 41.00%\n",
      "----------Iter = 9400----------\n",
      "Loss = 7.675683\n",
      "Variable usage = 42.00%\n",
      "----------Iter = 9450----------\n",
      "Loss = 7.658265\n",
      "Variable usage = 41.00%\n",
      "----------Iter = 9500----------\n",
      "Loss = 7.802570\n",
      "Variable usage = 44.00%\n",
      "----------Iter = 9550----------\n",
      "Loss = 7.639545\n",
      "Variable usage = 40.00%\n",
      "----------Iter = 9600----------\n",
      "Loss = 7.674375\n",
      "Variable usage = 45.00%\n",
      "----------Iter = 9650----------\n",
      "Loss = 7.613753\n",
      "Variable usage = 41.00%\n",
      "----------Iter = 9700----------\n",
      "Loss = 7.607201\n",
      "Variable usage = 42.00%\n",
      "----------Iter = 9750----------\n",
      "Loss = 7.593387\n",
      "Variable usage = 41.00%\n",
      "----------Iter = 9800----------\n",
      "Loss = 7.581613\n",
      "Variable usage = 41.00%\n",
      "----------Iter = 9850----------\n",
      "Loss = 7.572362\n",
      "Variable usage = 40.00%\n",
      "----------Iter = 9900----------\n",
      "Loss = 7.681755\n",
      "Variable usage = 44.00%\n",
      "----------Iter = 9950----------\n",
      "Loss = 8.617476\n",
      "Variable usage = 46.00%\n",
      "----------Iter = 10000----------\n",
      "Loss = 7.546292\n",
      "Variable usage = 41.00%\n",
      "----------Iter = 10050----------\n",
      "Loss = 7.533462\n",
      "Variable usage = 41.00%\n",
      "----------Iter = 10100----------\n",
      "Loss = 7.522205\n",
      "Variable usage = 41.00%\n",
      "----------Iter = 10150----------\n",
      "Loss = 7.514280\n",
      "Variable usage = 42.00%\n",
      "----------Iter = 10200----------\n",
      "Loss = 7.511531\n",
      "Variable usage = 40.00%\n",
      "----------Iter = 10250----------\n",
      "Loss = 7.499330\n",
      "Variable usage = 41.00%\n",
      "----------Iter = 10300----------\n",
      "Loss = 7.853525\n",
      "Variable usage = 47.00%\n",
      "----------Iter = 10350----------\n",
      "Loss = 7.477268\n",
      "Variable usage = 41.00%\n",
      "----------Iter = 10400----------\n",
      "Loss = 7.493417\n",
      "Variable usage = 43.00%\n",
      "----------Iter = 10450----------\n",
      "Loss = 7.457860\n",
      "Variable usage = 41.00%\n",
      "----------Iter = 10500----------\n",
      "Loss = 7.447736\n",
      "Variable usage = 41.00%\n",
      "----------Iter = 10550----------\n",
      "Loss = 7.447927\n",
      "Variable usage = 42.00%\n",
      "----------Iter = 10600----------\n",
      "Loss = 7.435858\n",
      "Variable usage = 42.00%\n",
      "----------Iter = 10650----------\n",
      "Loss = 7.443219\n",
      "Variable usage = 41.00%\n",
      "----------Iter = 10700----------\n",
      "Loss = 7.420154\n",
      "Variable usage = 41.00%\n",
      "----------Iter = 10750----------\n",
      "Loss = 7.409769\n",
      "Variable usage = 41.00%\n",
      "----------Iter = 10800----------\n",
      "Loss = 7.491867\n",
      "Variable usage = 43.00%\n",
      "----------Iter = 10850----------\n",
      "Loss = 7.391877\n",
      "Variable usage = 40.00%\n",
      "----------Iter = 10900----------\n",
      "Loss = 7.390416\n",
      "Variable usage = 41.00%\n",
      "----------Iter = 10950----------\n",
      "Loss = 7.376701\n",
      "Variable usage = 41.00%\n",
      "----------Iter = 11000----------\n",
      "Loss = 7.365346\n",
      "Variable usage = 41.00%\n",
      "----------Iter = 11050----------\n",
      "Loss = 7.355987\n",
      "Variable usage = 41.00%\n",
      "----------Iter = 11100----------\n",
      "Loss = 7.359258\n",
      "Variable usage = 41.00%\n",
      "----------Iter = 11150----------\n",
      "Loss = 7.373726\n",
      "Variable usage = 43.00%\n",
      "----------Iter = 11200----------\n",
      "Loss = 7.333866\n",
      "Variable usage = 42.00%\n",
      "----------Iter = 11250----------\n",
      "Loss = 7.341575\n",
      "Variable usage = 43.00%\n",
      "----------Iter = 11300----------\n",
      "Loss = 7.440093\n",
      "Variable usage = 48.00%\n",
      "----------Iter = 11350----------\n",
      "Loss = 7.316018\n",
      "Variable usage = 41.00%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------Iter = 11400----------\n",
      "Loss = 7.306719\n",
      "Variable usage = 41.00%\n",
      "----------Iter = 11450----------\n",
      "Loss = 7.943285\n",
      "Variable usage = 47.00%\n",
      "----------Iter = 11500----------\n",
      "Loss = 7.293257\n",
      "Variable usage = 41.00%\n",
      "----------Iter = 11550----------\n",
      "Loss = 7.289017\n",
      "Variable usage = 41.00%\n",
      "----------Iter = 11600----------\n",
      "Loss = 7.292337\n",
      "Variable usage = 43.00%\n",
      "----------Iter = 11650----------\n",
      "Loss = 7.277341\n",
      "Variable usage = 43.00%\n",
      "----------Iter = 11700----------\n",
      "Loss = 7.266911\n",
      "Variable usage = 40.00%\n",
      "----------Iter = 11750----------\n",
      "Loss = 7.264633\n",
      "Variable usage = 41.00%\n",
      "----------Iter = 11800----------\n",
      "Loss = 7.253737\n",
      "Variable usage = 42.00%\n",
      "----------Iter = 11850----------\n",
      "Loss = 7.244449\n",
      "Variable usage = 42.00%\n",
      "----------Iter = 11900----------\n",
      "Loss = 7.267607\n",
      "Variable usage = 43.00%\n",
      "----------Iter = 11950----------\n",
      "Loss = 8.327649\n",
      "Variable usage = 52.00%\n",
      "----------Iter = 12000----------\n",
      "Loss = 7.227138\n",
      "Variable usage = 42.00%\n",
      "----------Iter = 12050----------\n",
      "Loss = 7.253246\n",
      "Variable usage = 42.00%\n",
      "----------Iter = 12100----------\n",
      "Loss = 7.212163\n",
      "Variable usage = 41.00%\n",
      "----------Iter = 12150----------\n",
      "Loss = 7.412408\n",
      "Variable usage = 47.00%\n",
      "----------Iter = 12200----------\n",
      "Loss = 7.219008\n",
      "Variable usage = 45.00%\n",
      "----------Iter = 12250----------\n",
      "Loss = 7.193661\n",
      "Variable usage = 42.00%\n",
      "----------Iter = 12300----------\n",
      "Loss = 8.119765\n",
      "Variable usage = 47.00%\n",
      "----------Iter = 12350----------\n",
      "Loss = 7.615903\n",
      "Variable usage = 45.00%\n",
      "----------Iter = 12400----------\n",
      "Loss = 7.182203\n",
      "Variable usage = 41.00%\n",
      "----------Iter = 12450----------\n",
      "Loss = 9.941813\n",
      "Variable usage = 47.00%\n",
      "----------Iter = 12500----------\n",
      "Loss = 7.345634\n",
      "Variable usage = 45.00%\n",
      "----------Iter = 12550----------\n",
      "Loss = 7.446543\n",
      "Variable usage = 48.00%\n",
      "----------Iter = 12600----------\n",
      "Loss = 7.155346\n",
      "Variable usage = 42.00%\n",
      "----------Iter = 12650----------\n",
      "Loss = 8.115943\n",
      "Variable usage = 47.00%\n",
      "----------Iter = 12700----------\n",
      "Loss = 7.141914\n",
      "Variable usage = 41.00%\n",
      "----------Iter = 12750----------\n",
      "Loss = 7.134190\n",
      "Variable usage = 41.00%\n",
      "----------Iter = 12800----------\n",
      "Loss = 7.757382\n",
      "Variable usage = 46.00%\n",
      "----------Iter = 12850----------\n",
      "Loss = 7.124977\n",
      "Variable usage = 41.00%\n",
      "----------Iter = 12900----------\n",
      "Loss = 7.125997\n",
      "Variable usage = 41.00%\n",
      "----------Iter = 12950----------\n",
      "Loss = 7.114265\n",
      "Variable usage = 41.00%\n",
      "----------Iter = 13000----------\n",
      "Loss = 7.476342\n",
      "Variable usage = 46.00%\n",
      "----------Iter = 13050----------\n",
      "Loss = 7.106885\n",
      "Variable usage = 41.00%\n",
      "----------Iter = 13100----------\n",
      "Loss = 7.100184\n",
      "Variable usage = 41.00%\n",
      "----------Iter = 13150----------\n",
      "Loss = 7.114250\n",
      "Variable usage = 43.00%\n",
      "----------Iter = 13200----------\n",
      "Loss = 7.091112\n",
      "Variable usage = 40.00%\n",
      "----------Iter = 13250----------\n",
      "Loss = 7.082904\n",
      "Variable usage = 41.00%\n",
      "----------Iter = 13300----------\n",
      "Loss = 7.080080\n",
      "Variable usage = 41.00%\n",
      "----------Iter = 13350----------\n",
      "Loss = 8.833364\n",
      "Variable usage = 47.00%\n",
      "----------Iter = 13400----------\n",
      "Loss = 7.385179\n",
      "Variable usage = 45.00%\n",
      "----------Iter = 13450----------\n",
      "Loss = 7.070685\n",
      "Variable usage = 41.00%\n",
      "----------Iter = 13500----------\n",
      "Loss = 7.060850\n",
      "Variable usage = 41.00%\n",
      "----------Iter = 13550----------\n",
      "Loss = 7.755893\n",
      "Variable usage = 46.00%\n",
      "----------Iter = 13600----------\n",
      "Loss = 7.051248\n",
      "Variable usage = 42.00%\n",
      "----------Iter = 13650----------\n",
      "Loss = 7.046575\n",
      "Variable usage = 41.00%\n",
      "----------Iter = 13700----------\n",
      "Loss = 7.038720\n",
      "Variable usage = 42.00%\n",
      "----------Iter = 13750----------\n",
      "Loss = 7.433838\n",
      "Variable usage = 46.00%\n",
      "----------Iter = 13800----------\n",
      "Loss = 7.028220\n",
      "Variable usage = 41.00%\n",
      "----------Iter = 13850----------\n",
      "Loss = 7.021597\n",
      "Variable usage = 41.00%\n",
      "----------Iter = 13900----------\n",
      "Loss = 7.033314\n",
      "Variable usage = 41.00%\n",
      "----------Iter = 13950----------\n",
      "Loss = 7.175157\n",
      "Variable usage = 43.00%\n",
      "----------Iter = 14000----------\n",
      "Loss = 7.014764\n",
      "Variable usage = 41.00%\n",
      "----------Iter = 14050----------\n",
      "Loss = 7.105470\n",
      "Variable usage = 44.00%\n",
      "----------Iter = 14100----------\n",
      "Loss = 7.099518\n",
      "Variable usage = 46.00%\n",
      "----------Iter = 14150----------\n",
      "Loss = 7.006076\n",
      "Variable usage = 42.00%\n",
      "----------Iter = 14200----------\n",
      "Loss = 7.346305\n",
      "Variable usage = 46.00%\n",
      "----------Iter = 14250----------\n",
      "Loss = 7.000312\n",
      "Variable usage = 40.00%\n",
      "----------Iter = 14300----------\n",
      "Loss = 6.992226\n",
      "Variable usage = 40.00%\n",
      "----------Iter = 14350----------\n",
      "Loss = 7.032319\n",
      "Variable usage = 45.00%\n",
      "----------Iter = 14400----------\n",
      "Loss = 6.981082\n",
      "Variable usage = 41.00%\n",
      "----------Iter = 14450----------\n",
      "Loss = 6.988059\n",
      "Variable usage = 40.00%\n",
      "----------Iter = 14500----------\n",
      "Loss = 6.991612\n",
      "Variable usage = 43.00%\n",
      "----------Iter = 14550----------\n",
      "Loss = 6.975650\n",
      "Variable usage = 43.00%\n",
      "----------Iter = 14600----------\n",
      "Loss = 8.986027\n",
      "Variable usage = 46.00%\n",
      "----------Iter = 14650----------\n",
      "Loss = 6.964870\n",
      "Variable usage = 40.00%\n",
      "----------Iter = 14700----------\n",
      "Loss = 7.036186\n",
      "Variable usage = 43.00%\n",
      "----------Iter = 14750----------\n",
      "Loss = 6.964251\n",
      "Variable usage = 41.00%\n",
      "----------Iter = 14800----------\n",
      "Loss = 6.949820\n",
      "Variable usage = 41.00%\n",
      "----------Iter = 14850----------\n",
      "Loss = 6.968446\n",
      "Variable usage = 41.00%\n",
      "----------Iter = 14900----------\n",
      "Loss = 7.521081\n",
      "Variable usage = 52.00%\n",
      "----------Iter = 14950----------\n",
      "Loss = 7.034589\n",
      "Variable usage = 45.00%\n",
      "----------Iter = 15000----------\n",
      "Loss = 7.025309\n",
      "Variable usage = 42.00%\n",
      "----------Iter = 15050----------\n",
      "Loss = 6.929732\n",
      "Variable usage = 41.00%\n",
      "----------Iter = 15100----------\n",
      "Loss = 6.932838\n",
      "Variable usage = 41.00%\n",
      "----------Iter = 15150----------\n",
      "Loss = 6.943941\n",
      "Variable usage = 43.00%\n",
      "----------Iter = 15200----------\n",
      "Loss = 6.946379\n",
      "Variable usage = 43.00%\n",
      "----------Iter = 15250----------\n",
      "Loss = 6.915536\n",
      "Variable usage = 41.00%\n",
      "----------Iter = 15300----------\n",
      "Loss = 6.929466\n",
      "Variable usage = 42.00%\n",
      "----------Iter = 15350----------\n",
      "Loss = 6.911153\n",
      "Variable usage = 40.00%\n",
      "----------Iter = 15400----------\n",
      "Loss = 8.201170\n",
      "Variable usage = 47.00%\n",
      "----------Iter = 15450----------\n",
      "Loss = 7.727969\n",
      "Variable usage = 49.00%\n",
      "----------Iter = 15500----------\n",
      "Loss = 6.899027\n",
      "Variable usage = 41.00%\n",
      "----------Iter = 15550----------\n",
      "Loss = 6.934226\n",
      "Variable usage = 43.00%\n",
      "----------Iter = 15600----------\n",
      "Loss = 6.905127\n",
      "Variable usage = 41.00%\n",
      "----------Iter = 15650----------\n",
      "Loss = 7.001198\n",
      "Variable usage = 46.00%\n",
      "----------Iter = 15700----------\n",
      "Loss = 6.940574\n",
      "Variable usage = 44.00%\n",
      "----------Iter = 15750----------\n",
      "Loss = 6.885054\n",
      "Variable usage = 40.00%\n",
      "----------Iter = 15800----------\n",
      "Loss = 7.141454\n",
      "Variable usage = 46.00%\n",
      "----------Iter = 15850----------\n",
      "Loss = 6.964855\n",
      "Variable usage = 47.00%\n",
      "----------Iter = 15900----------\n",
      "Loss = 6.876435\n",
      "Variable usage = 41.00%\n",
      "----------Iter = 15950----------\n",
      "Loss = 6.869527\n",
      "Variable usage = 42.00%\n",
      "----------Iter = 16000----------\n",
      "Loss = 6.966534\n",
      "Variable usage = 44.00%\n",
      "----------Iter = 16050----------\n",
      "Loss = 6.886636\n",
      "Variable usage = 42.00%\n",
      "----------Iter = 16100----------\n",
      "Loss = 6.859000\n",
      "Variable usage = 41.00%\n",
      "----------Iter = 16150----------\n",
      "Loss = 6.868256\n",
      "Variable usage = 41.00%\n",
      "----------Iter = 16200----------\n",
      "Loss = 6.853031\n",
      "Variable usage = 41.00%\n",
      "----------Iter = 16250----------\n",
      "Loss = 6.844910\n",
      "Variable usage = 41.00%\n",
      "----------Iter = 16300----------\n",
      "Loss = 7.652704\n",
      "Variable usage = 47.00%\n",
      "----------Iter = 16350----------\n",
      "Loss = 7.515447\n",
      "Variable usage = 51.00%\n",
      "----------Iter = 16400----------\n",
      "Loss = 7.162240\n",
      "Variable usage = 46.00%\n",
      "----------Iter = 16450----------\n",
      "Loss = 6.836555\n",
      "Variable usage = 41.00%\n",
      "----------Iter = 16500----------\n",
      "Loss = 6.865346\n",
      "Variable usage = 44.00%\n",
      "----------Iter = 16550----------\n",
      "Loss = 7.104178\n",
      "Variable usage = 47.00%\n",
      "----------Iter = 16600----------\n",
      "Loss = 7.119256\n",
      "Variable usage = 49.00%\n",
      "----------Iter = 16650----------\n",
      "Loss = 6.830408\n",
      "Variable usage = 40.00%\n",
      "----------Iter = 16700----------\n",
      "Loss = 7.038967\n",
      "Variable usage = 44.00%\n",
      "----------Iter = 16750----------\n",
      "Loss = 6.975617\n",
      "Variable usage = 44.00%\n",
      "----------Iter = 16800----------\n",
      "Loss = 6.813757\n",
      "Variable usage = 41.00%\n",
      "----------Iter = 16850----------\n",
      "Loss = 9.677210\n",
      "Variable usage = 52.00%\n",
      "----------Iter = 16900----------\n",
      "Loss = 6.809146\n",
      "Variable usage = 41.00%\n",
      "----------Iter = 16950----------\n",
      "Loss = 6.939068\n",
      "Variable usage = 47.00%\n",
      "----------Iter = 17000----------\n",
      "Loss = 7.272241\n",
      "Variable usage = 45.00%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------Iter = 17050----------\n",
      "Loss = 10.980513\n",
      "Variable usage = 45.00%\n",
      "----------Iter = 17100----------\n",
      "Loss = 6.814874\n",
      "Variable usage = 40.00%\n",
      "----------Iter = 17150----------\n",
      "Loss = 6.793804\n",
      "Variable usage = 40.00%\n",
      "----------Iter = 17200----------\n",
      "Loss = 6.978200\n",
      "Variable usage = 46.00%\n",
      "----------Iter = 17250----------\n",
      "Loss = 6.787875\n",
      "Variable usage = 41.00%\n",
      "----------Iter = 17300----------\n",
      "Loss = 6.784895\n",
      "Variable usage = 41.00%\n",
      "----------Iter = 17350----------\n",
      "Loss = 8.638019\n",
      "Variable usage = 54.00%\n",
      "----------Iter = 17400----------\n",
      "Loss = 6.795036\n",
      "Variable usage = 39.00%\n",
      "----------Iter = 17450----------\n",
      "Loss = 6.782236\n",
      "Variable usage = 41.00%\n",
      "----------Iter = 17500----------\n",
      "Loss = 6.789529\n",
      "Variable usage = 40.00%\n",
      "----------Iter = 17550----------\n",
      "Loss = 14.321657\n",
      "Variable usage = 59.00%\n",
      "----------Iter = 17600----------\n",
      "Loss = 6.781985\n",
      "Variable usage = 41.00%\n",
      "----------Iter = 17650----------\n",
      "Loss = 7.292339\n",
      "Variable usage = 46.00%\n",
      "----------Iter = 17700----------\n",
      "Loss = 6.773602\n",
      "Variable usage = 39.00%\n",
      "----------Iter = 17750----------\n",
      "Loss = 6.773810\n",
      "Variable usage = 41.00%\n",
      "----------Iter = 17800----------\n",
      "Loss = 6.763889\n",
      "Variable usage = 40.00%\n",
      "----------Iter = 17850----------\n",
      "Loss = 6.759275\n",
      "Variable usage = 41.00%\n",
      "----------Iter = 17900----------\n",
      "Loss = 6.779808\n",
      "Variable usage = 41.00%\n",
      "----------Iter = 17950----------\n",
      "Loss = 8.063263\n",
      "Variable usage = 47.00%\n",
      "----------Iter = 18000----------\n",
      "Loss = 6.942791\n",
      "Variable usage = 44.00%\n",
      "----------Iter = 18050----------\n",
      "Loss = 7.076552\n",
      "Variable usage = 46.00%\n",
      "----------Iter = 18100----------\n",
      "Loss = 7.619231\n",
      "Variable usage = 52.00%\n",
      "Stopping early\n"
     ]
    }
   ],
   "source": [
    "clstm = cLSTM(X.shape[-1], hidden=100).cuda(device=device)\n",
    "train_loss_list = train_model_ista(\n",
    "    clstm, X, context=10, lam=10.0, lam_ridge=1e-2, lr=1e-3, max_iter=20000,\n",
    "    check_every=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "47e87f56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True variable usage = 40.00%\n",
      "Estimated variable usage = 41.00%\n",
      "Accuracy = 97.00%\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x360 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "GC_est = clstm.GC().cpu().data.numpy()\n",
    "\n",
    "print('True variable usage = %.2f%%' % (100 * np.mean(GC)))\n",
    "print('Estimated variable usage = %.2f%%' % (100 * np.mean(GC_est)))\n",
    "print('Accuracy = %.2f%%' % (100 * np.mean(GC == GC_est)))\n",
    "\n",
    "# Make figures\n",
    "fig, axarr = plt.subplots(1, 2, figsize=(10, 5))\n",
    "axarr[0].imshow(GC, cmap='Blues')\n",
    "axarr[0].set_title('GC actual')\n",
    "axarr[0].set_ylabel('Affected series')\n",
    "axarr[0].set_xlabel('Causal series')\n",
    "axarr[0].set_xticks([])\n",
    "axarr[0].set_yticks([])\n",
    "\n",
    "axarr[1].imshow(GC_est, cmap='Blues', vmin=0, vmax=1, extent=(0, len(GC_est), len(GC_est), 0))\n",
    "axarr[1].set_ylabel('Affected series')\n",
    "axarr[1].set_xlabel('Causal series')\n",
    "axarr[1].set_xticks([])\n",
    "axarr[1].set_yticks([])\n",
    "\n",
    "# Mark disagreements\n",
    "for i in range(len(GC_est)):\n",
    "    for j in range(len(GC_est)):\n",
    "        if GC[i, j] != GC_est[i, j]:\n",
    "            rect = plt.Rectangle((j, i-0.05), 1, 1, facecolor='none', edgecolor='red', linewidth=1)\n",
    "            axarr[1].add_patch(rect)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1741dd77",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
