{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.layers import Dense\n",
    "\n",
    "from keras import optimizers\n",
    "from keras.layers import Dense, Activation, Flatten, Conv2D, MaxPooling2D\n",
    "from keras.utils.np_utils import to_categorical\n",
    "\n",
    "from keras.models import Sequential, Model \n",
    "from keras.utils import np_utils\n",
    "from keras.layers import  BatchNormalization,Dense, Conv2D, Convolution2D, MaxPooling2D, Dropout, Flatten, TimeDistributed, InputLayer, LSTM\n",
    "from keras.layers import Input, Reshape, Activation, add, Add\n",
    "from keras.utils.vis_utils import model_to_dot\n",
    "from keras.utils.vis_utils import plot_model\n",
    "from keras import optimizers\n",
    "\n",
    "from sklearn.metrics import recall_score \n",
    "from sklearn.metrics import precision_score \n",
    "from sklearn.metrics import f1_score \n",
    "from keras import backend as K\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "file=open(\"mindbigdata.txt\",\"r\")\n",
    "data=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "for line in file.readlines():\n",
    "    data.append(line.replace(\"\\t\",\",\").split(\",\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "eeg=pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "eeg=eeg.dropna(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=eeg.drop(eeg.columns[[0,1,2,3,4,5]],axis='columns')\n",
    "y=eeg[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>126</th>\n",
       "      <th>127</th>\n",
       "      <th>128</th>\n",
       "      <th>129</th>\n",
       "      <th>130</th>\n",
       "      <th>131</th>\n",
       "      <th>132</th>\n",
       "      <th>133</th>\n",
       "      <th>134</th>\n",
       "      <th>135</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.592058</td>\n",
       "      <td>0.589073</td>\n",
       "      <td>0.587879</td>\n",
       "      <td>0.590147</td>\n",
       "      <td>0.592177</td>\n",
       "      <td>0.592774</td>\n",
       "      <td>0.592416</td>\n",
       "      <td>0.591222</td>\n",
       "      <td>0.591102</td>\n",
       "      <td>0.592177</td>\n",
       "      <td>...</td>\n",
       "      <td>0.586923</td>\n",
       "      <td>0.586326</td>\n",
       "      <td>0.587759</td>\n",
       "      <td>0.589073</td>\n",
       "      <td>0.58955</td>\n",
       "      <td>0.590028</td>\n",
       "      <td>0.589431</td>\n",
       "      <td>0.588595</td>\n",
       "      <td>0.58752</td>\n",
       "      <td>0.586446</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.613908</td>\n",
       "      <td>0.610684</td>\n",
       "      <td>0.610445</td>\n",
       "      <td>0.613311</td>\n",
       "      <td>0.616177</td>\n",
       "      <td>0.61558</td>\n",
       "      <td>0.613311</td>\n",
       "      <td>0.614027</td>\n",
       "      <td>0.616057</td>\n",
       "      <td>0.616415</td>\n",
       "      <td>...</td>\n",
       "      <td>0.614027</td>\n",
       "      <td>0.61343</td>\n",
       "      <td>0.615221</td>\n",
       "      <td>0.617012</td>\n",
       "      <td>0.619042</td>\n",
       "      <td>0.619878</td>\n",
       "      <td>0.617729</td>\n",
       "      <td>0.616296</td>\n",
       "      <td>0.617371</td>\n",
       "      <td>0.618206</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.62537</td>\n",
       "      <td>0.623102</td>\n",
       "      <td>0.622147</td>\n",
       "      <td>0.622505</td>\n",
       "      <td>0.623938</td>\n",
       "      <td>0.62537</td>\n",
       "      <td>0.624654</td>\n",
       "      <td>0.622982</td>\n",
       "      <td>0.623102</td>\n",
       "      <td>0.623818</td>\n",
       "      <td>...</td>\n",
       "      <td>0.619162</td>\n",
       "      <td>0.620356</td>\n",
       "      <td>0.622027</td>\n",
       "      <td>0.623102</td>\n",
       "      <td>0.623579</td>\n",
       "      <td>0.622505</td>\n",
       "      <td>0.621072</td>\n",
       "      <td>0.620833</td>\n",
       "      <td>0.620236</td>\n",
       "      <td>0.618684</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.548357</td>\n",
       "      <td>0.54788</td>\n",
       "      <td>0.546686</td>\n",
       "      <td>0.545253</td>\n",
       "      <td>0.546208</td>\n",
       "      <td>0.54979</td>\n",
       "      <td>0.551103</td>\n",
       "      <td>0.549074</td>\n",
       "      <td>0.547163</td>\n",
       "      <td>0.546327</td>\n",
       "      <td>...</td>\n",
       "      <td>0.548954</td>\n",
       "      <td>0.548238</td>\n",
       "      <td>0.549074</td>\n",
       "      <td>0.549909</td>\n",
       "      <td>0.550148</td>\n",
       "      <td>0.550506</td>\n",
       "      <td>0.550506</td>\n",
       "      <td>0.549671</td>\n",
       "      <td>0.547521</td>\n",
       "      <td>0.546447</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.615938</td>\n",
       "      <td>0.616057</td>\n",
       "      <td>0.615221</td>\n",
       "      <td>0.615938</td>\n",
       "      <td>0.617848</td>\n",
       "      <td>0.618326</td>\n",
       "      <td>0.616774</td>\n",
       "      <td>0.615818</td>\n",
       "      <td>0.616057</td>\n",
       "      <td>0.616893</td>\n",
       "      <td>...</td>\n",
       "      <td>0.618087</td>\n",
       "      <td>0.618326</td>\n",
       "      <td>0.618206</td>\n",
       "      <td>0.617968</td>\n",
       "      <td>0.619162</td>\n",
       "      <td>0.619878</td>\n",
       "      <td>0.619281</td>\n",
       "      <td>0.618565</td>\n",
       "      <td>0.617251</td>\n",
       "      <td>0.616296</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>910471</th>\n",
       "      <td>0.622982</td>\n",
       "      <td>0.622505</td>\n",
       "      <td>0.622266</td>\n",
       "      <td>0.620236</td>\n",
       "      <td>0.620833</td>\n",
       "      <td>0.62537</td>\n",
       "      <td>0.624773</td>\n",
       "      <td>0.619759</td>\n",
       "      <td>0.618206</td>\n",
       "      <td>0.619162</td>\n",
       "      <td>...</td>\n",
       "      <td>0.63146</td>\n",
       "      <td>0.633251</td>\n",
       "      <td>0.632296</td>\n",
       "      <td>0.628117</td>\n",
       "      <td>0.629908</td>\n",
       "      <td>0.632773</td>\n",
       "      <td>0.630982</td>\n",
       "      <td>0.629549</td>\n",
       "      <td>0.629908</td>\n",
       "      <td>0.630863</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>910472</th>\n",
       "      <td>0.55373</td>\n",
       "      <td>0.553133</td>\n",
       "      <td>0.555282</td>\n",
       "      <td>0.554685</td>\n",
       "      <td>0.551223</td>\n",
       "      <td>0.550865</td>\n",
       "      <td>0.553969</td>\n",
       "      <td>0.557073</td>\n",
       "      <td>0.556118</td>\n",
       "      <td>0.55182</td>\n",
       "      <td>...</td>\n",
       "      <td>0.572954</td>\n",
       "      <td>0.576416</td>\n",
       "      <td>0.576655</td>\n",
       "      <td>0.574745</td>\n",
       "      <td>0.574625</td>\n",
       "      <td>0.571879</td>\n",
       "      <td>0.570566</td>\n",
       "      <td>0.574506</td>\n",
       "      <td>0.573431</td>\n",
       "      <td>0.569372</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>910473</th>\n",
       "      <td>0.634206</td>\n",
       "      <td>0.634206</td>\n",
       "      <td>0.6354</td>\n",
       "      <td>0.635042</td>\n",
       "      <td>0.634684</td>\n",
       "      <td>0.634922</td>\n",
       "      <td>0.634803</td>\n",
       "      <td>0.635758</td>\n",
       "      <td>0.635878</td>\n",
       "      <td>0.632893</td>\n",
       "      <td>...</td>\n",
       "      <td>0.646146</td>\n",
       "      <td>0.648295</td>\n",
       "      <td>0.646743</td>\n",
       "      <td>0.644594</td>\n",
       "      <td>0.645788</td>\n",
       "      <td>0.644833</td>\n",
       "      <td>0.643519</td>\n",
       "      <td>0.645668</td>\n",
       "      <td>0.64543</td>\n",
       "      <td>0.643997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>910474</th>\n",
       "      <td>0.488179</td>\n",
       "      <td>0.487702</td>\n",
       "      <td>0.487582</td>\n",
       "      <td>0.486985</td>\n",
       "      <td>0.487463</td>\n",
       "      <td>0.488538</td>\n",
       "      <td>0.488299</td>\n",
       "      <td>0.488776</td>\n",
       "      <td>0.489732</td>\n",
       "      <td>0.487463</td>\n",
       "      <td>...</td>\n",
       "      <td>0.515283</td>\n",
       "      <td>0.517313</td>\n",
       "      <td>0.517194</td>\n",
       "      <td>0.514806</td>\n",
       "      <td>0.513612</td>\n",
       "      <td>0.51194</td>\n",
       "      <td>0.512537</td>\n",
       "      <td>0.514567</td>\n",
       "      <td>0.509552</td>\n",
       "      <td>0.505851</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>910475</th>\n",
       "      <td>0.482568</td>\n",
       "      <td>0.481971</td>\n",
       "      <td>0.484239</td>\n",
       "      <td>0.483881</td>\n",
       "      <td>0.481971</td>\n",
       "      <td>0.482687</td>\n",
       "      <td>0.484239</td>\n",
       "      <td>0.48412</td>\n",
       "      <td>0.483284</td>\n",
       "      <td>0.482687</td>\n",
       "      <td>...</td>\n",
       "      <td>0.502627</td>\n",
       "      <td>0.505015</td>\n",
       "      <td>0.505015</td>\n",
       "      <td>0.501791</td>\n",
       "      <td>0.501313</td>\n",
       "      <td>0.499881</td>\n",
       "      <td>0.498567</td>\n",
       "      <td>0.501552</td>\n",
       "      <td>0.50203</td>\n",
       "      <td>0.499403</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>910476 rows × 136 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             0         1         2         3         4         5         6    \\\n",
       "0       0.592058  0.589073  0.587879  0.590147  0.592177  0.592774  0.592416   \n",
       "1       0.613908  0.610684  0.610445  0.613311  0.616177   0.61558  0.613311   \n",
       "2        0.62537  0.623102  0.622147  0.622505  0.623938   0.62537  0.624654   \n",
       "3       0.548357   0.54788  0.546686  0.545253  0.546208   0.54979  0.551103   \n",
       "4       0.615938  0.616057  0.615221  0.615938  0.617848  0.618326  0.616774   \n",
       "...          ...       ...       ...       ...       ...       ...       ...   \n",
       "910471  0.622982  0.622505  0.622266  0.620236  0.620833   0.62537  0.624773   \n",
       "910472   0.55373  0.553133  0.555282  0.554685  0.551223  0.550865  0.553969   \n",
       "910473  0.634206  0.634206    0.6354  0.635042  0.634684  0.634922  0.634803   \n",
       "910474  0.488179  0.487702  0.487582  0.486985  0.487463  0.488538  0.488299   \n",
       "910475  0.482568  0.481971  0.484239  0.483881  0.481971  0.482687  0.484239   \n",
       "\n",
       "             7         8         9    ...       126       127       128  \\\n",
       "0       0.591222  0.591102  0.592177  ...  0.586923  0.586326  0.587759   \n",
       "1       0.614027  0.616057  0.616415  ...  0.614027   0.61343  0.615221   \n",
       "2       0.622982  0.623102  0.623818  ...  0.619162  0.620356  0.622027   \n",
       "3       0.549074  0.547163  0.546327  ...  0.548954  0.548238  0.549074   \n",
       "4       0.615818  0.616057  0.616893  ...  0.618087  0.618326  0.618206   \n",
       "...          ...       ...       ...  ...       ...       ...       ...   \n",
       "910471  0.619759  0.618206  0.619162  ...   0.63146  0.633251  0.632296   \n",
       "910472  0.557073  0.556118   0.55182  ...  0.572954  0.576416  0.576655   \n",
       "910473  0.635758  0.635878  0.632893  ...  0.646146  0.648295  0.646743   \n",
       "910474  0.488776  0.489732  0.487463  ...  0.515283  0.517313  0.517194   \n",
       "910475   0.48412  0.483284  0.482687  ...  0.502627  0.505015  0.505015   \n",
       "\n",
       "             129       130       131       132       133       134       135  \n",
       "0       0.589073   0.58955  0.590028  0.589431  0.588595   0.58752  0.586446  \n",
       "1       0.617012  0.619042  0.619878  0.617729  0.616296  0.617371  0.618206  \n",
       "2       0.623102  0.623579  0.622505  0.621072  0.620833  0.620236  0.618684  \n",
       "3       0.549909  0.550148  0.550506  0.550506  0.549671  0.547521  0.546447  \n",
       "4       0.617968  0.619162  0.619878  0.619281  0.618565  0.617251  0.616296  \n",
       "...          ...       ...       ...       ...       ...       ...       ...  \n",
       "910471  0.628117  0.629908  0.632773  0.630982  0.629549  0.629908  0.630863  \n",
       "910472  0.574745  0.574625  0.571879  0.570566  0.574506  0.573431  0.569372  \n",
       "910473  0.644594  0.645788  0.644833  0.643519  0.645668   0.64543  0.643997  \n",
       "910474  0.514806  0.513612   0.51194  0.512537  0.514567  0.509552  0.505851  \n",
       "910475  0.501791  0.501313  0.499881  0.498567  0.501552   0.50203  0.499403  \n",
       "\n",
       "[910476 rows x 136 columns]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=X.values\n",
    "y=y.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler=MinMaxScaler()\n",
    "X[:]=scaler.fit_transform(X[:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=None, shuffle=True, stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(637333, 136) (273143, 136) (637333, 11) (273143, 11)\n"
     ]
    }
   ],
   "source": [
    "y_train = to_categorical(y_train, 11)\n",
    "y_test = to_categorical(y_test, 11)\n",
    "\n",
    "print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recall(y_target, y_pred):\n",
    "    # clip(t, clip_value_min, clip_value_max) : clip_value_min~clip_value_max 이외 가장자리를 깎아 낸다\n",
    "    # round : 반올림한다\n",
    "    y_target_yn = K.round(K.clip(y_target, 0, 1)) # 실제값을 0(Negative) 또는 1(Positive)로 설정한다\n",
    "    y_pred_yn = K.round(K.clip(y_pred, 0, 1)) # 예측값을 0(Negative) 또는 1(Positive)로 설정한다\n",
    "\n",
    "    # True Positive는 실제 값과 예측 값이 모두 1(Positive)인 경우이다\n",
    "    count_true_positive = K.sum(y_target_yn * y_pred_yn) \n",
    "\n",
    "    # (True Positive + False Negative) = 실제 값이 1(Positive) 전체\n",
    "    count_true_positive_false_negative = K.sum(y_target_yn)\n",
    "\n",
    "    # Recall =  (True Positive) / (True Positive + False Negative)\n",
    "    # K.epsilon()는 'divide by zero error' 예방차원에서 작은 수를 더한다\n",
    "    recall = count_true_positive / (count_true_positive_false_negative + K.epsilon())\n",
    "\n",
    "    # return a single tensor value\n",
    "    return recall\n",
    "\n",
    "\n",
    "def precision(y_target, y_pred):\n",
    "    # clip(t, clip_value_min, clip_value_max) : clip_value_min~clip_value_max 이외 가장자리를 깎아 낸다\n",
    "    # round : 반올림한다\n",
    "    y_pred_yn = K.round(K.clip(y_pred, 0, 1)) # 예측값을 0(Negative) 또는 1(Positive)로 설정한다\n",
    "    y_target_yn = K.round(K.clip(y_target, 0, 1)) # 실제값을 0(Negative) 또는 1(Positive)로 설정한다\n",
    "\n",
    "    # True Positive는 실제 값과 예측 값이 모두 1(Positive)인 경우이다\n",
    "    count_true_positive = K.sum(y_target_yn * y_pred_yn) \n",
    "\n",
    "    # (True Positive + False Positive) = 예측 값이 1(Positive) 전체\n",
    "    count_true_positive_false_positive = K.sum(y_pred_yn)\n",
    "\n",
    "    # Precision = (True Positive) / (True Positive + False Positive)\n",
    "    # K.epsilon()는 'divide by zero error' 예방차원에서 작은 수를 더한다\n",
    "    precision = count_true_positive / (count_true_positive_false_positive + K.epsilon())\n",
    "\n",
    "    # return a single tensor value\n",
    "    return precision\n",
    "\n",
    "\n",
    "def f1score(y_target, y_pred):\n",
    "    _recall = recall(y_target, y_pred)\n",
    "    _precision = precision(y_target, y_pred)\n",
    "    # K.epsilon()는 'divide by zero error' 예방차원에서 작은 수를 더한다\n",
    "    _f1score = ( 2 * _recall * _precision) / (_recall + _precision+ K.epsilon())\n",
    "    \n",
    "    # return a single tensor value\n",
    "    return _f1score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_4\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_4 (InputLayer)         (None, 136)               0         \n",
      "_________________________________________________________________\n",
      "dense_18 (Dense)             (None, 150)               20550     \n",
      "_________________________________________________________________\n",
      "dropout_12 (Dropout)         (None, 150)               0         \n",
      "_________________________________________________________________\n",
      "dense_19 (Dense)             (None, 200)               30200     \n",
      "_________________________________________________________________\n",
      "dense_20 (Dense)             (None, 11)                2211      \n",
      "=================================================================\n",
      "Total params: 52,961\n",
      "Trainable params: 52,961\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "def model_create(eeg_data):\n",
    "    eeg_input=Input(shape=(136,)) #입력 정의\n",
    "    \n",
    "    eeg_output = Dense(50, activation='relu')(eeg_input)\n",
    "    eeg_output = Dropout(0.5)(eeg_output)\n",
    "    eeg_output = Dense(150, activation='relu')(eeg_input)\n",
    "    eeg_output = Dropout(0.5)(eeg_output)\n",
    "    eeg_output = Dense(150, activation='relu')(eeg_input)\n",
    "    eeg_output = Dropout(0.5)(eeg_output)\n",
    "    eeg_output = Dense(200, activation='relu')(eeg_output)\n",
    "    \n",
    "    model = Dense(11, activation='sigmoid')(eeg_output)\n",
    "    \n",
    "    model = keras.models.Model(inputs=eeg_input, outputs=model) \n",
    "    \n",
    "    model.compile(loss='binary_crossentropy', optimizer = 'adam', metrics=['accuracy', precision, recall, f1score])\n",
    "    \n",
    "    return model \n",
    "\n",
    "model=model_create(X)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 637333 samples, validate on 273143 samples\n",
      "Epoch 1/50\n",
      "637333/637333 [==============================] - 261s 409us/step - loss: 0.2972 - accuracy: 0.9090 - precision: 3.2964e-05 - recall: 4.2364e-05 - f1score: 2.3168e-05 - val_loss: 0.2969 - val_accuracy: 0.9093 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_f1score: 0.0000e+00\n",
      "Epoch 2/50\n",
      "637333/637333 [==============================] - 261s 409us/step - loss: 0.2967 - accuracy: 0.9090 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1score: 0.0000e+00 - val_loss: 0.2967 - val_accuracy: 0.9093 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_f1score: 0.0000e+00\n",
      "Epoch 3/50\n",
      "637333/637333 [==============================] - 260s 408us/step - loss: 0.2967 - accuracy: 0.9090 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1score: 0.0000e+00 - val_loss: 0.2968 - val_accuracy: 0.9093 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_f1score: 0.0000e+00\n",
      "Epoch 4/50\n",
      "637333/637333 [==============================] - 266s 418us/step - loss: 0.2967 - accuracy: 0.9090 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1score: 0.0000e+00 - val_loss: 0.2966 - val_accuracy: 0.9093 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_f1score: 0.0000e+00\n",
      "Epoch 5/50\n",
      "157140/637333 [======>.......................] - ETA: 2:46 - loss: 0.2967 - accuracy: 0.9092 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1score: 0.0000e+00"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-32-633999bac352>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mhist\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m50\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\envs\\env\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[0;32m   1237\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1238\u001b[0m                                         \u001b[0mvalidation_steps\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvalidation_steps\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1239\u001b[1;33m                                         validation_freq=validation_freq)\n\u001b[0m\u001b[0;32m   1240\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1241\u001b[0m     def evaluate(self,\n",
      "\u001b[1;32m~\\anaconda3\\envs\\env\\lib\\site-packages\\keras\\engine\\training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[1;34m(model, fit_function, fit_inputs, out_labels, batch_size, epochs, verbose, callbacks, val_function, val_inputs, shuffle, initial_epoch, steps_per_epoch, validation_steps, validation_freq)\u001b[0m\n\u001b[0;32m    194\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    195\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 196\u001b[1;33m                 \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfit_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    197\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    198\u001b[0m                 \u001b[1;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mo\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\env\\lib\\site-packages\\tensorflow_core\\python\\keras\\backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   3725\u001b[0m         \u001b[0mvalue\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmath_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcast\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtensor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3726\u001b[0m       \u001b[0mconverted_inputs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3727\u001b[1;33m     \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_graph_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mconverted_inputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3728\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3729\u001b[0m     \u001b[1;31m# EagerTensor.numpy() will often make a copy to ensure memory safety.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\env\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1549\u001b[0m       \u001b[0mTypeError\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mFor\u001b[0m \u001b[0minvalid\u001b[0m \u001b[0mpositional\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0mkeyword\u001b[0m \u001b[0margument\u001b[0m \u001b[0mcombinations\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1550\u001b[0m     \"\"\"\n\u001b[1;32m-> 1551\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1552\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1553\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\env\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, args, kwargs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1589\u001b[0m       raise TypeError(\"Keyword arguments {} unknown. Expected {}.\".format(\n\u001b[0;32m   1590\u001b[0m           list(kwargs.keys()), list(self._arg_keywords)))\n\u001b[1;32m-> 1591\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call_flat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcaptured_inputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1592\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1593\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_filtered_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\env\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1690\u001b[0m       \u001b[1;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1691\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[1;32m-> 1692\u001b[1;33m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[0;32m   1693\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[0;32m   1694\u001b[0m         \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\env\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    543\u001b[0m               \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    544\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"executor_type\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexecutor_type\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"config_proto\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 545\u001b[1;33m               ctx=ctx)\n\u001b[0m\u001b[0;32m    546\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    547\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[1;32m~\\anaconda3\\envs\\env\\lib\\site-packages\\tensorflow_core\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     59\u001b[0m     tensors = pywrap_tensorflow.TFE_Py_Execute(ctx._handle, device_name,\n\u001b[0;32m     60\u001b[0m                                                \u001b[0mop_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 61\u001b[1;33m                                                num_outputs)\n\u001b[0m\u001b[0;32m     62\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     63\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "hist=model.fit(X_train, y_train, epochs=50, batch_size=10, validation_data=(X_test,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, loss_ax = plt.subplots()\n",
    "\n",
    "acc_ax = loss_ax.twinx()\n",
    "\n",
    "loss_ax.plot(hist.history['loss'], 'y', label='train loss')\n",
    "loss_ax.plot(hist.history['val_loss'], 'r', label='val loss')\n",
    "\n",
    "acc_ax.plot(hist.history['accuracy'], 'b', label='train acc')\n",
    "acc_ax.plot(hist.history['val_accuracy'], 'g', label='val acc')\n",
    "\n",
    "loss_ax.set_xlabel('epoch')\n",
    "loss_ax.set_ylabel('loss')\n",
    "acc_ax.set_ylabel('accuray')\n",
    "\n",
    "loss_ax.legend(loc='upper left')\n",
    "acc_ax.legend(loc='lower left')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "#results = model.evaluate(X_test, y_test)\n",
    "print('Test accuracy: ', results)\n",
    "\n",
    "_loss, _acc, _precision, _recall, _f1score = model.evaluate(X_test, y_test)\n",
    "print('loss: {:.3f}, accuracy: {:.3f}, precision: {:.3f}, recall: {:.3f}, f1score: {:.3f}'.format(_loss, _acc, _precision, _recall, _f1score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, rec_ax = plt.subplots()\n",
    "\n",
    "pre_ax = rec_ax.twinx()\n",
    "\n",
    "rec_ax.plot(hist.history['recall'], 'y', label='train recall')\n",
    "rec_ax.plot(hist.history['val_recall'], 'r', label='val recall')\n",
    "\n",
    "pre_ax.plot(hist.history['precision'], 'b', label='train precision')\n",
    "pre_ax.plot(hist.history['val_precision'], 'g', label='val precision')\n",
    "\n",
    "rec_ax.set_xlabel('epoch')\n",
    "rec_ax.set_ylabel('recall')\n",
    "pre_ax.set_ylabel('precision')\n",
    "\n",
    "rec_ax.legend(loc='upper left')\n",
    "pre_ax.legend(loc='lower left')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
