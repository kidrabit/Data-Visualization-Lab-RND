{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "from keras.layers import Dense\n",
    "\n",
    "from keras import optimizers\n",
    "from keras.layers import Dense, Activation, Flatten, Conv2D, MaxPooling2D\n",
    "from keras.utils.np_utils import to_categorical\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import genfromtxt\n",
    "EEG = genfromtxt(\"Confusion during MOOC/EEG_data.csv\", delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "#6의 raw데이터에 이상이 있으므로 6을 제외하고 분석\n",
    "EEG=EEG[1:,:]\n",
    "EEG=pd.DataFrame(EEG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(12811, 15) (1275,)\n"
     ]
    }
   ],
   "source": [
    "remove_6=EEG[EEG[0]==6].index\n",
    "print(EEG.shape, remove_6.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(11536, 15)\n"
     ]
    }
   ],
   "source": [
    "EEG=EEG.drop(remove_6)\n",
    "print(EEG.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "c_EEG=EEG[EEG[14]==0].index\n",
    "nc_EEG=EEG[EEG[14]==1].index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "ConfusedEEG=EEG.drop(c_EEG)\n",
    "NonConfusedEEG=EEG.drop(nc_EEG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "ConfusedEEG=ConfusedEEG.values\n",
    "NonConfusedEEG=NonConfusedEEG.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "index=[]\n",
    "for i in range(7) :\n",
    "    index.append(random.randint(0,5606))\n",
    "    \n",
    "NonConfusedEEG=pd.DataFrame(NonConfusedEEG)\n",
    "NonConfusedEEG=NonConfusedEEG.drop(index)\n",
    "NonConfusedEEG=NonConfusedEEG.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "index=[]\n",
    "for i in range(29) :\n",
    "    index.append(random.randint(0,5928))\n",
    "    \n",
    "ConfusedEEG=pd.DataFrame(ConfusedEEG)\n",
    "ConfusedEEG=ConfusedEEG.drop(index)\n",
    "ConfusedEEG=ConfusedEEG.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5900, 15) (5600, 15)\n"
     ]
    }
   ],
   "source": [
    "X=ConfusedEEG\n",
    "NX=NonConfusedEEG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#동영상, raw, 주파수 data 사용\n",
    "X1=pd.DataFrame(X)\n",
    "X1=X1.drop(X1.columns[[0,2,3,13,14]], axis='columns')\n",
    "#사람 정보, raw, 주파수 data 사용\n",
    "X2=pd.DataFrame(X)\n",
    "X2=X2.drop(X2.columns[[1,2,3,13,14]], axis='columns')\n",
    "#동영상, raw, 주파수 data 사용\n",
    "X3=pd.DataFrame(X)\n",
    "X3=X3.drop(X3.columns[[0,2,3,4,13,14]], axis='columns')\n",
    "#사람 정보, 주파수 data 사용\n",
    "X4=pd.DataFrame(X)\n",
    "X4=X4.drop(X4.columns[[1,2,3,4,13,14]], axis='columns')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "#동영상, raw, 주파수 data 사용\n",
    "NX1=pd.DataFrame(NX)\n",
    "NX1=NX1.drop(NX1.columns[[0,2,3,13,14]], axis='columns')\n",
    "#사람 정보, raw, 주파수 data 사용\n",
    "NX2=pd.DataFrame(NX)\n",
    "NX2=NX2.drop(NX2.columns[[1,2,3,13,14]], axis='columns')\n",
    "#동영상, raw, 주파수 data 사용\n",
    "NX3=pd.DataFrame(NX)\n",
    "NX3=NX3.drop(NX3.columns[[0,2,3,4,13,14]], axis='columns')\n",
    "#사람 정보, 주파수 data 사용\n",
    "NX4=pd.DataFrame(NX)\n",
    "NX4=NX4.drop(NX4.columns[[1,2,3,4,13,14]], axis='columns')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5900, 10) (5900, 10) (5900, 9) (5900, 9)\n",
      "(5600, 10) (5600, 10) (5600, 9) (5600, 9)\n"
     ]
    }
   ],
   "source": [
    "X1=X1.values\n",
    "X2=X2.values\n",
    "X3=X3.values\n",
    "X4=X4.values\n",
    "\n",
    "NX1=NX1.values\n",
    "NX2=NX2.values\n",
    "NX3=NX3.values\n",
    "NX4=NX4.values\n",
    "\n",
    "print(X1.shape, X2.shape, X3.shape, X4.shape)\n",
    "print(NX1.shape, NX2.shape, NX3.shape, NX4.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5900, 10) (5600, 10)\n"
     ]
    }
   ],
   "source": [
    "#X=X1\n",
    "X=X2\n",
    "#X=X3\n",
    "#X=X4\n",
    "\n",
    "#NX=NX1\n",
    "NX=NX2\n",
    "#NX=NX3\n",
    "#NX=NX4\n",
    "\n",
    "print(X.shape, NX.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [],
   "source": [
    "#정규화\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler=MinMaxScaler()\n",
    "X[:]=scaler.fit_transform(X[:])\n",
    "NX[:]=scaler.fit_transform(NX[:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 10)\n"
     ]
    }
   ],
   "source": [
    "X = X.reshape(590,10, 10)\n",
    "NX = NX.reshape(560,10, 10)\n",
    "print(X[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1150, 1)\n"
     ]
    }
   ],
   "source": [
    "label_X= np.zeros((590,1))\n",
    "label_NX=np.ones((560,1))\n",
    "label=np.r_[label_X,label_NX]\n",
    "print(label.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1150, 10, 10)\n"
     ]
    }
   ],
   "source": [
    "data=np.r_[X,NX]\n",
    "print(data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(data, label, test_size=0.4, random_state=115)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(690, 2) (460, 2)\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras import utils as np_utils\n",
    "#one-hot encoding\n",
    "y_train      = np_utils.to_categorical(y_train)\n",
    "y_test       = np_utils.to_categorical(y_test)\n",
    "\n",
    "print(y_train.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(690, 10, 10, 1) (460, 10, 10, 1)\n"
     ]
    }
   ],
   "source": [
    "X_train = X_train.reshape(X_train.shape[0], X_train.shape[1], X_train.shape[2],1)\n",
    "X_test = X_test.reshape(X_test.shape[0], X_test.shape[1], X_test.shape[2],1)\n",
    "\n",
    "print(X_train.shape, X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential, Model \n",
    "from keras.utils import np_utils\n",
    "from keras.layers import  BatchNormalization,Dense, Conv2D, Convolution2D, MaxPooling2D, Dropout, Flatten, TimeDistributed, InputLayer, LSTM\n",
    "from keras.layers import Input, Reshape, Activation, add, Add\n",
    "from keras.utils.vis_utils import model_to_dot\n",
    "from keras.utils.vis_utils import plot_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_5\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_17 (Conv2D)           (None, 10, 10, 50)        250       \n",
      "_________________________________________________________________\n",
      "activation_17 (Activation)   (None, 10, 10, 50)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_18 (Conv2D)           (None, 10, 10, 50)        10050     \n",
      "_________________________________________________________________\n",
      "activation_18 (Activation)   (None, 10, 10, 50)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_5 (MaxPooling2 (None, 5, 5, 50)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_19 (Conv2D)           (None, 5, 5, 50)          10050     \n",
      "_________________________________________________________________\n",
      "activation_19 (Activation)   (None, 5, 5, 50)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_20 (Conv2D)           (None, 5, 5, 50)          10050     \n",
      "_________________________________________________________________\n",
      "activation_20 (Activation)   (None, 5, 5, 50)          0         \n",
      "_________________________________________________________________\n",
      "flatten_5 (Flatten)          (None, 1250)              0         \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 50)                62550     \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 2)                 102       \n",
      "=================================================================\n",
      "Total params: 93,052\n",
      "Trainable params: 93,052\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "def basic_cnn():\n",
    "    model = Sequential()\n",
    "\n",
    "    model.add(Conv2D(input_shape = (X_train.shape[1], X_train.shape[2], X_train.shape[3]), filters = 50, kernel_size = (2,2), strides = (1,1), padding = 'same'))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Conv2D(filters = 50, kernel_size = (2,2), strides = (1,1), padding = 'same'))\n",
    "    model.add(Activation('relu'))\n",
    "    \n",
    "    model.add(MaxPooling2D(pool_size = (2,2)))\n",
    "    \n",
    "    model.add(Conv2D(filters = 50, kernel_size = (2,2), strides = (1,1), padding = 'same'))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Conv2D(filters = 50, kernel_size = (2,2), strides = (1,1), padding = 'same'))\n",
    "    model.add(Activation('relu'))\n",
    "\n",
    "    # prior layer should be flattend to be connected to dense layers\n",
    "    model.add(Flatten())\n",
    "    \n",
    "    \n",
    "    # final layer with 10 neurons to classify the instances\n",
    "    model.add(Dense(50, activation = 'relu'))\n",
    "    \n",
    "    # final layer with 10 neurons to classify the instances\n",
    "    model.add(Dense(2, activation = 'softmax'))\n",
    "\n",
    "    adam = optimizers.Adam(lr = 0.001)\n",
    "    model.compile(loss = 'categorical_crossentropy', optimizer = adam, metrics = ['accuracy'])\n",
    "\n",
    "    return model\n",
    "\n",
    "model = basic_cnn()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 690 samples, validate on 460 samples\n",
      "Epoch 1/300\n",
      "690/690 [==============================] - 0s 410us/step - loss: 0.6902 - accuracy: 0.5246 - val_loss: 0.6979 - val_accuracy: 0.4783\n",
      "Epoch 2/300\n",
      "690/690 [==============================] - 0s 142us/step - loss: 0.6727 - accuracy: 0.5522 - val_loss: 0.6933 - val_accuracy: 0.4783\n",
      "Epoch 3/300\n",
      "690/690 [==============================] - 0s 136us/step - loss: 0.6405 - accuracy: 0.6232 - val_loss: 0.6445 - val_accuracy: 0.6326\n",
      "Epoch 4/300\n",
      "690/690 [==============================] - 0s 145us/step - loss: 0.6033 - accuracy: 0.6623 - val_loss: 0.5872 - val_accuracy: 0.6826\n",
      "Epoch 5/300\n",
      "690/690 [==============================] - 0s 142us/step - loss: 0.4592 - accuracy: 0.8058 - val_loss: 0.3703 - val_accuracy: 0.8630\n",
      "Epoch 6/300\n",
      "690/690 [==============================] - 0s 137us/step - loss: 0.3099 - accuracy: 0.8812 - val_loss: 0.2318 - val_accuracy: 0.9043\n",
      "Epoch 7/300\n",
      "690/690 [==============================] - 0s 139us/step - loss: 0.2298 - accuracy: 0.9217 - val_loss: 0.1634 - val_accuracy: 0.9457\n",
      "Epoch 8/300\n",
      "690/690 [==============================] - 0s 142us/step - loss: 0.1711 - accuracy: 0.9449 - val_loss: 0.1318 - val_accuracy: 0.9457\n",
      "Epoch 9/300\n",
      "690/690 [==============================] - 0s 153us/step - loss: 0.1421 - accuracy: 0.9435 - val_loss: 0.1329 - val_accuracy: 0.9543\n",
      "Epoch 10/300\n",
      "690/690 [==============================] - 0s 139us/step - loss: 0.1156 - accuracy: 0.9623 - val_loss: 0.1122 - val_accuracy: 0.9587\n",
      "Epoch 11/300\n",
      "690/690 [==============================] - 0s 132us/step - loss: 0.1022 - accuracy: 0.9652 - val_loss: 0.1194 - val_accuracy: 0.9609\n",
      "Epoch 12/300\n",
      "690/690 [==============================] - 0s 137us/step - loss: 0.1437 - accuracy: 0.9406 - val_loss: 0.1145 - val_accuracy: 0.9630\n",
      "Epoch 13/300\n",
      "690/690 [==============================] - 0s 134us/step - loss: 0.1060 - accuracy: 0.9652 - val_loss: 0.1118 - val_accuracy: 0.9630\n",
      "Epoch 14/300\n",
      " 50/690 [=>............................] - ETA: 0s - loss: 0.0873 - accuracy: 0.9800"
     ]
    }
   ],
   "source": [
    "hist=model.fit(X_train, y_train, epochs=300, batch_size=50, validation_data=(X_test,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, loss_ax = plt.subplots()\n",
    "\n",
    "acc_ax = loss_ax.twinx()\n",
    "\n",
    "loss_ax.plot(hist.history['loss'], 'y', label='train loss')\n",
    "loss_ax.plot(hist.history['val_loss'], 'r', label='val loss')\n",
    "\n",
    "acc_ax.plot(hist.history['accuracy'], 'b', label='train acc')\n",
    "acc_ax.plot(hist.history['val_accuracy'], 'g', label='val acc')\n",
    "\n",
    "loss_ax.set_xlabel('epoch')\n",
    "loss_ax.set_ylabel('loss')\n",
    "acc_ax.set_ylabel('accuray')\n",
    "\n",
    "loss_ax.legend(loc='upper left')\n",
    "acc_ax.legend(loc='lower left')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "results = model.evaluate(X_test, y_test)\n",
    "print('Test accuracy: ', results[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
