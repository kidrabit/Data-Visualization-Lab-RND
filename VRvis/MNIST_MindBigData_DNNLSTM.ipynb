{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.layers import Dense\n",
    "\n",
    "from keras import optimizers\n",
    "from keras.layers import Dense, Activation, Flatten, Conv2D, MaxPooling2D\n",
    "from keras.utils.np_utils import to_categorical\n",
    "\n",
    "from keras.models import Sequential, Model \n",
    "from keras.utils import np_utils\n",
    "from keras.layers import  BatchNormalization,Dense, Conv2D, Convolution2D, MaxPooling2D, Dropout, Flatten, TimeDistributed, InputLayer, LSTM\n",
    "from keras.layers import Input, Reshape, Activation, add, Add\n",
    "from keras.utils.vis_utils import model_to_dot\n",
    "from keras.utils.vis_utils import plot_model\n",
    "from keras import optimizers\n",
    "\n",
    "from sklearn.metrics import recall_score \n",
    "from sklearn.metrics import precision_score \n",
    "from sklearn.metrics import f1_score \n",
    "from keras import backend as K\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "file=open(\"mindbigdata.txt\",\"r\")\n",
    "data=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "for line in file.readlines():\n",
    "    data.append(line.replace(\"\\t\",\",\").split(\",\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "eeg=pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "eeg=eeg.dropna(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "EEG=eeg.drop(eeg.columns[[0,1,2,5]],axis='columns')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#번호별로 data 분리\n",
    "EEG_0=EEG[EEG[4].isin(['0'])]\n",
    "EEG_1=EEG[EEG[4].isin(['1'])]\n",
    "EEG_2=EEG[EEG[4].isin(['2'])]\n",
    "EEG_3=EEG[EEG[4].isin(['3'])]\n",
    "EEG_4=EEG[EEG[4].isin(['4'])]\n",
    "EEG_5=EEG[EEG[4].isin(['5'])]\n",
    "EEG_6=EEG[EEG[4].isin(['6'])]\n",
    "EEG_7=EEG[EEG[4].isin(['7'])]\n",
    "EEG_8=EEG[EEG[4].isin(['8'])]\n",
    "EEG_9=EEG[EEG[4].isin(['9'])]\n",
    "EEG__1=EEG[EEG[4].isin(['-1'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "EEG_0=pd.DataFrame(EEG_0.values)\n",
    "EEG_1=pd.DataFrame(EEG_1.values)\n",
    "EEG_2=pd.DataFrame(EEG_2.values)\n",
    "EEG_3=pd.DataFrame(EEG_3.values)\n",
    "EEG_4=pd.DataFrame(EEG_4.values)\n",
    "EEG_5=pd.DataFrame(EEG_5.values)\n",
    "EEG_6=pd.DataFrame(EEG_6.values)\n",
    "EEG_7=pd.DataFrame(EEG_7.values)\n",
    "EEG_8=pd.DataFrame(EEG_8.values)\n",
    "EEG_9=pd.DataFrame(EEG_9.values)\n",
    "EEG__1=pd.DataFrame(EEG__1.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "EEG_0=EEG_0.drop(EEG_0.columns[[0,1]],axis='columns')\n",
    "EEG_1=EEG_1.drop(EEG_1.columns[[0,1]],axis='columns')\n",
    "EEG_2=EEG_2.drop(EEG_2.columns[[0,1]],axis='columns')\n",
    "EEG_3=EEG_3.drop(EEG_3.columns[[0,1]],axis='columns')\n",
    "EEG_4=EEG_4.drop(EEG_4.columns[[0,1]],axis='columns')\n",
    "EEG_5=EEG_5.drop(EEG_5.columns[[0,1]],axis='columns')\n",
    "EEG_6=EEG_6.drop(EEG_6.columns[[0,1]],axis='columns')\n",
    "EEG_7=EEG_7.drop(EEG_7.columns[[0,1]],axis='columns')\n",
    "EEG_8=EEG_8.drop(EEG_8.columns[[0,1]],axis='columns')\n",
    "EEG_9=EEG_9.drop(EEG_9.columns[[0,1]],axis='columns')\n",
    "EEG__1=EEG__1.drop(EEG__1.columns[[0,1]],axis='columns')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "eeg_0=EEG_0.values\n",
    "eeg_1=EEG_1.values\n",
    "eeg_2=EEG_2.values\n",
    "eeg_3=EEG_3.values\n",
    "eeg_4=EEG_4.values\n",
    "eeg_5=EEG_5.values\n",
    "eeg_6=EEG_6.values\n",
    "eeg_7=EEG_7.values\n",
    "eeg_8=EEG_8.values\n",
    "eeg_9=EEG_9.values\n",
    "eeg__1=EEG__1.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler=MinMaxScaler()\n",
    "eeg_0[:]=scaler.fit_transform(eeg_0[:])*255\n",
    "eeg_1[:]=scaler.fit_transform(eeg_1[:])*255\n",
    "eeg_2[:]=scaler.fit_transform(eeg_2[:])*255\n",
    "eeg_3[:]=scaler.fit_transform(eeg_3[:])*255\n",
    "eeg_4[:]=scaler.fit_transform(eeg_4[:])*255\n",
    "eeg_5[:]=scaler.fit_transform(eeg_5[:])*255\n",
    "eeg_6[:]=scaler.fit_transform(eeg_6[:])*255\n",
    "eeg_7[:]=scaler.fit_transform(eeg_7[:])*255\n",
    "eeg_8[:]=scaler.fit_transform(eeg_8[:])*255\n",
    "eeg_9[:]=scaler.fit_transform(eeg_9[:])*255\n",
    "eeg__1[:]=scaler.fit_transform(eeg__1[:])*255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reshapeLinearData(eeg):\n",
    "    data=[]\n",
    "    data=np.r_[eeg[0],eeg[1]]\n",
    "    for i in range(2,len(eeg)):\n",
    "        data=np.r_[data,eeg[i]]\n",
    "    return data\n",
    "\n",
    "def convertT(eeg) :\n",
    "    data=[]\n",
    "    for i in range(len(eeg)) :\n",
    "        data.append(eeg[i].T)\n",
    "    data=np.asarray(data)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_0=eeg_0.reshape(-1,14,136)\n",
    "data_1=eeg_1.reshape(-1,14,136)\n",
    "data_2=eeg_2.reshape(-1,14,136)\n",
    "data_3=eeg_3.reshape(-1,14,136)\n",
    "data_4=eeg_4.reshape(-1,14,136)\n",
    "data_5=eeg_5.reshape(-1,14,136)\n",
    "data_6=eeg_6.reshape(-1,14,136)\n",
    "data_7=eeg_7.reshape(-1,14,136)\n",
    "data_8=eeg_8.reshape(-1,14,136)\n",
    "data_9=eeg_9.reshape(-1,14,136)\n",
    "data__1=eeg__1.reshape(-1,14,136)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "eeg_0=convertT(data_0)\n",
    "eeg_1=convertT(data_1)\n",
    "eeg_2=convertT(data_2)\n",
    "eeg_3=convertT(data_3)\n",
    "eeg_4=convertT(data_4)\n",
    "eeg_5=convertT(data_5)\n",
    "eeg_6=convertT(data_6)\n",
    "eeg_7=convertT(data_7)\n",
    "eeg_8=convertT(data_8)\n",
    "eeg_9=convertT(data_9)\n",
    "eeg__1=convertT(data__1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finish\n",
      "finish\n",
      "finish\n",
      "finish\n",
      "finish\n",
      "finish\n",
      "finish\n",
      "finish\n",
      "finish\n",
      "finish\n"
     ]
    }
   ],
   "source": [
    "eeg_0=reshapeLinearData(eeg_0)\n",
    "print(\"finish\")\n",
    "eeg_1=reshapeLinearData(eeg_1)\n",
    "print(\"finish\")\n",
    "eeg_2=reshapeLinearData(eeg_2)\n",
    "print(\"finish\")\n",
    "eeg_3=reshapeLinearData(eeg_3)\n",
    "print(\"finish\")\n",
    "eeg_4=reshapeLinearData(eeg_4)\n",
    "print(\"finish\")\n",
    "eeg_5=reshapeLinearData(eeg_5)\n",
    "print(\"finish\")\n",
    "eeg_6=reshapeLinearData(eeg_6)\n",
    "print(\"finish\")\n",
    "eeg_7=reshapeLinearData(eeg_7)\n",
    "print(\"finish\")\n",
    "eeg_8=reshapeLinearData(eeg_8)\n",
    "print(\"finish\")\n",
    "eeg_9=reshapeLinearData(eeg_9)\n",
    "print(\"finish\")\n",
    "eeg__1=reshapeLinearData(eeg__1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(886176, 14) (863736, 14) (883320, 14) (900048, 14) (863464, 14) (893656, 14) (887128, 14) (861832, 14) (891072, 14) (892568, 14) (21624, 14)\n"
     ]
    }
   ],
   "source": [
    "print(eeg_0.shape,eeg_1.shape,eeg_2.shape,eeg_3.shape,eeg_4.shape,eeg_5.shape,eeg_6.shape,eeg_7.shape,eeg_8.shape,eeg_9.shape,eeg__1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_0=np.zeros((len(eeg_0),1))\n",
    "label_1=np.ones((len(eeg_1),1))\n",
    "label_2=np.full((len(eeg_2),1),2)\n",
    "label_3=np.full((len(eeg_3),1),2)\n",
    "label_4=np.full((len(eeg_4),1),2)\n",
    "label_5=np.full((len(eeg_5),1),2)\n",
    "label_6=np.full((len(eeg_6),1),2)\n",
    "label_7=np.full((len(eeg_7),1),2)\n",
    "label_8=np.full((len(eeg_8),1),2)\n",
    "label_9=np.full((len(eeg_9),1),2)\n",
    "label__1=np.full((len(eeg__1),1),-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8844624, 14)\n"
     ]
    }
   ],
   "source": [
    "data=np.r_[eeg_0,eeg_1]\n",
    "data=np.r_[data,eeg_2]\n",
    "data=np.r_[data,eeg_3]\n",
    "data=np.r_[data,eeg_4]\n",
    "data=np.r_[data,eeg_5]\n",
    "data=np.r_[data,eeg_6]\n",
    "data=np.r_[data,eeg_7]\n",
    "data=np.r_[data,eeg_8]\n",
    "data=np.r_[data,eeg_9]\n",
    "data=np.r_[data,eeg__1]\n",
    "print(data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8844624, 1)\n"
     ]
    }
   ],
   "source": [
    "label=np.r_[label_0,label_1]\n",
    "label=np.r_[label,label_2]\n",
    "label=np.r_[label,label_3]\n",
    "label=np.r_[label,label_4]\n",
    "label=np.r_[label,label_5]\n",
    "label=np.r_[label,label_6]\n",
    "label=np.r_[label,label_7]\n",
    "label=np.r_[label,label_8]\n",
    "label=np.r_[label,label_9]\n",
    "label=np.r_[label,label__1]\n",
    "print(label.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(data, label, test_size=0.3, random_state=None, shuffle=True, stratify=label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6191236, 14) (2653388, 14) (6191236, 11) (2653388, 11)\n"
     ]
    }
   ],
   "source": [
    "y_train = to_categorical(y_train, 11)\n",
    "y_test = to_categorical(y_test, 11)\n",
    "\n",
    "print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recall(y_target, y_pred):\n",
    "    # clip(t, clip_value_min, clip_value_max) : clip_value_min~clip_value_max 이외 가장자리를 깎아 낸다\n",
    "    # round : 반올림한다\n",
    "    y_target_yn = K.round(K.clip(y_target, 0, 1)) # 실제값을 0(Negative) 또는 1(Positive)로 설정한다\n",
    "    y_pred_yn = K.round(K.clip(y_pred, 0, 1)) # 예측값을 0(Negative) 또는 1(Positive)로 설정한다\n",
    "\n",
    "    # True Positive는 실제 값과 예측 값이 모두 1(Positive)인 경우이다\n",
    "    count_true_positive = K.sum(y_target_yn * y_pred_yn) \n",
    "\n",
    "    # (True Positive + False Negative) = 실제 값이 1(Positive) 전체\n",
    "    count_true_positive_false_negative = K.sum(y_target_yn)\n",
    "\n",
    "    # Recall =  (True Positive) / (True Positive + False Negative)\n",
    "    # K.epsilon()는 'divide by zero error' 예방차원에서 작은 수를 더한다\n",
    "    recall = count_true_positive / (count_true_positive_false_negative + K.epsilon())\n",
    "\n",
    "    # return a single tensor value\n",
    "    return recall\n",
    "\n",
    "\n",
    "def precision(y_target, y_pred):\n",
    "    # clip(t, clip_value_min, clip_value_max) : clip_value_min~clip_value_max 이외 가장자리를 깎아 낸다\n",
    "    # round : 반올림한다\n",
    "    y_pred_yn = K.round(K.clip(y_pred, 0, 1)) # 예측값을 0(Negative) 또는 1(Positive)로 설정한다\n",
    "    y_target_yn = K.round(K.clip(y_target, 0, 1)) # 실제값을 0(Negative) 또는 1(Positive)로 설정한다\n",
    "\n",
    "    # True Positive는 실제 값과 예측 값이 모두 1(Positive)인 경우이다\n",
    "    count_true_positive = K.sum(y_target_yn * y_pred_yn) \n",
    "\n",
    "    # (True Positive + False Positive) = 예측 값이 1(Positive) 전체\n",
    "    count_true_positive_false_positive = K.sum(y_pred_yn)\n",
    "\n",
    "    # Precision = (True Positive) / (True Positive + False Positive)\n",
    "    # K.epsilon()는 'divide by zero error' 예방차원에서 작은 수를 더한다\n",
    "    precision = count_true_positive / (count_true_positive_false_positive + K.epsilon())\n",
    "\n",
    "    # return a single tensor value\n",
    "    return precision\n",
    "\n",
    "\n",
    "def f1score(y_target, y_pred):\n",
    "    _recall = recall(y_target, y_pred)\n",
    "    _precision = precision(y_target, y_pred)\n",
    "    # K.epsilon()는 'divide by zero error' 예방차원에서 작은 수를 더한다\n",
    "    _f1score = ( 2 * _recall * _precision) / (_recall + _precision+ K.epsilon())\n",
    "    \n",
    "    # return a single tensor value\n",
    "    return _f1score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         (None, 14)                0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 150)               2250      \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 150)               0         \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 200)               30200     \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 11)                2211      \n",
      "=================================================================\n",
      "Total params: 34,661\n",
      "Trainable params: 34,661\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "def model_create(eeg_data):\n",
    "    eeg_input=Input(shape=(14,)) #입력 정의\n",
    "    \n",
    "    eeg_output = Dense(50, activation='relu')(eeg_input)\n",
    "    eeg_output = Dropout(0.5)(eeg_output)\n",
    "    eeg_output = Dense(150, activation='relu')(eeg_input)\n",
    "    eeg_output = Dropout(0.5)(eeg_output)\n",
    "    eeg_output = Dense(150, activation='relu')(eeg_input)\n",
    "    eeg_output = Dropout(0.5)(eeg_output)\n",
    "    eeg_output = Dense(200, activation='relu')(eeg_output)\n",
    "    \n",
    "    model = Dense(11, activation='sigmoid')(eeg_output)\n",
    "    \n",
    "    model = keras.models.Model(inputs=eeg_input, outputs=model) \n",
    "    \n",
    "    model.compile(loss='binary_crossentropy', optimizer = 'adam', metrics=['accuracy', precision, recall, f1score])\n",
    "    \n",
    "    return model \n",
    "\n",
    "model=model_create(data)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 6191236 samples, validate on 2653388 samples\n",
      "Epoch 1/20\n",
      "6191236/6191236 [==============================] - 3182s 514us/step - loss: 0.1049 - accuracy: 0.9659 - precision: 0.8026 - recall: 0.8026 - f1score: 0.8026 - val_loss: 0.1039 - val_accuracy: 0.9650 - val_precision: 0.8023 - val_recall: 0.8023 - val_f1score: 0.8023\n",
      "Epoch 2/20\n",
      "6191236/6191236 [==============================] - 11110s 2ms/step - loss: 0.1043 - accuracy: 0.9660 - precision: 0.8030 - recall: 0.8030 - f1score: 0.8030 - val_loss: 0.1039 - val_accuracy: 0.9650 - val_precision: 0.8023 - val_recall: 0.8023 - val_f1score: 0.8023\n",
      "Epoch 3/20\n",
      "6191236/6191236 [==============================] - 2372s 383us/step - loss: 0.1042 - accuracy: 0.9660 - precision: 0.8031 - recall: 0.8031 - f1score: 0.8031 - val_loss: 0.1039 - val_accuracy: 0.9649 - val_precision: 0.8022 - val_recall: 0.8022 - val_f1score: 0.8022\n",
      "Epoch 4/20\n",
      "6191236/6191236 [==============================] - 3475s 561us/step - loss: 0.1042 - accuracy: 0.9660 - precision: 0.8031 - recall: 0.8031 - f1score: 0.8031 - val_loss: 0.1038 - val_accuracy: 0.9650 - val_precision: 0.8023 - val_recall: 0.8023 - val_f1score: 0.8023\n",
      "Epoch 5/20\n",
      "6191236/6191236 [==============================] - 2649s 428us/step - loss: 0.1042 - accuracy: 0.9660 - precision: 0.8031 - recall: 0.8031 - f1score: 0.8031 - val_loss: 0.1038 - val_accuracy: 0.9650 - val_precision: 0.8023 - val_recall: 0.8023 - val_f1score: 0.8023\n",
      "Epoch 6/20\n",
      "6191236/6191236 [==============================] - 2729s 441us/step - loss: 0.1041 - accuracy: 0.9660 - precision: 0.8032 - recall: 0.8032 - f1score: 0.8032 - val_loss: 0.1038 - val_accuracy: 0.9650 - val_precision: 0.8023 - val_recall: 0.8023 - val_f1score: 0.8023\n",
      "Epoch 7/20\n",
      "6191236/6191236 [==============================] - 2686s 434us/step - loss: 0.1042 - accuracy: 0.9660 - precision: 0.8031 - recall: 0.8031 - f1score: 0.8031 - val_loss: 0.1039 - val_accuracy: 0.9650 - val_precision: 0.8023 - val_recall: 0.8023 - val_f1score: 0.8023\n",
      "Epoch 8/20\n",
      "6191236/6191236 [==============================] - 2686s 434us/step - loss: 0.1042 - accuracy: 0.9660 - precision: 0.8031 - recall: 0.8031 - f1score: 0.8031 - val_loss: 0.1039 - val_accuracy: 0.9650 - val_precision: 0.8023 - val_recall: 0.8023 - val_f1score: 0.8023\n",
      "Epoch 9/20\n",
      "6191236/6191236 [==============================] - 2648s 428us/step - loss: 0.1041 - accuracy: 0.9660 - precision: 0.8031 - recall: 0.8031 - f1score: 0.8031 - val_loss: 0.1038 - val_accuracy: 0.9650 - val_precision: 0.8023 - val_recall: 0.8023 - val_f1score: 0.8023\n",
      "Epoch 10/20\n",
      "6191236/6191236 [==============================] - 2663s 430us/step - loss: 0.1041 - accuracy: 0.9660 - precision: 0.8031 - recall: 0.8031 - f1score: 0.8031 - val_loss: 0.1038 - val_accuracy: 0.9650 - val_precision: 0.8023 - val_recall: 0.8023 - val_f1score: 0.8023\n",
      "Epoch 11/20\n",
      "6191236/6191236 [==============================] - 2685s 434us/step - loss: 0.1041 - accuracy: 0.9660 - precision: 0.8032 - recall: 0.8031 - f1score: 0.8031 - val_loss: 0.1038 - val_accuracy: 0.9650 - val_precision: 0.8023 - val_recall: 0.8023 - val_f1score: 0.8023\n",
      "Epoch 12/20\n",
      "6191236/6191236 [==============================] - 2683s 433us/step - loss: 0.1041 - accuracy: 0.9660 - precision: 0.8032 - recall: 0.8031 - f1score: 0.8031 - val_loss: 0.1039 - val_accuracy: 0.9650 - val_precision: 0.8023 - val_recall: 0.8023 - val_f1score: 0.8023\n",
      "Epoch 13/20\n",
      "6191236/6191236 [==============================] - 2668s 431us/step - loss: 0.1041 - accuracy: 0.9660 - precision: 0.8033 - recall: 0.8032 - f1score: 0.8032 - val_loss: 0.1038 - val_accuracy: 0.9650 - val_precision: 0.8023 - val_recall: 0.8023 - val_f1score: 0.8023\n",
      "Epoch 14/20\n",
      "6191236/6191236 [==============================] - 2691s 435us/step - loss: 0.1040 - accuracy: 0.9660 - precision: 0.8033 - recall: 0.8032 - f1score: 0.8032 - val_loss: 0.1038 - val_accuracy: 0.9650 - val_precision: 0.8023 - val_recall: 0.8023 - val_f1score: 0.8023\n",
      "Epoch 15/20\n",
      "6191170/6191236 [============================>.] - ETA: 0s - loss: 0.1040 - accuracy: 0.9660 - precision: 0.8033 - recall: 0.8033 - f1score: 0.8033"
     ]
    }
   ],
   "source": [
    "hist=model.fit(X_train, y_train, epochs=20, batch_size=10, validation_data=(X_test,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, loss_ax = plt.subplots()\n",
    "\n",
    "acc_ax = loss_ax.twinx()\n",
    "\n",
    "loss_ax.plot(hist.history['loss'], 'y', label='train loss')\n",
    "loss_ax.plot(hist.history['val_loss'], 'r', label='val loss')\n",
    "\n",
    "acc_ax.plot(hist.history['accuracy'], 'b', label='train acc')\n",
    "acc_ax.plot(hist.history['val_accuracy'], 'g', label='val acc')\n",
    "\n",
    "loss_ax.set_xlabel('epoch')\n",
    "loss_ax.set_ylabel('loss')\n",
    "acc_ax.set_ylabel('accuray')\n",
    "\n",
    "loss_ax.legend(loc='upper left')\n",
    "acc_ax.legend(loc='lower left')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "#results = model.evaluate(X_test, y_test)\n",
    "print('Test accuracy: ', results)\n",
    "\n",
    "_loss, _acc, _precision, _recall, _f1score = model.evaluate(X_test, y_test)\n",
    "print('loss: {:.3f}, accuracy: {:.3f}, precision: {:.3f}, recall: {:.3f}, f1score: {:.3f}'.format(_loss, _acc, _precision, _recall, _f1score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, rec_ax = plt.subplots()\n",
    "\n",
    "pre_ax = rec_ax.twinx()\n",
    "\n",
    "rec_ax.plot(hist.history['recall'], 'y', label='train recall')\n",
    "rec_ax.plot(hist.history['val_recall'], 'r', label='val recall')\n",
    "\n",
    "pre_ax.plot(hist.history['precision'], 'b', label='train precision')\n",
    "pre_ax.plot(hist.history['val_precision'], 'g', label='val precision')\n",
    "\n",
    "rec_ax.set_xlabel('epoch')\n",
    "rec_ax.set_ylabel('recall')\n",
    "pre_ax.set_ylabel('precision')\n",
    "\n",
    "rec_ax.legend(loc='upper left')\n",
    "pre_ax.legend(loc='lower left')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_train = X_train.reshape(X_train.shape+ (1,))\n",
    "#X_test = X_test.reshape(X_test.shape + (1,))\n",
    "\n",
    "def lstm_model():\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(50, input_shape = (14,1), return_sequences = True))\n",
    "    model.add(LSTM(11, return_sequences = False))\n",
    "    model.add(Activation('sigmoid'))\n",
    "    \n",
    "    model.compile(loss = 'binary_crossentropy', optimizer = 'adam', metrics = ['accuracy'])\n",
    "    return model\n",
    "\n",
    "model = lstm_model()\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist=model.fit(X_train, y_train, epochs=20, batch_size=10, validation_data=(X_test,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, loss_ax = plt.subplots()\n",
    "\n",
    "acc_ax = loss_ax.twinx()\n",
    "\n",
    "loss_ax.plot(hist.history['loss'], 'y', label='train loss')\n",
    "loss_ax.plot(hist.history['val_loss'], 'r', label='val loss')\n",
    "\n",
    "acc_ax.plot(hist.history['accuracy'], 'b', label='train acc')\n",
    "acc_ax.plot(hist.history['val_accuracy'], 'g', label='val acc')\n",
    "\n",
    "loss_ax.set_xlabel('epoch')\n",
    "loss_ax.set_ylabel('loss')\n",
    "acc_ax.set_ylabel('accuray')\n",
    "\n",
    "loss_ax.legend(loc='upper left')\n",
    "acc_ax.legend(loc='lower left')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "#results = model.evaluate(X_test, y_test)\n",
    "print('Test accuracy: ', results)\n",
    "\n",
    "_loss, _acc, _precision, _recall, _f1score = model.evaluate(X_test, y_test)\n",
    "print('loss: {:.3f}, accuracy: {:.3f}, precision: {:.3f}, recall: {:.3f}, f1score: {:.3f}'.format(_loss, _acc, _precision, _recall, _f1score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, rec_ax = plt.subplots()\n",
    "\n",
    "pre_ax = rec_ax.twinx()\n",
    "\n",
    "rec_ax.plot(hist.history['recall'], 'y', label='train recall')\n",
    "rec_ax.plot(hist.history['val_recall'], 'r', label='val recall')\n",
    "\n",
    "pre_ax.plot(hist.history['precision'], 'b', label='train precision')\n",
    "pre_ax.plot(hist.history['val_precision'], 'g', label='val precision')\n",
    "\n",
    "rec_ax.set_xlabel('epoch')\n",
    "rec_ax.set_ylabel('recall')\n",
    "pre_ax.set_ylabel('precision')\n",
    "\n",
    "rec_ax.legend(loc='upper left')\n",
    "pre_ax.legend(loc='lower left')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
